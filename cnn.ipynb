{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import itertools\n",
    "import gc\n",
    "import gensim\n",
    "from collections import Counter\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load data from files\n",
    "positive_examples = list(open(\"./data/rt-polaritydata/rt-polarity.pos\").readlines())\n",
    "positive_examples = [s.strip() for s in positive_examples]\n",
    "negative_examples = list(open(\"./data/rt-polaritydata/rt-polarity.neg\").readlines())\n",
    "negative_examples = [s.strip() for s in negative_examples]\n",
    "\n",
    "# Generate labels\n",
    "positive_labels = [[0, 1] for _ in positive_examples]\n",
    "negative_labels = [[1, 0] for _ in negative_examples]\n",
    "# Concatenate positive and negative examples\n",
    "x_text = positive_examples + negative_examples\n",
    "y = np.concatenate([positive_labels, negative_labels], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at 90 minutes this movie is short , but it feels much longer .\n",
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "print x_text[len(positive_examples) + 242]\n",
    "print y[len(positive_examples) + 242]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Padding our sentences to the same length\n",
    "x_text_split = [s.split(\" \") for s in x_text]\n",
    "padded_sentences = [] \n",
    "# We pad all sentences to the maximum sentence length in the dataset\n",
    "SEQUENCE_LENGTH = max(len(x) for x in x_text_split)\n",
    "PADDING_WORD = \"<PAD/>\"\n",
    "for i in range(len(x_text_split)):\n",
    "    sentence = x_text_split[i]\n",
    "    num_padding = SEQUENCE_LENGTH - len(sentence)\n",
    "    new_sentence = sentence + [PADDING_WORD] * num_padding\n",
    "    padded_sentences.append(new_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEQUENCE_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build vocabulary\n",
    "word_counts = Counter(itertools.chain(*padded_sentences))\n",
    "# Mapping from index to word\n",
    "vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "# Mapping from word to index\n",
    "vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<PAD/>', 404991),\n",
       " ('.', 14010),\n",
       " ('the', 10096),\n",
       " (',', 10037),\n",
       " ('a', 7281),\n",
       " ('and', 6195),\n",
       " ('of', 6061),\n",
       " ('to', 4233),\n",
       " ('is', 3367),\n",
       " ('in', 2628),\n",
       " ('that', 2470),\n",
       " ('it', 2281),\n",
       " ('as', 1801),\n",
       " ('but', 1637),\n",
       " ('with', 1560),\n",
       " ('film', 1445),\n",
       " ('this', 1440),\n",
       " ('for', 1436),\n",
       " ('its', 1335),\n",
       " ('an', 1321),\n",
       " ('movie', 1268),\n",
       " (\"it's\", 1119),\n",
       " ('be', 939),\n",
       " ('on', 895),\n",
       " ('you', 892),\n",
       " ('not', 803),\n",
       " ('by', 795),\n",
       " ('about', 733),\n",
       " ('one', 727),\n",
       " ('more', 727),\n",
       " ('like', 720),\n",
       " ('has', 709),\n",
       " ('are', 708),\n",
       " ('at', 705),\n",
       " ('from', 673),\n",
       " ('than', 664),\n",
       " ('\"', 655),\n",
       " ('all', 641),\n",
       " ('--', 629),\n",
       " ('his', 628),\n",
       " ('have', 623),\n",
       " ('so', 555),\n",
       " ('if', 537),\n",
       " ('or', 519),\n",
       " ('story', 476),\n",
       " ('i', 466),\n",
       " ('too', 459),\n",
       " ('just', 438),\n",
       " ('who', 432),\n",
       " ('into', 417),\n",
       " ('what', 413),\n",
       " ('most', 402),\n",
       " ('out', 398),\n",
       " ('no', 387),\n",
       " ('much', 386),\n",
       " ('even', 382),\n",
       " ('good', 377),\n",
       " ('up', 376),\n",
       " ('will', 374),\n",
       " ('comedy', 353),\n",
       " ('time', 339),\n",
       " ('can', 337),\n",
       " ('some', 334),\n",
       " ('characters', 313),\n",
       " ('only', 308),\n",
       " ('little', 302),\n",
       " ('way', 296),\n",
       " ('their', 292),\n",
       " ('funny', 283),\n",
       " ('make', 278),\n",
       " ('enough', 267),\n",
       " ('been', 266),\n",
       " ('very', 265),\n",
       " ('your', 264),\n",
       " ('never', 262),\n",
       " ('when', 262),\n",
       " ('makes', 252),\n",
       " ('there', 250),\n",
       " ('may', 245),\n",
       " ('us', 241),\n",
       " ('which', 241),\n",
       " ('work', 236),\n",
       " ('best', 235),\n",
       " ('he', 234),\n",
       " ('bad', 233),\n",
       " ('director', 231),\n",
       " (\"doesn't\", 231),\n",
       " (')', 228),\n",
       " ('?', 227),\n",
       " ('any', 227),\n",
       " ('(', 226),\n",
       " ('love', 224),\n",
       " ('would', 222),\n",
       " ('life', 221),\n",
       " ('they', 219),\n",
       " ('while', 219),\n",
       " (':', 217),\n",
       " ('we', 217),\n",
       " ('was', 216),\n",
       " (\"there's\", 209),\n",
       " ('movies', 208),\n",
       " ('her', 206),\n",
       " ('well', 206),\n",
       " ('new', 206),\n",
       " ('through', 200),\n",
       " ('could', 199),\n",
       " ('really', 197),\n",
       " ('something', 197),\n",
       " ('how', 196),\n",
       " ('made', 193),\n",
       " ('them', 186),\n",
       " ('does', 185),\n",
       " ('performances', 185),\n",
       " ('own', 185),\n",
       " ('many', 183),\n",
       " ('should', 183),\n",
       " (\"that's\", 183),\n",
       " ('drama', 183),\n",
       " ('those', 181),\n",
       " ('films', 179),\n",
       " ('plot', 179),\n",
       " ('look', 179),\n",
       " ('see', 178),\n",
       " ('still', 178),\n",
       " (\"isn't\", 178),\n",
       " ('every', 178),\n",
       " ('two', 175),\n",
       " ('nothing', 173),\n",
       " ('people', 171),\n",
       " ('better', 169),\n",
       " ('long', 167),\n",
       " ('without', 167),\n",
       " ('other', 166),\n",
       " ('off', 165),\n",
       " ('get', 165),\n",
       " ('fun', 165),\n",
       " ('being', 164),\n",
       " ('action', 164),\n",
       " ('both', 161),\n",
       " ('great', 160),\n",
       " ('though', 158),\n",
       " ('big', 156),\n",
       " ('might', 156),\n",
       " (\"'\", 154),\n",
       " ('also', 153),\n",
       " ('another', 151),\n",
       " ('cast', 151),\n",
       " ('do', 149),\n",
       " ('between', 146),\n",
       " ('humor', 146),\n",
       " ('first', 146),\n",
       " ('audience', 146),\n",
       " ('kind', 146),\n",
       " ('sense', 145),\n",
       " ('such', 144),\n",
       " ('over', 144),\n",
       " ('ever', 142),\n",
       " ('character', 142),\n",
       " (\"don't\", 141),\n",
       " ('feels', 140),\n",
       " (';', 140),\n",
       " ('performance', 140),\n",
       " ('few', 139),\n",
       " ('script', 138),\n",
       " ('because', 138),\n",
       " (\"film's\", 137),\n",
       " ('here', 136),\n",
       " ('far', 136),\n",
       " ('often', 135),\n",
       " ('less', 134),\n",
       " ('thing', 134),\n",
       " ('seems', 134),\n",
       " ('real', 132),\n",
       " ('minutes', 132),\n",
       " ('feel', 130),\n",
       " ('thriller', 127),\n",
       " ('world', 127),\n",
       " ('tale', 127),\n",
       " ('almost', 127),\n",
       " ('picture', 127),\n",
       " (\"can't\", 126),\n",
       " ('quite', 125),\n",
       " ('documentary', 125),\n",
       " ('down', 125),\n",
       " ('yet', 124),\n",
       " ('interesting', 124),\n",
       " ('!', 121),\n",
       " ('these', 120),\n",
       " ('entertaining', 120),\n",
       " ('screen', 119),\n",
       " (\"you're\", 119),\n",
       " ('my', 119),\n",
       " ('rather', 119),\n",
       " ('end', 118),\n",
       " ('itself', 117),\n",
       " ('take', 116),\n",
       " ('full', 116),\n",
       " ('hollywood', 116),\n",
       " ('seen', 116),\n",
       " ('watching', 116),\n",
       " ('go', 115),\n",
       " ('hard', 115),\n",
       " ('ultimately', 115),\n",
       " ('heart', 114),\n",
       " ('moments', 113),\n",
       " ('de', 113),\n",
       " ('comes', 113),\n",
       " ('romantic', 113),\n",
       " ('lot', 111),\n",
       " ('despite', 111),\n",
       " ('american', 110),\n",
       " ('were', 109),\n",
       " ('where', 108),\n",
       " ('after', 108),\n",
       " ('me', 108),\n",
       " ('family', 107),\n",
       " ('before', 107),\n",
       " ('acting', 107),\n",
       " ('had', 106),\n",
       " ('old', 106),\n",
       " ('original', 106),\n",
       " ('then', 106),\n",
       " ('right', 105),\n",
       " ('find', 105),\n",
       " ('worth', 104),\n",
       " ('human', 104),\n",
       " ('gets', 104),\n",
       " ('same', 103),\n",
       " ('takes', 102),\n",
       " ('things', 101),\n",
       " ('times', 101),\n",
       " ('come', 101),\n",
       " ('dialogue', 98),\n",
       " ('actors', 97),\n",
       " ('back', 97),\n",
       " ('scenes', 97),\n",
       " ('man', 97),\n",
       " ('watch', 97),\n",
       " ('our', 96),\n",
       " ('material', 95),\n",
       " ('compelling', 94),\n",
       " ('young', 93),\n",
       " ('once', 93),\n",
       " ('music', 91),\n",
       " ('years', 90),\n",
       " ('works', 90),\n",
       " ('think', 89),\n",
       " ('emotional', 88),\n",
       " ('seem', 88),\n",
       " ('anyone', 88),\n",
       " ('want', 87),\n",
       " ('gives', 87),\n",
       " ('going', 86),\n",
       " ('least', 86),\n",
       " ('know', 86),\n",
       " ('say', 85),\n",
       " ('part', 85),\n",
       " ('sometimes', 84),\n",
       " ('piece', 84),\n",
       " ('again', 84),\n",
       " ('cinematic', 83),\n",
       " ('entertainment', 83),\n",
       " ('kids', 82),\n",
       " ('last', 82),\n",
       " (\"you'll\", 82),\n",
       " ('pretty', 82),\n",
       " ('give', 82),\n",
       " ('subject', 82),\n",
       " ('point', 82),\n",
       " ('special', 80),\n",
       " ('keep', 80),\n",
       " ('making', 80),\n",
       " ('bit', 80),\n",
       " ('whole', 79),\n",
       " ('together', 79),\n",
       " ('dull', 79),\n",
       " ('fascinating', 79),\n",
       " ('why', 79),\n",
       " ('cinema', 79),\n",
       " ('anything', 78),\n",
       " ('fans', 78),\n",
       " ('year', 77),\n",
       " ('away', 76),\n",
       " ('since', 76),\n",
       " ('-', 76),\n",
       " ('moving', 76),\n",
       " ('need', 75),\n",
       " ('manages', 75),\n",
       " ('style', 75),\n",
       " ('true', 74),\n",
       " ('star', 74),\n",
       " ('laughs', 74),\n",
       " ('show', 74),\n",
       " ('always', 73),\n",
       " ('history', 73),\n",
       " ('experience', 73),\n",
       " ('offers', 73),\n",
       " ('sweet', 73),\n",
       " ('clever', 73),\n",
       " ('high', 72),\n",
       " ('simply', 72),\n",
       " ('mr', 72),\n",
       " ('direction', 72),\n",
       " ('dark', 71),\n",
       " ('silly', 71),\n",
       " ('instead', 71),\n",
       " ('predictable', 70),\n",
       " ('care', 70),\n",
       " ('actually', 70),\n",
       " ('whose', 70),\n",
       " ('charm', 70),\n",
       " ('him', 70),\n",
       " ('visual', 69),\n",
       " ('art', 69),\n",
       " ('nearly', 69),\n",
       " ('everything', 69),\n",
       " ('flick', 69),\n",
       " ('series', 68),\n",
       " ('around', 68),\n",
       " ('title', 68),\n",
       " ('matter', 68),\n",
       " ('video', 67),\n",
       " ('comic', 67),\n",
       " ('place', 67),\n",
       " ('idea', 66),\n",
       " (\"he's\", 66),\n",
       " ('narrative', 66),\n",
       " ('war', 65),\n",
       " ('trying', 65),\n",
       " ('done', 65),\n",
       " ('screenplay', 65),\n",
       " ('goes', 65),\n",
       " ('short', 65),\n",
       " ('now', 64),\n",
       " ('probably', 64),\n",
       " ('genre', 64),\n",
       " ('women', 64),\n",
       " (\"movie's\", 63),\n",
       " ('familiar', 63),\n",
       " ('plays', 63),\n",
       " ('premise', 63),\n",
       " ('under', 63),\n",
       " ('enjoyable', 62),\n",
       " ('turns', 62),\n",
       " ('horror', 62),\n",
       " ('set', 61),\n",
       " ('lacks', 61),\n",
       " ('becomes', 61),\n",
       " ('she', 61),\n",
       " ('home', 61),\n",
       " ('three', 61),\n",
       " ('engaging', 61),\n",
       " ('although', 61),\n",
       " ('filmmakers', 61),\n",
       " ('feature', 60),\n",
       " ('feeling', 60),\n",
       " ('smart', 60),\n",
       " ('worst', 60),\n",
       " ('effects', 59),\n",
       " ('power', 59),\n",
       " (\"won't\", 59),\n",
       " ('enjoy', 59),\n",
       " ('amusing', 58),\n",
       " ('intelligent', 58),\n",
       " ('study', 58),\n",
       " ('half', 58),\n",
       " ('strong', 58),\n",
       " ('charming', 58),\n",
       " ('ending', 58),\n",
       " ('effort', 58),\n",
       " ('especially', 58),\n",
       " ('day', 58),\n",
       " ('debut', 57),\n",
       " ('theater', 57),\n",
       " ('likely', 57),\n",
       " ('boring', 57),\n",
       " ('lack', 57),\n",
       " ('men', 56),\n",
       " ('romance', 56),\n",
       " ('john', 56),\n",
       " ('portrait', 56),\n",
       " ('each', 56),\n",
       " ('beautiful', 55),\n",
       " (\"what's\", 55),\n",
       " ('mostly', 55),\n",
       " ('put', 55),\n",
       " ('else', 55),\n",
       " ('sort', 55),\n",
       " ('version', 55),\n",
       " ('certainly', 55),\n",
       " ('easy', 54),\n",
       " ('beautifully', 54),\n",
       " ('next', 54),\n",
       " ('surprisingly', 54),\n",
       " ('looking', 54),\n",
       " ('message', 54),\n",
       " (\"you've\", 54),\n",
       " ('become', 54),\n",
       " ('level', 54),\n",
       " ('along', 53),\n",
       " ('hours', 53),\n",
       " ('play', 53),\n",
       " ('problem', 53),\n",
       " ('quirky', 53),\n",
       " ('lives', 53),\n",
       " ('wit', 53),\n",
       " ('mind', 53),\n",
       " ('fine', 53),\n",
       " ('solid', 53),\n",
       " ('exercise', 53),\n",
       " ('rare', 53),\n",
       " ('leave', 52),\n",
       " ('fresh', 52),\n",
       " ('face', 52),\n",
       " ('directed', 52),\n",
       " ('obvious', 52),\n",
       " ('mess', 52),\n",
       " ('ideas', 52),\n",
       " ('sure', 52),\n",
       " ('looks', 52),\n",
       " ('whether', 52),\n",
       " ('past', 51),\n",
       " ('completely', 51),\n",
       " ('powerful', 51),\n",
       " ('modern', 51),\n",
       " ('interest', 51),\n",
       " ('french', 51),\n",
       " ('either', 51),\n",
       " ('energy', 51),\n",
       " ('fact', 50),\n",
       " ('beyond', 50),\n",
       " ('reason', 50),\n",
       " ('classic', 50),\n",
       " ('melodrama', 50),\n",
       " ('shot', 50),\n",
       " ('dramatic', 50),\n",
       " ('viewers', 50),\n",
       " ('neither', 50),\n",
       " ('recent', 49),\n",
       " ('stuff', 49),\n",
       " ('delivers', 49),\n",
       " ('tone', 49),\n",
       " (\"didn't\", 49),\n",
       " ('shows', 49),\n",
       " ('children', 49),\n",
       " ('filmmaking', 49),\n",
       " ('suspense', 49),\n",
       " ('fails', 49),\n",
       " (\"i'm\", 48),\n",
       " ('slow', 48),\n",
       " ('truly', 48),\n",
       " ('culture', 48),\n",
       " ('intriguing', 48),\n",
       " ('left', 48),\n",
       " ('everyone', 48),\n",
       " ('believe', 48),\n",
       " ('deeply', 48),\n",
       " ('actor', 47),\n",
       " ('ends', 47),\n",
       " ('death', 47),\n",
       " ('himself', 47),\n",
       " ('serious', 47),\n",
       " ('jokes', 47),\n",
       " ('tries', 46),\n",
       " ('ride', 46),\n",
       " ('black', 46),\n",
       " ('book', 46),\n",
       " ('sad', 45),\n",
       " ('tv', 45),\n",
       " ('small', 45),\n",
       " ('production', 45),\n",
       " ('touching', 45),\n",
       " ('spirit', 45),\n",
       " ('occasionally', 45),\n",
       " ('light', 45),\n",
       " ('audiences', 45),\n",
       " ('proves', 44),\n",
       " ('role', 44),\n",
       " ('formula', 44),\n",
       " ('hilarious', 44),\n",
       " ('passion', 44),\n",
       " ('reality', 44),\n",
       " ('terrific', 44),\n",
       " ('remains', 44),\n",
       " ('camera', 44),\n",
       " ('exactly', 44),\n",
       " ('satisfying', 44),\n",
       " ('opera', 44),\n",
       " ('adventure', 44),\n",
       " ('storytelling', 44),\n",
       " ('dumb', 44),\n",
       " ('project', 43),\n",
       " ('filmmaker', 43),\n",
       " ('line', 43),\n",
       " ('scene', 43),\n",
       " ('nor', 43),\n",
       " ('impossible', 43),\n",
       " ('seeing', 43),\n",
       " ('stories', 43),\n",
       " ('different', 43)]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts.most_common(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dennybritz/projects/python-venvs/tensorflow/lib/python2.7/site-packages/gensim/models/word2vec.py:1296: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "  return word in self.vocab\n"
     ]
    }
   ],
   "source": [
    "# Use word2vec for initial embeddings (optional)\n",
    "\n",
    "# Start with zero embeddings\n",
    "# initial_embeddings = np.random.randn(len(vocabulary), 300)\n",
    "\n",
    "# # Load word2vec and get embeddings\n",
    "# w2v_model = Word2Vec.load_word2vec_format(\"/Users/dennybritz/Downloads/GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "# for idx, word in enumerate(vocabulary_inv):\n",
    "#     if word in w2v_model:\n",
    "#         initial_embeddings[idx,:] = w2v_model[word]\n",
    "\n",
    "# # Save initial embeddings to avoid loading word2vec every time\n",
    "# np.save(\"./data/rt-polaritydata/initial_embeddings\", initial_embeddings)\n",
    "\n",
    "# # Clear word2vec\n",
    "# w2v_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optional: Load initial embeddings\n",
    "initial_embeddings = np.load(\"./data/rt-polaritydata/initial_embeddings.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples: 10662\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "\n",
    "# Our training data\n",
    "x = np.array([[vocabulary[word] for word in sentence] for sentence in padded_sentences])\n",
    "y = np.array(y)\n",
    "\n",
    "# Randomly shuffle data\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]\n",
    "\n",
    "print(\"Training examples: {:d}\".format(len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_example(x, y):\n",
    "    text = \" \".join([vocabulary_inv[i] for i in x])\n",
    "    label = \"POS\" if y[1] == 1 else \"NEG\"\n",
    "    print(\"{}: {}\".format(y, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Do this properly?\n",
    "x_train, x_dev = x_shuffled[:-1000], x_shuffled[-1000:]\n",
    "y_train, y_dev = y_shuffled[:-1000], y_shuffled[-1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]: this is surely one of the most frantic , virulent and foul-natured christmas season pics ever delivered by a hollywood studio . <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n"
     ]
    }
   ],
   "source": [
    "print_example(x_train[2], y_train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Helper function to batch data\n",
    "def batch_iter(data, batch_size, num_epochs):\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int(len(data)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_data = data[shuffle_indices]      \n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 3]\n"
     ]
    }
   ],
   "source": [
    "# Test data!?\n",
    "with tf.Graph().as_default():\n",
    "    sess = tf.Session()\n",
    "    with sess.as_default():\n",
    "        x = tf.convert_to_tensor([[1,1,1,0,0,0,0],[1,1,1,0,0,0,0]])\n",
    "        mask = tf.argmin(x, 1)\n",
    "        print mask.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classifications\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, vocabulary_size, sequence_length, num_classes=2, embedding_size=300,\n",
    "        filter_sizes=[3, 4, 5], num_filters=100, affine_dim=256):\n",
    "        \n",
    "        # Placeholders for our input and output\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32)\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedded_chars = self._build_embedding(\n",
    "            [vocabulary_size, embedding_size], self.input_x)\n",
    "        # Add another dimension, expected by the convolutional layer\n",
    "        self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "\n",
    "        # Create a convolution + maxpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.variable_scope(\"filter-%s\" % filter_size):\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                pool_ksize = [1, sequence_length - filter_size + 1, 1, 1]\n",
    "                pooled = self._build_conv_maxpool(filter_shape, pool_ksize, self.embedded_chars_expanded)\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.h_pool = tf.concat(3, pooled_outputs)\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "\n",
    "        # Affine Layer with dropout\n",
    "        # self.h_affine = self._build_affine([num_filters_total, affine_dim], self.h_pool_flat)\n",
    "        # self.h_drop = tf.nn.dropout(self.h_affine, dropout_keep_prob)\n",
    "        \n",
    "        # Add dropout\n",
    "        self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "        \n",
    "        # Softmax Layer (Final output)\n",
    "        self.y = self._build_softmax([num_filters_total, num_classes], self.h_drop)\n",
    "        self.predictions = tf.argmax(self.y, 1, name=\"predictions\")\n",
    "\n",
    "        # Our loss expression\n",
    "        self.loss = self._build_mean_ce_loss(self.y, self.input_y)\n",
    "        # Expression for the accuracy\n",
    "        self.accuracy = self._build_accuracy(self.y, self.input_y)\n",
    "\n",
    "        # Summaries\n",
    "        total_loss_summary = tf.scalar_summary(\"loss\", self.loss)\n",
    "        accuracy_summmary = tf.scalar_summary(\"accuracy\", self.accuracy)\n",
    "\n",
    "    def _build_embedding(self, shape, input_tensor):\n",
    "        \"\"\"\n",
    "        Builds an embedding layer. Returns the embedded tensor.\n",
    "        \"\"\"\n",
    "        # We force this on the CPU because the op isn't implemented for the GPU\n",
    "        with tf.device('/cpu:0'), tf.variable_scope(\"embedding\"):\n",
    "            W_intializer = tf.random_uniform_initializer(-1.0, 1.0)\n",
    "            W_embeddings = tf.get_variable(\"W\", shape, initializer=W_intializer)\n",
    "            return tf.nn.embedding_lookup(W_embeddings, input_tensor)\n",
    "\n",
    "    def _build_affine(self, shape, input_tensor, activation_func=tf.nn.relu):\n",
    "        \"\"\"\n",
    "        Builds an affine (fully-connected) layer\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"affine\"):\n",
    "            W = tf.get_variable(\"W\", shape, initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "            b = tf.get_variable(\"b\", shape[-1], initializer=tf.constant_initializer(0.1))\n",
    "            h = activation_func(tf.matmul(input_tensor, W) + b, name=\"h\")\n",
    "        return h\n",
    "\n",
    "    def _build_softmax(self, shape, input_tensor):\n",
    "        \"\"\"\n",
    "        Builds a softmax layer\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"softmax\"):\n",
    "            W_initializer = tf.truncated_normal_initializer(stddev=0.1)\n",
    "            b_initializer = tf.constant_initializer(0.1)\n",
    "            W = tf.get_variable(\"W\", shape, initializer=W_initializer)\n",
    "            b = tf.get_variable(\"b\", shape[-1:], initializer=b_initializer)\n",
    "            return tf.nn.softmax(tf.nn.bias_add(tf.matmul(input_tensor, W), b), name=\"y\")\n",
    "    \n",
    "    def _build_mean_ce_loss(self, predictions, labels):\n",
    "        \"\"\"\n",
    "        Calculates the mean cross-entropy loss\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"mean-ce-loss\"):\n",
    "            return -tf.reduce_mean(labels * tf.log(predictions), name=\"mean_ce_loss\")\n",
    "        \n",
    "    def _build_accuracy(self, predictions, labels):\n",
    "        \"\"\"\n",
    "        Returns the accuracy\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(tf.argmax(predictions, 1), tf.argmax(labels, 1))\n",
    "            return tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "    \n",
    "    def _build_conv_maxpool(self, filter_shape, pool_shape, input_tensor):\n",
    "        \"\"\"\n",
    "        Builds a convolutional layer with ReLU activation followed by a  max-pooling layer.\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"conv-maxpool\"):\n",
    "            W = tf.get_variable(\"W\", filter_shape, initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "            conv = tf.nn.conv2d(input_tensor, W, strides=[1, 1, 1, 1], padding=\"VALID\")\n",
    "            begin = tf.to_int32(tf.zeros([4]))\n",
    "            b = tf.get_variable(\"b\", filter_shape[-1], initializer=tf.constant_initializer(0.1))\n",
    "            h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"conv\")\n",
    "            return tf.nn.max_pool(h, ksize=pool_shape, strides=[1, 1, 1, 1], padding='VALID', name=\"pool\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /Users/dennybritz/projects/wildml/cnn-text-classification-tf/runs/1449189839\n",
      "\n",
      "\n",
      "Dev Set:\n",
      "2015-12-04T09:45:37.519971: step 100, loss 0.48426, acc 0.506\n",
      "2015-12-04T09:45:43.162219: step 100, loss 0.44472, acc 0.541\n",
      "\n",
      "Saved model checkpoint to /Users/dennybritz/projects/wildml/cnn-text-classification-tf/runs/1449189839/checkpoints/model-100\n",
      "\n",
      "\n",
      "Dev Set:\n",
      "2015-12-04T09:47:22.212004: step 200, loss 0.467162, acc 0.531\n",
      "2015-12-04T09:47:27.342086: step 200, loss 0.403516, acc 0.578\n",
      "\n",
      "Saved model checkpoint to /Users/dennybritz/projects/wildml/cnn-text-classification-tf/runs/1449189839/checkpoints/model-200\n",
      "\n",
      "\n",
      "Dev Set:\n",
      "2015-12-04T09:49:05.575714: step 300, loss 0.460155, acc 0.557\n",
      "2015-12-04T09:49:10.936702: step 300, loss 0.378775, acc 0.591\n",
      "\n",
      "Saved model checkpoint to /Users/dennybritz/projects/wildml/cnn-text-classification-tf/runs/1449189839/checkpoints/model-300\n",
      "\n",
      "\n",
      "Dev Set:\n",
      "2015-12-04T09:50:48.187866: step 400, loss 0.446946, acc 0.555\n",
      "2015-12-04T09:50:54.736985: step 400, loss 0.3541, acc 0.614\n",
      "\n",
      "Saved model checkpoint to /Users/dennybritz/projects/wildml/cnn-text-classification-tf/runs/1449189839/checkpoints/model-400\n",
      "\n",
      "\n",
      "Dev Set:\n",
      "2015-12-04T09:52:33.564588: step 500, loss 0.440271, acc 0.565\n",
      "2015-12-04T09:52:38.663521: step 500, loss 0.334067, acc 0.646\n",
      "\n",
      "Saved model checkpoint to /Users/dennybritz/projects/wildml/cnn-text-classification-tf/runs/1449189839/checkpoints/model-500\n",
      "\n",
      "\n",
      "Dev Set:\n",
      "2015-12-04T09:54:10.541371: step 600, loss 0.427158, acc 0.573\n",
      "2015-12-04T09:54:15.961737: step 600, loss 0.312127, acc 0.667\n",
      "\n",
      "Saved model checkpoint to /Users/dennybritz/projects/wildml/cnn-text-classification-tf/runs/1449189839/checkpoints/model-600\n",
      "\n",
      "\n",
      "Dev Set:\n",
      "2015-12-04T09:55:55.165646: step 700, loss 0.418976, acc 0.57\n",
      "2015-12-04T09:56:00.206814: step 700, loss 0.298894, acc 0.692\n",
      "\n",
      "Saved model checkpoint to /Users/dennybritz/projects/wildml/cnn-text-classification-tf/runs/1449189839/checkpoints/model-700\n",
      "\n",
      "\n",
      "Dev Set:\n",
      "2015-12-04T09:57:41.814687: step 800, loss 0.40455, acc 0.575\n",
      "2015-12-04T09:57:47.079431: step 800, loss 0.279982, acc 0.715\n",
      "\n",
      "Saved model checkpoint to /Users/dennybritz/projects/wildml/cnn-text-classification-tf/runs/1449189839/checkpoints/model-800\n",
      "\n",
      "\n",
      "Dev Set:\n",
      "2015-12-04T09:59:15.651868: step 900, loss 0.396629, acc 0.571\n",
      "2015-12-04T09:59:20.765780: step 900, loss 0.271544, acc 0.732\n",
      "\n",
      "Saved model checkpoint to /Users/dennybritz/projects/wildml/cnn-text-classification-tf/runs/1449189839/checkpoints/model-900\n",
      "\n",
      "\n",
      "Dev Set:\n",
      "2015-12-04T10:00:51.775448: step 1000, loss 0.385867, acc 0.582\n",
      "2015-12-04T10:00:57.103389: step 1000, loss 0.257064, acc 0.736\n",
      "\n",
      "Saved model checkpoint to /Users/dennybritz/projects/wildml/cnn-text-classification-tf/runs/1449189839/checkpoints/model-1000\n",
      "\n",
      "\n",
      "Dev Set:\n",
      "2015-12-04T10:02:30.406720: step 1100, loss 0.377711, acc 0.576\n",
      "2015-12-04T10:02:35.592898: step 1100, loss 0.249027, acc 0.751\n",
      "\n",
      "Saved model checkpoint to /Users/dennybritz/projects/wildml/cnn-text-classification-tf/runs/1449189839/checkpoints/model-1100\n",
      "\n",
      "\n",
      "Dev Set:\n",
      "2015-12-04T10:04:07.337482: step 1200, loss 0.369378, acc 0.581\n",
      "2015-12-04T10:04:12.523269: step 1200, loss 0.24108, acc 0.76\n",
      "\n",
      "Saved model checkpoint to /Users/dennybritz/projects/wildml/cnn-text-classification-tf/runs/1449189839/checkpoints/model-1200\n",
      "\n",
      "\n",
      "Dev Set:\n",
      "2015-12-04T10:06:10.996343: step 1300, loss 0.364546, acc 0.582\n",
      "2015-12-04T10:06:18.311508: step 1300, loss 0.238382, acc 0.776\n",
      "\n",
      "Saved model checkpoint to /Users/dennybritz/projects/wildml/cnn-text-classification-tf/runs/1449189839/checkpoints/model-1300\n",
      "\n",
      "\n",
      "Dev Set:\n",
      "2015-12-04T10:07:56.403448: step 1400, loss 0.355595, acc 0.583\n",
      "2015-12-04T10:08:01.496800: step 1400, loss 0.230497, acc 0.785\n",
      "\n",
      "Saved model checkpoint to /Users/dennybritz/projects/wildml/cnn-text-classification-tf/runs/1449189839/checkpoints/model-1400\n",
      "\n",
      "\n",
      "Dev Set:\n",
      "2015-12-04T10:09:39.979834: step 1500, loss 0.350413, acc 0.592\n",
      "2015-12-04T10:09:45.855091: step 1500, loss 0.230002, acc 0.793\n",
      "\n",
      "Saved model checkpoint to /Users/dennybritz/projects/wildml/cnn-text-classification-tf/runs/1449189839/checkpoints/model-1500\n",
      "\n",
      "\n",
      "Dev Set:\n",
      "2015-12-04T10:11:27.656006: step 1600, loss 0.34366, acc 0.587\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-123-7cb948574b53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nDev Set:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0meval_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdev_summary_writer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                 \u001b[0meval_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcurrent_step\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mCHECKPOINT_EVERY\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-123-7cb948574b53>\u001b[0m in \u001b[0;36meval_loss\u001b[0;34m(x_batch, y_batch, writer)\u001b[0m\n\u001b[1;32m     57\u001b[0m             step, summaries, loss, accuracy = sess.run(\n\u001b[1;32m     58\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m                 feed_dict)\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0mtime_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misoformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}: step {}, loss {:g}, acc {:g}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dennybritz/projects/python-venvs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munique_fetch_targets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[0;31m# User may have fetched the same tensor multiple times, but we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dennybritz/projects/python-venvs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, target_list, fetch_list, feed_dict)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m       return tf_session.TF_Run(self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 404\u001b[0;31m                                target_list)\n\u001b[0m\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatusNotOK\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 500\n",
    "EVALUATE_EVERY = CHECKPOINT_EVERY = 100\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=True)      \n",
    "    sess = tf.Session(config=session_conf)  \n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(\n",
    "            vocabulary_size=len(vocabulary),\n",
    "            sequence_length=SEQUENCE_LENGTH,\n",
    "            num_classes=2,\n",
    "            embedding_size=300,\n",
    "            filter_sizes=[3, 4, 5],\n",
    "            num_filters=100)\n",
    "        \n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\")\n",
    "        optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        # Keep track of gradient values and sparsity\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                tf.histogram_summary(\"{}/grad\".format(v.name), g)\n",
    "                tf.scalar_summary(\"{}/grad-sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "        \n",
    "        \n",
    "        # Summary Writers\n",
    "        summary_op = tf.merge_all_summaries()\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.train.SummaryWriter(train_summary_dir, sess.graph_def)\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.train.SummaryWriter(dev_summary_dir, sess.graph_def)\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "        \n",
    "        # Checkpointing\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        # Tensorflow assumes this directory already exists so we need to create it\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.all_variables())\n",
    "        \n",
    "        # Initialize all variables\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        \n",
    "        # Optional: Initialize embeddings\n",
    "#         with tf.variable_scope(\"embedding\", reuse=True):\n",
    "#             embedding_W = tf.get_variable(\"W\")\n",
    "#         sess.run(embedding_W.assign(initial_embeddings))\n",
    "    \n",
    "        def eval_loss(x_batch, y_batch, writer=None):\n",
    "            feed_dict = {cnn.input_x: x_batch, cnn.input_y: y_batch, cnn.dropout_keep_prob: 1.0 }\n",
    "            step, summaries, loss, accuracy = sess.run(\n",
    "                [global_step, summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "        \n",
    "        # A single training step\n",
    "        def train_step(x_batch, y_batch):\n",
    "            feed_dict = { cnn.input_x: x_batch, cnn.input_y: y_batch, cnn.dropout_keep_prob: 0.25 }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            # Print and write metrics\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            # print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "        \n",
    "        batches = batch_iter(zip(x_train, y_train), BATCH_SIZE, NUM_EPOCHS)\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % EVALUATE_EVERY == 0:\n",
    "                print(\"\\nDev Set:\")\n",
    "                eval_loss(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                eval_loss(x_train[:1000], y_train[:1000], writer=None)\n",
    "                print(\"\")\n",
    "            if current_step % CHECKPOINT_EVERY == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))                \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
