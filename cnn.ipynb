{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import itertools\n",
    "import gc\n",
    "import gensim\n",
    "from collections import Counter\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load data from files\n",
    "positive_examples = list(open(\"./data/rt-polaritydata/rt-polarity.pos\").readlines())\n",
    "positive_examples = [s.strip() for s in positive_examples]\n",
    "negative_examples = list(open(\"./data/rt-polaritydata/rt-polarity.neg\").readlines())\n",
    "negative_examples = [s.strip() for s in negative_examples]\n",
    "# Generate labels\n",
    "positive_labels = [[0, 1] for _ in positive_examples]\n",
    "negative_labels = [[1, 0] for _ in negative_examples]\n",
    "# Concatenate positive and negative examples\n",
    "x_text = positive_examples + negative_examples\n",
    "y = positive_labels + negative_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10662"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Padding our sentences to the same length\n",
    "x_text_split = [s.split(\" \") for s in x_text]\n",
    "padded_sentences = [] \n",
    "# We pad all sentences to the maximum sentence length in the dataset\n",
    "SEQUENCE_LENGTH = max(len(x) for x in x_text_split)\n",
    "PADDING_WORD = \"<PAD/>\"\n",
    "for i in range(len(x_text_split)):\n",
    "    sentence = x_text_split[i]\n",
    "    num_padding = SEQUENCE_LENGTH - len(sentence)\n",
    "    new_sentence = [PADDING_WORD] * num_padding + sentence\n",
    "    padded_sentences.append(new_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEQUENCE_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build vocabulary\n",
    "word_counts = Counter(itertools.chain(*padded_sentences))\n",
    "# Mapping from word to index\n",
    "vocabulary = {x[0]: i for i, x in enumerate(word_counts.most_common())}\n",
    "# Mapping from index to word\n",
    "vocabulary_inv = [x[0] for x in word_counts.most_common()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21426"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument of type 'NoneType' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-205-e1d685e1110e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary_inv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mw2v_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0minitial_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw2v_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Save initial embeddings to avoid loading word2vec every time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data/rt-polaritydata/initial_embeddings\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: argument of type 'NoneType' is not iterable"
     ]
    }
   ],
   "source": [
    "# # Use word2vec for initial embeddings (optional)\n",
    "\n",
    "# Start with zero embeddings\n",
    "# initial_embeddings = np.zeros(len(vocabulary), 300)\n",
    "\n",
    "# Load word2vec and get embeddings\n",
    "# w2v_model = Word2Vec.load_word2vec_format(\"/Users/dennybritz/Downloads/GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "# for idx, word in enumerate(vocabulary_inv):\n",
    "#     if word in w2v_model:\n",
    "#         initial_embeddings[idx,:] = w2v_model[word]\n",
    "\n",
    "# # Save initial embeddings to avoid loading word2vec every time\n",
    "# np.save(\"./data/rt-polaritydata/initial_embeddings\", initial_embeddings)\n",
    "\n",
    "# # Clear word2vec\n",
    "# w2v_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optional: Load initial embeddings\n",
    "initial_embeddings = np.load(\"./data/rt-polaritydata/initial_embeddings.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples: 10662\n"
     ]
    }
   ],
   "source": [
    "# Our training data\n",
    "x = np.array([[vocabulary[word] for word in sentence] for sentence in padded_sentences])\n",
    "y = np.array(y)\n",
    "# Randomly shuffle data\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x = x[shuffle_indices]\n",
    "y = y[shuffle_indices]\n",
    "print(\"Training examples: {:d}\".format(len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Do this properly?\n",
    "x_train, x_dev, x_test = x[:-2000], x[-2000:-1000], x[-1000:]\n",
    "y_train, y_dev, y_test = y[:-2000], y[-2000:-1000], y[-1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Helper function to batch data\n",
    "def batch_iter(data, batch_size, num_epochs):\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int(len(data)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test data!?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CNN Implementation\n",
    "class CharCNN(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classifications\n",
    "    Embedding -> Convolutinal Layer -> Affine Layer -> Softmax Prediction\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, vocabulary_size, sequence_length, num_classes=2, embedding_size=128,\n",
    "            filter_sizes=[3, 4, 5], num_filters=100, affine_dim=256, dropout_keep_prob=0.5):\n",
    "        \n",
    "        # Placeholders for our input and output\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length])\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes])\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedded_chars = self._build_embedding([vocabulary_size, embedding_size], self.input_x)\n",
    "        # Add another dimension, expected by the convolutional layer\n",
    "        self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "\n",
    "        # Create a convolution + maxpool layer for each filter\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.variable_scope(\"filter-%s\" % filter_size):\n",
    "                # Define the shape of the filter\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                # We pool over the complete output of the convolution\n",
    "                pool_ksize = [1, sequence_length - filter_size + 1, 1, 1]\n",
    "                # Build the layer\n",
    "                pooled = self._build_conv_maxpool(filter_shape, pool_ksize, self.embedded_chars_expanded)\n",
    "                # Keep track of the layer\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.h_pool = tf.concat(3, pooled_outputs)\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "\n",
    "        # Affine Layer with dropout\n",
    "        self.h_affine = self._build_affine([num_filters_total, affine_dim], self.h_pool_flat)\n",
    "        self.h_affine_drop = tf.nn.dropout(self.h_affine, dropout_keep_prob)\n",
    "\n",
    "        # Softmax Layer (Final output)\n",
    "        self.y = self._build_softmax([affine_dim, num_classes], self.h_affine_drop)\n",
    "        self.predictions = tf.argmax(self.y, 1)\n",
    "        # self.y = self._build_softmax([num_filters_total, num_classes], self.h_pool_flat)\n",
    "        # self.predictions = tf.argmax(self.y, 1)        \n",
    "\n",
    "        # Loss\n",
    "        self.loss = self._build_mean_ce_loss(self.y, self.input_y)\n",
    "        # Accuracy\n",
    "        self.accuracy = self._build_accuracy(self.y, self.input_y)\n",
    "\n",
    "        # Summaries\n",
    "        total_loss_summary = tf.scalar_summary(\"loss\", self.loss)\n",
    "        accuracy_summmary = tf.scalar_summary(\"accuracy\", self.accuracy)\n",
    "\n",
    "    def _build_embedding(self, shape, input_tensor):\n",
    "        \"\"\"\n",
    "        Builds an embedding layer. Returns the embedded tensor.\n",
    "        \"\"\"\n",
    "        # We force this on the CPU because the op isn't implemented for the GPU\n",
    "        with tf.device('/cpu:0'):\n",
    "            W_intializer = tf.random_uniform_initializer(-1.0, 1.0)\n",
    "            W_embeddings = tf.get_variable(\"W\", shape, initializer=W_intializer)\n",
    "            return tf.nn.embedding_lookup(W_embeddings, input_tensor)\n",
    "\n",
    "    def _build_affine(self, shape, input_tensor, activation_func=tf.nn.relu):\n",
    "        \"\"\"\n",
    "        Builds an affine (fully-connected) layer\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"affine\"):\n",
    "            W = tf.Variable(tf.truncated_normal(shape, stddev=0.1), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=shape[-1:]), name=\"b\")\n",
    "            h = activation_func(tf.matmul(input_tensor, W) + b, name=\"h\")\n",
    "        return h\n",
    "\n",
    "    def _build_softmax(self, shape, input_tensor):\n",
    "        \"\"\"\n",
    "        Builds a softmax layer\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"softmax\"):\n",
    "            W_initializer = tf.truncated_normal_initializer(stddev=0.1)\n",
    "            b_initializer = tf.constant_initializer(0.1)\n",
    "            W = tf.get_variable(\"W\", shape, initializer=W_initializer)\n",
    "            b = tf.get_variable(\"b\", shape[-1:], initializer=b_initializer)\n",
    "            return tf.nn.softmax(tf.matmul(input_tensor, W) + b, name=\"y\")\n",
    "    \n",
    "    def _build_mean_ce_loss(self, predictions, labels):\n",
    "        \"\"\"\n",
    "        Calculates the mean cross-entropy loss\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"mean-ce-loss\"):\n",
    "            return -tf.reduce_mean(labels * tf.log(predictions), name=\"mean_ce_loss\")\n",
    "        \n",
    "    def _build_accuracy(self, predictions, labels):\n",
    "        \"\"\"\n",
    "        Returns accuracy tensor\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(tf.argmax(predictions, 1), tf.argmax(labels, 1))\n",
    "            return tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")                \n",
    "    \n",
    "    def _build_conv_maxpool(self, filter_shape, pool_shape, input_tensor):\n",
    "        \"\"\"\n",
    "        Builds a convolutional layer with ReLU activation followed by a  max-pooling layer.\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"conv-maxpool\"):\n",
    "            # Convolution Filter\n",
    "            W = tf.get_variable(\"W\", filter_shape, initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "            # Performs the convolution\n",
    "            conv = tf.nn.conv2d(input_tensor, W, strides=[1, 1, 1, 1], padding=\"VALID\")\n",
    "            # Bias term\n",
    "            b = tf.get_variable(\"b\", filter_shape[-1], initializer=tf.constant_initializer(0.1))\n",
    "            # Nonlinearity\n",
    "            h = tf.nn.relu(conv + b, name=\"conv\")\n",
    "            # Maxpooling\n",
    "            return tf.nn.max_pool(h, ksize=pool_shape, strides=[1, 1, 1, 1], padding='VALID', name=\"pool\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'assign'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-c9e57e77e4f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# Optional: Initialize embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0membedding_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tensor_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"embedding/W:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mprint_summaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'assign'"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "        allow_soft_placement=True,\n",
    "        log_device_placement=False)      \n",
    "    sess = tf.Session(config=session_conf)  \n",
    "    with sess.as_default():\n",
    "        cnn = CharCNN(len(vocabulary), SEQUENCE_LENGTH, 2, embedding_size=300)\n",
    "        \n",
    "        # Define Training procedure\n",
    "        # A variable to keep track of the global step. Tensorflow uses this automatically.\n",
    "        global_step = tf.Variable(0, name=\"global_step\")\n",
    "        # Our optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "        # We want to minmize the loss\n",
    "        train_op = optimizer.minimize(cnn.loss, global_step=global_step)\n",
    "        \n",
    "        # Summary Writer\n",
    "        summary_op = tf.merge_all_summaries()\n",
    "        out_dir = os.path.join(os.path.curdir, \"runs\", str(int(time.time())))        \n",
    "        train_summary_dir = os.path.abspath(os.path.join(out_dir, \"summaries\", \"train\"))\n",
    "        train_summary_writer = tf.train.SummaryWriter(train_summary_dir, sess.graph_def)\n",
    "        \n",
    "        # Checkpointing\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        # Tensorflow assumes this directory already exists so we need to create it\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.all_variables())\n",
    "        \n",
    "        # Initialize all variables\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        \n",
    "        # Optional: Initialize embeddings\n",
    "        # W_tensor = sess.graph.get_tensor_by_name(\"embedding/W:0\")\n",
    "        # sess.run(cnn.W_embeddings.assign(initial_embeddings))\n",
    "        \n",
    "        def print_summaries(summaries):\n",
    "            \"\"\"\n",
    "            Prints Event summary protocol buffers\n",
    "            \"\"\"\n",
    "            summary_obj = tf.Summary.FromString(summaries)\n",
    "            # Don't include summaries about queues\n",
    "            filtered_summaries = [v for v in summary_obj.value if \"queue/\" not in v.tag]\n",
    "            summary_str = \"\\n\".join([\"{}: {:f}\".format(v.tag, v.simple_value) for v in filtered_summaries])\n",
    "            print(\"\\n{}\\n\".format(summary_str))\n",
    "        \n",
    "        def eval_loss(x_batch, y_batch):\n",
    "            feed_dict = {cnn.input_x: x_batch, cnn.input_y: y_batch }\n",
    "            step, loss, accuracy = sess.run([global_step, cnn.loss, cnn.accuracy], feed_dict)\n",
    "            print(\"loss {:g}, acc: {:g}\".format(loss, accuracy))\n",
    "        \n",
    "        # A single training step\n",
    "        def train_step(x_batch, y_batch):\n",
    "            feed_dict = {\n",
    "                cnn.input_x: x_batch,\n",
    "                cnn.input_y: y_batch\n",
    "            }\n",
    "            # Run the graph\n",
    "            _, step, summaries = sess.run([train_op, global_step, summary_op], feed_dict)\n",
    "            # Print Step\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}\".format(time_str, step))\n",
    "            # print_summaries(summaries)\n",
    "            # Write summaries\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "            # Maybe checkpoint the model: TODO\n",
    "            return \n",
    "        \n",
    "        x_batches = batch_iter(x_train, 128, 20)\n",
    "        y_batches = batch_iter(y_train, 128, 20)\n",
    "        for x_batch, y_batch in zip(x_batches, y_batches):\n",
    "            train_step(x_batch, y_batch)\n",
    "            if tf.train.global_step(sess, global_step) % 50 == 0:\n",
    "                eval_loss(x_dev, y_dev)\n",
    "                eval_loss(x_train, y_train)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t = sess.graph.get_collection(tf.GraphKeys.VARIABLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(59), Dimension(300)])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.embedded_chars.get_shape()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
