{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import itertools\n",
    "import gc\n",
    "import gensim\n",
    "from collections import Counter\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load data from files\n",
    "positive_examples = list(open(\"./data/rt-polaritydata/rt-polarity.pos\").readlines())\n",
    "positive_examples = [s.strip() for s in positive_examples]\n",
    "negative_examples = list(open(\"./data/rt-polaritydata/rt-polarity.neg\").readlines())\n",
    "negative_examples = [s.strip() for s in negative_examples]\n",
    "\n",
    "# Generate labels\n",
    "positive_labels = [[0, 1] for _ in positive_examples]\n",
    "negative_labels = [[1, 0] for _ in negative_examples]\n",
    "# Concatenate positive and negative examples\n",
    "x_text = positive_examples + negative_examples\n",
    "y = np.concatenate([positive_labels, negative_labels], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at 90 minutes this movie is short , but it feels much longer .\n",
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "print x_text[len(positive_examples) + 242]\n",
    "print y[len(positive_examples) + 242]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Padding our sentences to the same length\n",
    "x_text_split = [s.split(\" \") for s in x_text]\n",
    "padded_sentences = [] \n",
    "# We pad all sentences to the maximum sentence length in the dataset\n",
    "SEQUENCE_LENGTH = max(len(x) for x in x_text_split)\n",
    "PADDING_WORD = \"<PAD/>\"\n",
    "for i in range(len(x_text_split)):\n",
    "    sentence = x_text_split[i]\n",
    "    num_padding = SEQUENCE_LENGTH - len(sentence)\n",
    "    new_sentence = sentence + [PADDING_WORD] * num_padding\n",
    "    padded_sentences.append(new_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build vocabulary\n",
    "word_counts = Counter(itertools.chain(*padded_sentences))\n",
    "# Mapping from index to word\n",
    "vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "# Mapping from word to index\n",
    "vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use word2vec for initial embeddings (optional)\n",
    "\n",
    "# Start with zero embeddings\n",
    "# initial_embeddings = np.random.randn(len(vocabulary), 300)\n",
    "\n",
    "# # Load word2vec and get embeddings\n",
    "# w2v_model = Word2Vec.load_word2vec_format(\"/Users/dennybritz/Downloads/GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "# for idx, word in enumerate(vocabulary_inv):\n",
    "#     if word in w2v_model:\n",
    "#         initial_embeddings[idx,:] = w2v_model[word]\n",
    "\n",
    "# # Save initial embeddings to avoid loading word2vec every time\n",
    "# np.save(\"./data/rt-polaritydata/initial_embeddings\", initial_embeddings)\n",
    "\n",
    "# # Clear word2vec\n",
    "# w2v_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optional: Load initial embeddings\n",
    "# initial_embeddings = np.load(\"./data/rt-polaritydata/initial_embeddings.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples: 10662\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "\n",
    "# Our training data\n",
    "x = np.array([[vocabulary[word] for word in sentence] for sentence in padded_sentences])\n",
    "y = np.array(y)\n",
    "\n",
    "# Randomly shuffle data\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]\n",
    "\n",
    "print(\"Training examples: {:d}\".format(len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_example(x, y):\n",
    "    text = \" \".join([vocabulary_inv[i] for i in x])\n",
    "    label = \"POS\" if y[1] == 1 else \"NEG\"\n",
    "    print(\"{}: {}\".format(y, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train, x_dev = x_shuffled[:-1000], x_shuffled[-1000:]\n",
    "y_train, y_dev = y_shuffled[:-1000], y_shuffled[-1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]: this is surely one of the most frantic , virulent and foul-natured christmas season pics ever delivered by a hollywood studio . <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n"
     ]
    }
   ],
   "source": [
    "print_example(x_train[2], y_train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Helper function to batch data\n",
    "def batch_iter(data, batch_size, num_epochs):\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int(len(data)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_data = data[shuffle_indices]      \n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, vocabulary_size, sequence_length, num_classes=2, embedding_size=128,\n",
    "        filter_sizes=[3, 4, 5], num_filters=100):\n",
    "        \n",
    "        # Placeholders for input and output and dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "        \n",
    "        # Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            W = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0), name=\"W\")\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(W, self.input_x)\n",
    "        self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "\n",
    "        # Create a convolution + maxpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.h_pool = tf.concat(3, pooled_outputs)\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "        \n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "        \n",
    "        # Final (unnormalized) scores and predictions\n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.Variable(tf.truncated_normal([num_filters_total, num_classes], stddev=0.1), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]))\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "\n",
    "        # Mean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(self.scores, self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses)\n",
    "        \n",
    "        # Expression for the accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /Users/dennybritz/projects/wildml/cnn-text-classification-tf/runs/1449728457\n",
      "\n",
      "2015-12-10T07:20:58.815154: step 1, loss 2.2056, acc 0.59375\n",
      "2015-12-10T07:20:59.060182: step 2, loss 2.92305, acc 0.484375\n",
      "2015-12-10T07:20:59.295441: step 3, loss 2.87611, acc 0.4375\n",
      "2015-12-10T07:20:59.531448: step 4, loss 3.06232, acc 0.46875\n",
      "2015-12-10T07:20:59.909716: step 5, loss 2.71052, acc 0.453125\n",
      "2015-12-10T07:21:00.280271: step 6, loss 2.47328, acc 0.53125\n",
      "2015-12-10T07:21:00.561111: step 7, loss 2.68373, acc 0.484375\n",
      "2015-12-10T07:21:00.781605: step 8, loss 2.8035, acc 0.40625\n",
      "2015-12-10T07:21:01.005022: step 9, loss 2.34087, acc 0.546875\n",
      "2015-12-10T07:21:01.217038: step 10, loss 3.15695, acc 0.453125\n",
      "2015-12-10T07:21:01.429059: step 11, loss 2.33409, acc 0.546875\n",
      "2015-12-10T07:21:01.652953: step 12, loss 2.56852, acc 0.515625\n",
      "2015-12-10T07:21:01.869522: step 13, loss 2.11229, acc 0.515625\n",
      "2015-12-10T07:21:02.112597: step 14, loss 2.79021, acc 0.484375\n",
      "2015-12-10T07:21:02.341191: step 15, loss 3.09202, acc 0.515625\n",
      "2015-12-10T07:21:02.561904: step 16, loss 2.313, acc 0.484375\n",
      "2015-12-10T07:21:02.781572: step 17, loss 2.21003, acc 0.53125\n",
      "2015-12-10T07:21:03.002268: step 18, loss 2.26798, acc 0.484375\n",
      "2015-12-10T07:21:03.216737: step 19, loss 2.16117, acc 0.5\n",
      "2015-12-10T07:21:03.437005: step 20, loss 2.19084, acc 0.46875\n",
      "2015-12-10T07:21:03.661220: step 21, loss 1.52736, acc 0.5625\n",
      "2015-12-10T07:21:03.887766: step 22, loss 1.92918, acc 0.5625\n",
      "2015-12-10T07:21:04.105735: step 23, loss 2.10172, acc 0.46875\n",
      "2015-12-10T07:21:04.324190: step 24, loss 2.4457, acc 0.375\n",
      "2015-12-10T07:21:04.539164: step 25, loss 1.6628, acc 0.53125\n",
      "2015-12-10T07:21:04.781872: step 26, loss 2.43133, acc 0.453125\n",
      "2015-12-10T07:21:05.016748: step 27, loss 1.88691, acc 0.484375\n",
      "2015-12-10T07:21:05.233910: step 28, loss 1.71177, acc 0.546875\n",
      "2015-12-10T07:21:05.442161: step 29, loss 1.77245, acc 0.578125\n",
      "2015-12-10T07:21:05.653785: step 30, loss 2.21806, acc 0.53125\n",
      "2015-12-10T07:21:05.861332: step 31, loss 1.83306, acc 0.5\n",
      "2015-12-10T07:21:06.072475: step 32, loss 2.62679, acc 0.421875\n",
      "2015-12-10T07:21:06.374011: step 33, loss 1.7113, acc 0.515625\n",
      "2015-12-10T07:21:06.879811: step 34, loss 2.08959, acc 0.484375\n",
      "2015-12-10T07:21:07.116138: step 35, loss 2.24541, acc 0.453125\n",
      "2015-12-10T07:21:07.352211: step 36, loss 1.91069, acc 0.59375\n",
      "2015-12-10T07:21:07.595563: step 37, loss 1.9441, acc 0.59375\n",
      "2015-12-10T07:21:07.836407: step 38, loss 2.15686, acc 0.546875\n",
      "2015-12-10T07:21:08.115118: step 39, loss 2.09824, acc 0.453125\n",
      "2015-12-10T07:21:08.416260: step 40, loss 2.01924, acc 0.453125\n",
      "2015-12-10T07:21:08.638806: step 41, loss 2.02831, acc 0.46875\n",
      "2015-12-10T07:21:08.866239: step 42, loss 1.81698, acc 0.515625\n",
      "2015-12-10T07:21:09.087943: step 43, loss 1.90641, acc 0.46875\n",
      "2015-12-10T07:21:09.308771: step 44, loss 1.25083, acc 0.609375\n",
      "2015-12-10T07:21:09.543417: step 45, loss 1.85009, acc 0.59375\n",
      "2015-12-10T07:21:09.776594: step 46, loss 2.35126, acc 0.40625\n",
      "2015-12-10T07:21:09.996306: step 47, loss 1.97767, acc 0.546875\n",
      "2015-12-10T07:21:10.209381: step 48, loss 2.12831, acc 0.4375\n",
      "2015-12-10T07:21:10.438683: step 49, loss 2.05137, acc 0.625\n",
      "2015-12-10T07:21:10.660839: step 50, loss 1.72455, acc 0.53125\n",
      "2015-12-10T07:21:10.895230: step 51, loss 2.23366, acc 0.484375\n",
      "2015-12-10T07:21:11.106817: step 52, loss 2.39889, acc 0.4375\n",
      "2015-12-10T07:21:11.320558: step 53, loss 2.15124, acc 0.46875\n",
      "2015-12-10T07:21:11.577051: step 54, loss 2.18139, acc 0.4375\n",
      "2015-12-10T07:21:11.810685: step 55, loss 1.77103, acc 0.484375\n",
      "2015-12-10T07:21:12.042173: step 56, loss 2.04658, acc 0.609375\n",
      "2015-12-10T07:21:12.256158: step 57, loss 1.4573, acc 0.546875\n",
      "2015-12-10T07:21:12.476316: step 58, loss 2.1503, acc 0.515625\n",
      "2015-12-10T07:21:12.693099: step 59, loss 1.85927, acc 0.5625\n",
      "2015-12-10T07:21:12.923825: step 60, loss 2.21814, acc 0.484375\n",
      "2015-12-10T07:21:13.155447: step 61, loss 1.60493, acc 0.546875\n",
      "2015-12-10T07:21:13.400728: step 62, loss 1.68115, acc 0.625\n",
      "2015-12-10T07:21:13.678170: step 63, loss 1.50119, acc 0.53125\n",
      "2015-12-10T07:21:13.920262: step 64, loss 2.39065, acc 0.453125\n",
      "2015-12-10T07:21:14.157842: step 65, loss 1.98006, acc 0.53125\n",
      "2015-12-10T07:21:14.409233: step 66, loss 1.8954, acc 0.484375\n",
      "2015-12-10T07:21:14.987147: step 67, loss 2.5345, acc 0.4375\n",
      "2015-12-10T07:21:15.293603: step 68, loss 2.6406, acc 0.40625\n",
      "2015-12-10T07:21:15.771883: step 69, loss 1.51713, acc 0.5625\n",
      "2015-12-10T07:21:16.034121: step 70, loss 2.22769, acc 0.453125\n",
      "2015-12-10T07:21:16.433387: step 71, loss 2.24127, acc 0.484375\n",
      "2015-12-10T07:21:16.756282: step 72, loss 1.81342, acc 0.546875\n",
      "2015-12-10T07:21:17.160199: step 73, loss 2.08096, acc 0.484375\n",
      "2015-12-10T07:21:17.486396: step 74, loss 1.88973, acc 0.5\n",
      "2015-12-10T07:21:17.832660: step 75, loss 1.85336, acc 0.515625\n",
      "2015-12-10T07:21:18.117107: step 76, loss 2.1299, acc 0.53125\n",
      "2015-12-10T07:21:18.370066: step 77, loss 2.37495, acc 0.46875\n",
      "2015-12-10T07:21:18.654315: step 78, loss 2.02174, acc 0.375\n",
      "2015-12-10T07:21:18.939425: step 79, loss 2.38826, acc 0.46875\n",
      "2015-12-10T07:21:19.159522: step 80, loss 2.16144, acc 0.484375\n",
      "2015-12-10T07:21:19.382906: step 81, loss 2.64719, acc 0.5\n",
      "2015-12-10T07:21:19.605228: step 82, loss 2.20574, acc 0.4375\n",
      "2015-12-10T07:21:19.837759: step 83, loss 2.57346, acc 0.484375\n",
      "2015-12-10T07:21:20.075697: step 84, loss 2.17249, acc 0.421875\n",
      "2015-12-10T07:21:20.346534: step 85, loss 1.54336, acc 0.46875\n",
      "2015-12-10T07:21:20.721300: step 86, loss 1.41753, acc 0.578125\n",
      "2015-12-10T07:21:21.052770: step 87, loss 2.41093, acc 0.46875\n",
      "2015-12-10T07:21:21.337384: step 88, loss 1.63012, acc 0.5625\n",
      "2015-12-10T07:21:21.596304: step 89, loss 1.89421, acc 0.5\n",
      "2015-12-10T07:21:21.892086: step 90, loss 1.42906, acc 0.625\n",
      "2015-12-10T07:21:22.194047: step 91, loss 2.35419, acc 0.421875\n",
      "2015-12-10T07:21:22.515519: step 92, loss 1.3181, acc 0.625\n",
      "2015-12-10T07:21:22.812737: step 93, loss 1.61355, acc 0.578125\n",
      "2015-12-10T07:21:23.065644: step 94, loss 2.45128, acc 0.40625\n",
      "2015-12-10T07:21:23.289153: step 95, loss 0.935401, acc 0.640625\n",
      "2015-12-10T07:21:23.505246: step 96, loss 2.01024, acc 0.453125\n",
      "2015-12-10T07:21:23.905414: step 97, loss 2.0988, acc 0.5\n",
      "2015-12-10T07:21:24.297544: step 98, loss 2.13619, acc 0.5625\n",
      "2015-12-10T07:21:24.620975: step 99, loss 1.97487, acc 0.40625\n",
      "2015-12-10T07:21:24.852036: step 100, loss 1.88647, acc 0.5\n",
      "\n",
      "Dev Set:\n",
      "2015-12-10T07:21:25.711042: step 100, loss 0.827686, acc 0.512\n",
      "\n",
      "Saved model checkpoint to /Users/dennybritz/projects/wildml/cnn-text-classification-tf/runs/1449728457/checkpoints/model-100\n",
      "\n",
      "2015-12-10T07:21:26.388135: step 101, loss 2.04099, acc 0.46875\n",
      "2015-12-10T07:21:26.724815: step 102, loss 1.88403, acc 0.4375\n",
      "2015-12-10T07:21:26.954655: step 103, loss 1.84406, acc 0.453125\n",
      "2015-12-10T07:21:27.186428: step 104, loss 2.3205, acc 0.4375\n",
      "2015-12-10T07:21:27.409921: step 105, loss 2.92878, acc 0.359375\n",
      "2015-12-10T07:21:27.648991: step 106, loss 1.46316, acc 0.53125\n",
      "2015-12-10T07:21:28.074260: step 107, loss 1.54079, acc 0.59375\n",
      "2015-12-10T07:21:28.306198: step 108, loss 1.27852, acc 0.609375\n",
      "2015-12-10T07:21:28.528474: step 109, loss 2.39705, acc 0.40625\n",
      "2015-12-10T07:21:28.779831: step 110, loss 1.65706, acc 0.5625\n",
      "2015-12-10T07:21:29.147385: step 111, loss 2.45467, acc 0.46875\n",
      "2015-12-10T07:21:29.365000: step 112, loss 2.60916, acc 0.4375\n",
      "2015-12-10T07:21:29.590322: step 113, loss 1.74476, acc 0.640625\n",
      "2015-12-10T07:21:30.003817: step 114, loss 1.69121, acc 0.578125\n",
      "2015-12-10T07:21:30.226976: step 115, loss 1.25771, acc 0.609375\n",
      "2015-12-10T07:21:30.438888: step 116, loss 2.08826, acc 0.453125\n",
      "2015-12-10T07:21:30.664857: step 117, loss 1.81049, acc 0.515625\n",
      "2015-12-10T07:21:30.878282: step 118, loss 1.71596, acc 0.5625\n",
      "2015-12-10T07:21:31.271717: step 119, loss 2.31547, acc 0.46875\n",
      "2015-12-10T07:21:31.485293: step 120, loss 2.22625, acc 0.484375\n",
      "2015-12-10T07:21:31.701541: step 121, loss 2.34007, acc 0.46875\n",
      "2015-12-10T07:21:31.914331: step 122, loss 2.86427, acc 0.390625\n",
      "2015-12-10T07:21:32.311819: step 123, loss 1.52171, acc 0.53125\n",
      "2015-12-10T07:21:32.528137: step 124, loss 1.49237, acc 0.546875\n",
      "2015-12-10T07:21:32.749273: step 125, loss 1.74628, acc 0.578125\n",
      "2015-12-10T07:21:32.959770: step 126, loss 1.98377, acc 0.515625\n",
      "2015-12-10T07:21:33.171118: step 127, loss 1.58169, acc 0.515625\n",
      "2015-12-10T07:21:33.390701: step 128, loss 1.90595, acc 0.484375\n",
      "2015-12-10T07:21:33.603332: step 129, loss 1.28842, acc 0.546875\n",
      "2015-12-10T07:21:33.827519: step 130, loss 1.9791, acc 0.546875\n",
      "2015-12-10T07:21:34.044677: step 131, loss 2.1618, acc 0.453125\n",
      "2015-12-10T07:21:34.262778: step 132, loss 1.75492, acc 0.5\n",
      "2015-12-10T07:21:34.479903: step 133, loss 1.90796, acc 0.546875\n",
      "2015-12-10T07:21:34.695468: step 134, loss 1.51181, acc 0.578125\n",
      "2015-12-10T07:21:34.914811: step 135, loss 1.88283, acc 0.53125\n",
      "2015-12-10T07:21:35.341538: step 136, loss 1.87936, acc 0.40625\n",
      "2015-12-10T07:21:35.705562: step 137, loss 1.75156, acc 0.515625\n",
      "2015-12-10T07:21:36.005747: step 138, loss 2.80094, acc 0.390625\n",
      "2015-12-10T07:21:36.399892: step 139, loss 2.16246, acc 0.484375\n",
      "2015-12-10T07:21:36.780274: step 140, loss 1.78037, acc 0.546875\n",
      "2015-12-10T07:21:37.277593: step 141, loss 2.00093, acc 0.515625\n",
      "2015-12-10T07:21:37.525737: step 142, loss 1.83989, acc 0.484375\n",
      "2015-12-10T07:21:37.738656: step 143, loss 1.57818, acc 0.5625\n",
      "2015-12-10T07:21:37.957696: step 144, loss 1.70315, acc 0.515625\n",
      "2015-12-10T07:21:38.175019: step 145, loss 1.65177, acc 0.5625\n",
      "2015-12-10T07:21:38.397963: step 146, loss 1.52803, acc 0.515625\n",
      "2015-12-10T07:21:38.619564: step 147, loss 1.38103, acc 0.578125\n",
      "2015-12-10T07:21:38.835743: step 148, loss 1.74705, acc 0.515625\n",
      "2015-12-10T07:21:39.230432: step 149, loss 2.59557, acc 0.453125\n",
      "2015-12-10T07:21:39.446869: step 150, loss 1.79121, acc 0.4375\n",
      "2015-12-10T07:21:39.653869: step 151, loss 1.80204, acc 0.5\n",
      "2015-12-10T07:21:39.864777: step 152, loss 1.5169, acc 0.515625\n",
      "2015-12-10T07:21:40.065881: step 153, loss 2.75378, acc 0.390625\n",
      "2015-12-10T07:21:40.286889: step 154, loss 1.41201, acc 0.578125\n",
      "2015-12-10T07:21:40.494100: step 155, loss 1.5348, acc 0.5\n",
      "2015-12-10T07:21:40.707350: step 156, loss 1.54643, acc 0.53125\n",
      "2015-12-10T07:21:40.922433: step 157, loss 1.94125, acc 0.453125\n",
      "2015-12-10T07:21:41.287296: step 158, loss 1.96618, acc 0.484375\n",
      "2015-12-10T07:21:41.654952: step 159, loss 1.53983, acc 0.578125\n",
      "2015-12-10T07:21:41.872703: step 160, loss 1.54845, acc 0.609375\n",
      "2015-12-10T07:21:42.083803: step 161, loss 1.57658, acc 0.609375\n",
      "2015-12-10T07:21:42.378560: step 162, loss 2.38705, acc 0.5625\n",
      "2015-12-10T07:21:42.760098: step 163, loss 1.96819, acc 0.4375\n",
      "2015-12-10T07:21:42.985968: step 164, loss 1.30406, acc 0.609375\n",
      "2015-12-10T07:21:43.206163: step 165, loss 1.42062, acc 0.53125\n",
      "2015-12-10T07:21:43.430912: step 166, loss 1.57412, acc 0.609375\n",
      "2015-12-10T07:21:43.680525: step 167, loss 1.93393, acc 0.53125\n",
      "2015-12-10T07:21:43.906163: step 168, loss 1.44541, acc 0.546875\n",
      "2015-12-10T07:21:44.126501: step 169, loss 2.01152, acc 0.515625\n",
      "2015-12-10T07:21:44.363309: step 170, loss 2.13636, acc 0.5\n",
      "2015-12-10T07:21:44.600642: step 171, loss 1.54554, acc 0.578125\n",
      "2015-12-10T07:21:44.819649: step 172, loss 2.18275, acc 0.421875\n",
      "2015-12-10T07:21:45.201799: step 173, loss 1.90793, acc 0.5\n",
      "2015-12-10T07:21:45.433105: step 174, loss 1.67806, acc 0.484375\n",
      "2015-12-10T07:21:45.647627: step 175, loss 2.14601, acc 0.375\n",
      "2015-12-10T07:21:45.856779: step 176, loss 1.00908, acc 0.71875\n",
      "2015-12-10T07:21:46.069927: step 177, loss 1.98487, acc 0.421875\n",
      "2015-12-10T07:21:46.281081: step 178, loss 1.86585, acc 0.546875\n",
      "2015-12-10T07:21:46.514337: step 179, loss 1.43591, acc 0.609375\n",
      "2015-12-10T07:21:46.739784: step 180, loss 2.13158, acc 0.53125\n",
      "2015-12-10T07:21:46.964269: step 181, loss 2.36401, acc 0.46875\n",
      "2015-12-10T07:21:47.187528: step 182, loss 2.25714, acc 0.484375\n",
      "2015-12-10T07:21:47.402756: step 183, loss 2.02615, acc 0.46875\n",
      "2015-12-10T07:21:47.622517: step 184, loss 1.90292, acc 0.5\n",
      "2015-12-10T07:21:47.834600: step 185, loss 1.72726, acc 0.53125\n",
      "2015-12-10T07:21:48.057467: step 186, loss 1.58146, acc 0.546875\n",
      "2015-12-10T07:21:48.409197: step 187, loss 1.90028, acc 0.546875\n",
      "2015-12-10T07:21:48.677368: step 188, loss 1.57243, acc 0.5625\n",
      "2015-12-10T07:21:48.932605: step 189, loss 1.65578, acc 0.515625\n",
      "2015-12-10T07:21:49.184256: step 190, loss 1.69638, acc 0.515625\n",
      "2015-12-10T07:21:49.442582: step 191, loss 1.9418, acc 0.46875\n",
      "2015-12-10T07:21:49.711991: step 192, loss 2.47347, acc 0.453125\n",
      "2015-12-10T07:21:50.107969: step 193, loss 2.28616, acc 0.421875\n",
      "2015-12-10T07:21:50.454687: step 194, loss 1.70608, acc 0.578125\n",
      "2015-12-10T07:21:50.754170: step 195, loss 1.95748, acc 0.453125\n",
      "2015-12-10T07:21:51.035753: step 196, loss 1.9208, acc 0.390625\n",
      "2015-12-10T07:21:51.369175: step 197, loss 2.16676, acc 0.578125\n",
      "2015-12-10T07:21:51.715557: step 198, loss 1.91518, acc 0.515625\n",
      "2015-12-10T07:21:52.095362: step 199, loss 1.6916, acc 0.59375\n",
      "2015-12-10T07:21:52.430523: step 200, loss 2.17375, acc 0.484375\n",
      "\n",
      "Dev Set:\n",
      "2015-12-10T07:21:53.146959: step 200, loss 0.81573, acc 0.525\n",
      "\n",
      "Saved model checkpoint to /Users/dennybritz/projects/wildml/cnn-text-classification-tf/runs/1449728457/checkpoints/model-200\n",
      "\n",
      "2015-12-10T07:21:53.510120: step 201, loss 2.05355, acc 0.53125\n",
      "2015-12-10T07:21:53.795570: step 202, loss 1.95223, acc 0.546875\n",
      "2015-12-10T07:21:54.061260: step 203, loss 2.30816, acc 0.40625\n",
      "2015-12-10T07:21:54.324948: step 204, loss 0.748069, acc 0.71875\n",
      "2015-12-10T07:21:54.602168: step 205, loss 1.5236, acc 0.578125\n",
      "2015-12-10T07:21:54.930798: step 206, loss 1.97187, acc 0.5\n",
      "2015-12-10T07:21:55.206087: step 207, loss 1.59185, acc 0.453125\n",
      "2015-12-10T07:21:55.494581: step 208, loss 1.62354, acc 0.4375\n",
      "2015-12-10T07:21:55.798064: step 209, loss 2.2278, acc 0.46875\n",
      "2015-12-10T07:21:56.035500: step 210, loss 1.73944, acc 0.578125\n",
      "2015-12-10T07:21:56.260504: step 211, loss 1.83003, acc 0.4375\n",
      "2015-12-10T07:21:56.498924: step 212, loss 1.8502, acc 0.515625\n",
      "2015-12-10T07:21:56.723464: step 213, loss 2.02622, acc 0.5\n",
      "2015-12-10T07:21:56.949327: step 214, loss 1.53757, acc 0.5625\n",
      "2015-12-10T07:21:57.172744: step 215, loss 2.02237, acc 0.453125\n",
      "2015-12-10T07:21:57.408155: step 216, loss 1.86911, acc 0.515625\n",
      "2015-12-10T07:21:57.730582: step 217, loss 1.70742, acc 0.640625\n",
      "2015-12-10T07:21:58.075514: step 218, loss 1.87823, acc 0.53125\n",
      "2015-12-10T07:21:58.347335: step 219, loss 2.15256, acc 0.53125\n",
      "2015-12-10T07:21:58.657753: step 220, loss 2.13648, acc 0.375\n",
      "2015-12-10T07:21:58.901663: step 221, loss 1.46558, acc 0.53125\n",
      "2015-12-10T07:21:59.116461: step 222, loss 2.08929, acc 0.515625\n",
      "2015-12-10T07:21:59.389478: step 223, loss 1.22144, acc 0.640625\n",
      "2015-12-10T07:21:59.687391: step 224, loss 1.92228, acc 0.4375\n",
      "2015-12-10T07:21:59.960654: step 225, loss 1.55462, acc 0.5\n",
      "2015-12-10T07:22:00.262913: step 226, loss 1.97976, acc 0.5625\n",
      "2015-12-10T07:22:00.534475: step 227, loss 1.79859, acc 0.578125\n",
      "2015-12-10T07:22:00.872014: step 228, loss 1.71882, acc 0.578125\n",
      "2015-12-10T07:22:01.181674: step 229, loss 2.23057, acc 0.5\n",
      "2015-12-10T07:22:01.490505: step 230, loss 1.83631, acc 0.5625\n",
      "2015-12-10T07:22:01.787812: step 231, loss 1.53671, acc 0.515625\n",
      "2015-12-10T07:22:02.080022: step 232, loss 2.17336, acc 0.484375\n",
      "2015-12-10T07:22:02.389238: step 233, loss 1.92673, acc 0.46875"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 500\n",
    "EVALUATE_EVERY = CHECKPOINT_EVERY = 100\n",
    "DROPOUT_KEEP_PROB = 0.5\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=True)      \n",
    "    sess = tf.Session(config=session_conf)  \n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(\n",
    "            vocabulary_size=len(vocabulary),\n",
    "            sequence_length=SEQUENCE_LENGTH,\n",
    "            num_classes=2,\n",
    "            embedding_size=128,\n",
    "            filter_sizes=[3, 4, 5],\n",
    "            num_filters=80)\n",
    "        \n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\")\n",
    "        optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "        \n",
    "        # Keep track of gradient values and sparsity\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.histogram_summary(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.scalar_summary(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.merge_summary(grad_summaries)\n",
    "        \n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "        \n",
    "        # Train Summaries\n",
    "        loss_summary = tf.scalar_summary(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.scalar_summary(\"accuracy\", cnn.accuracy)\n",
    "        train_summary_op = tf.merge_summary([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.train.SummaryWriter(train_summary_dir, sess.graph_def)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.merge_summary([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.train.SummaryWriter(dev_summary_dir, sess.graph_def)\n",
    "       \n",
    "        # Checkpointing\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        # Tensorflow assumes this directory already exists so we need to create it\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.all_variables())\n",
    "        \n",
    "        # Initialize all variables\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "    \n",
    "        # A single training step\n",
    "        def train_step(x_batch, y_batch):\n",
    "            feed_dict = { cnn.input_x: x_batch, cnn.input_y: y_batch, cnn.dropout_keep_prob: DROPOUT_KEEP_PROB }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "            \n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            feed_dict = {cnn.input_x: x_batch, cnn.input_y: y_batch, cnn.dropout_keep_prob: 1.0 }\n",
    "            step, summaries, loss, accuracy = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "        \n",
    "        batches = batch_iter(zip(x_train, y_train), BATCH_SIZE, NUM_EPOCHS)\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % EVALUATE_EVERY == 0:\n",
    "                print(\"\\nDev Set:\")\n",
    "                dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % CHECKPOINT_EVERY == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))                \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
