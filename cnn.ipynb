{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import data_helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x, y, vocabulary, vocabulary_inv = data_helpers.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "# Randomly shuffle data\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]\n",
    "\n",
    "# Split train/test set\n",
    "# TODO: This is very crude, should use cross-validation\n",
    "x_train, x_dev = x_shuffled[:-1000], x_shuffled[-1000:]\n",
    "y_train, y_dev = y_shuffled[:-1000], y_shuffled[-1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, vocabulary_size, sequence_length, num_classes=2, embedding_size=128,\n",
    "        filter_sizes=[3, 4, 5], num_filters=100):\n",
    "        \n",
    "        # Placeholders for input and output and dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "        \n",
    "        # Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            W = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0), name=\"W\")\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(W, self.input_x)\n",
    "        self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "\n",
    "        # Create a convolution + maxpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.h_pool = tf.concat(3, pooled_outputs)\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "        \n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "        \n",
    "        # Final (unnormalized) scores and predictions\n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.Variable(tf.truncated_normal([num_filters_total, num_classes], stddev=0.1), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]))\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "\n",
    "        # Mean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(self.scores, self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses)\n",
    "        \n",
    "        # Expression for the accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /Users/dennybritz/projects/wildml/cnn-text-classification-tf/runs/1449731188\n",
      "\n",
      "2015-12-10T08:06:29.306352: step 1, loss 2.46018, acc 0.453125\n",
      "2015-12-10T08:06:29.598443: step 2, loss 2.21895, acc 0.5\n",
      "2015-12-10T08:06:29.860732: step 3, loss 2.13484, acc 0.484375\n",
      "2015-12-10T08:06:30.127405: step 4, loss 1.67236, acc 0.53125\n",
      "2015-12-10T08:06:30.365780: step 5, loss 1.8633, acc 0.46875\n",
      "2015-12-10T08:06:30.591764: step 6, loss 2.51452, acc 0.4375\n",
      "2015-12-10T08:06:30.820956: step 7, loss 1.81396, acc 0.546875\n",
      "2015-12-10T08:06:31.047919: step 8, loss 1.48453, acc 0.625\n",
      "2015-12-10T08:06:31.271223: step 9, loss 1.64171, acc 0.515625\n",
      "2015-12-10T08:06:31.490915: step 10, loss 2.04186, acc 0.484375\n",
      "2015-12-10T08:06:31.719753: step 11, loss 1.545, acc 0.5625\n",
      "2015-12-10T08:06:31.947065: step 12, loss 1.76173, acc 0.5\n",
      "2015-12-10T08:06:32.184173: step 13, loss 2.009, acc 0.5625\n",
      "2015-12-10T08:06:32.678610: step 14, loss 1.74997, acc 0.5\n",
      "2015-12-10T08:06:32.898872: step 15, loss 1.85376, acc 0.484375\n",
      "2015-12-10T08:06:33.129410: step 16, loss 2.06608, acc 0.421875\n",
      "2015-12-10T08:06:33.352990: step 17, loss 2.05908, acc 0.40625\n",
      "2015-12-10T08:06:33.568814: step 18, loss 1.53481, acc 0.609375\n",
      "2015-12-10T08:06:33.966724: step 19, loss 1.75167, acc 0.453125\n",
      "2015-12-10T08:06:34.199915: step 20, loss 1.8768, acc 0.484375\n",
      "2015-12-10T08:06:34.430715: step 21, loss 2.02672, acc 0.515625\n",
      "2015-12-10T08:06:34.668238: step 22, loss 2.62306, acc 0.359375\n",
      "2015-12-10T08:06:34.898771: step 23, loss 2.4976, acc 0.46875\n",
      "2015-12-10T08:06:35.141056: step 24, loss 1.9045, acc 0.53125\n",
      "2015-12-10T08:06:35.442518: step 25, loss 1.86477, acc 0.484375\n",
      "2015-12-10T08:06:35.917372: step 26, loss 2.01345, acc 0.53125\n",
      "2015-12-10T08:06:36.270662: step 27, loss 1.58342, acc 0.5\n",
      "2015-12-10T08:06:36.594082: step 28, loss 1.46447, acc 0.625\n",
      "2015-12-10T08:06:36.843645: step 29, loss 1.91554, acc 0.46875\n",
      "2015-12-10T08:06:37.099219: step 30, loss 2.35094, acc 0.453125\n",
      "2015-12-10T08:06:37.378166: step 31, loss 1.55706, acc 0.609375\n",
      "2015-12-10T08:06:37.617977: step 32, loss 1.77472, acc 0.5625\n",
      "2015-12-10T08:06:37.852673: step 33, loss 1.82913, acc 0.5\n",
      "2015-12-10T08:06:38.113902: step 34, loss 1.89228, acc 0.5\n",
      "2015-12-10T08:06:38.360605: step 35, loss 1.7216, acc 0.515625\n",
      "2015-12-10T08:06:38.693700: step 36, loss 1.84934, acc 0.53125\n",
      "2015-12-10T08:06:38.912596: step 37, loss 2.20053, acc 0.46875\n",
      "2015-12-10T08:06:39.131556: step 38, loss 1.93602, acc 0.453125\n",
      "2015-12-10T08:06:39.351524: step 39, loss 1.78381, acc 0.546875\n",
      "2015-12-10T08:06:39.573666: step 40, loss 2.16407, acc 0.453125\n",
      "2015-12-10T08:06:39.799326: step 41, loss 1.74869, acc 0.515625\n",
      "2015-12-10T08:06:40.026732: step 42, loss 2.10843, acc 0.53125\n",
      "2015-12-10T08:06:40.249169: step 43, loss 2.36475, acc 0.4375\n",
      "2015-12-10T08:06:40.466527: step 44, loss 1.97451, acc 0.46875\n",
      "2015-12-10T08:06:40.697166: step 45, loss 1.2655, acc 0.625\n",
      "2015-12-10T08:06:40.916238: step 46, loss 1.67045, acc 0.5\n",
      "2015-12-10T08:06:41.190008: step 47, loss 1.81203, acc 0.53125\n",
      "2015-12-10T08:06:41.439115: step 48, loss 1.84231, acc 0.40625\n",
      "2015-12-10T08:06:41.662825: step 49, loss 1.64688, acc 0.484375\n",
      "2015-12-10T08:06:41.909682: step 50, loss 2.33481, acc 0.390625\n",
      "2015-12-10T08:06:42.148458: step 51, loss 1.94383, acc 0.609375\n",
      "2015-12-10T08:06:42.367233: step 52, loss 1.45707, acc 0.640625\n",
      "2015-12-10T08:06:42.588671: step 53, loss 1.87757, acc 0.4375\n",
      "2015-12-10T08:06:42.798427: step 54, loss 1.49219, acc 0.53125\n",
      "2015-12-10T08:06:43.006300: step 55, loss 2.50546, acc 0.328125\n",
      "2015-12-10T08:06:43.223451: step 56, loss 1.8912, acc 0.484375\n",
      "2015-12-10T08:06:43.446728: step 57, loss 1.69635, acc 0.546875\n",
      "2015-12-10T08:06:43.679238: step 58, loss 1.71926, acc 0.578125\n",
      "2015-12-10T08:06:43.917496: step 59, loss 1.6713, acc 0.546875\n",
      "2015-12-10T08:06:44.147988: step 60, loss 1.45248, acc 0.53125\n",
      "2015-12-10T08:06:44.364821: step 61, loss 1.49105, acc 0.53125\n",
      "2015-12-10T08:06:44.590713: step 62, loss 2.3499, acc 0.46875\n",
      "2015-12-10T08:06:44.833172: step 63, loss 1.67899, acc 0.5625\n",
      "2015-12-10T08:06:45.060873: step 64, loss 1.55599, acc 0.53125\n",
      "2015-12-10T08:06:45.285670: step 65, loss 1.55625, acc 0.53125\n",
      "2015-12-10T08:06:45.501847: step 66, loss 1.84224, acc 0.4375\n",
      "2015-12-10T08:06:45.740156: step 67, loss 2.05234, acc 0.453125\n",
      "2015-12-10T08:06:45.968434: step 68, loss 2.54451, acc 0.40625\n",
      "2015-12-10T08:06:46.188667: step 69, loss 1.715, acc 0.5625\n",
      "2015-12-10T08:06:46.438630: step 70, loss 1.85156, acc 0.515625\n",
      "2015-12-10T08:06:46.659550: step 71, loss 1.77689, acc 0.53125\n",
      "2015-12-10T08:06:46.909821: step 72, loss 1.77499, acc 0.5\n",
      "2015-12-10T08:06:47.139790: step 73, loss 1.60717, acc 0.453125\n",
      "2015-12-10T08:06:47.387584: step 74, loss 1.27829, acc 0.59375\n",
      "2015-12-10T08:06:47.624335: step 75, loss 1.62127, acc 0.484375\n",
      "2015-12-10T08:06:47.862677: step 76, loss 1.81566, acc 0.5\n",
      "2015-12-10T08:06:48.075071: step 77, loss 1.38278, acc 0.609375\n",
      "2015-12-10T08:06:48.308549: step 78, loss 1.89563, acc 0.484375\n",
      "2015-12-10T08:06:48.542982: step 79, loss 1.53016, acc 0.546875\n",
      "2015-12-10T08:06:48.764987: step 80, loss 2.11937, acc 0.5\n",
      "2015-12-10T08:06:48.991775: step 81, loss 2.20384, acc 0.453125\n",
      "2015-12-10T08:06:49.229048: step 82, loss 2.11399, acc 0.453125\n",
      "2015-12-10T08:06:49.444451: step 83, loss 2.37755, acc 0.421875\n",
      "2015-12-10T08:06:49.663585: step 84, loss 1.93945, acc 0.453125\n",
      "2015-12-10T08:06:49.886733: step 85, loss 1.84532, acc 0.546875\n",
      "2015-12-10T08:06:50.110443: step 86, loss 2.39983, acc 0.359375\n",
      "2015-12-10T08:06:50.335039: step 87, loss 1.63934, acc 0.515625\n",
      "2015-12-10T08:06:50.552803: step 88, loss 1.58286, acc 0.53125\n",
      "2015-12-10T08:06:50.775962: step 89, loss 1.69703, acc 0.53125\n",
      "2015-12-10T08:06:50.993040: step 90, loss 1.86477, acc 0.46875\n",
      "2015-12-10T08:06:51.203466: step 91, loss 2.09703, acc 0.421875\n",
      "2015-12-10T08:06:51.427615: step 92, loss 1.82659, acc 0.5\n",
      "2015-12-10T08:06:51.646614: step 93, loss 1.36081, acc 0.546875\n",
      "2015-12-10T08:06:51.856029: step 94, loss 2.02761, acc 0.515625\n",
      "2015-12-10T08:06:52.066757: step 95, loss 1.91081, acc 0.578125\n",
      "2015-12-10T08:06:52.287414: step 96, loss 1.98799, acc 0.53125\n",
      "2015-12-10T08:06:52.516840: step 97, loss 1.99249, acc 0.46875\n",
      "2015-12-10T08:06:52.737316: step 98, loss 2.1926, acc 0.4375\n",
      "2015-12-10T08:06:52.964359: step 99, loss 1.6951, acc 0.546875\n",
      "2015-12-10T08:06:53.187193: step 100, loss 1.54266, acc 0.515625\n",
      "\n",
      "Dev Set:\n",
      "2015-12-10T08:06:53.930673: step 100, loss 0.89458, acc 0.479\n",
      "\n",
      "Saved model checkpoint to /Users/dennybritz/projects/wildml/cnn-text-classification-tf/runs/1449731188/checkpoints/model-100\n",
      "\n",
      "2015-12-10T08:06:54.395149: step 101, loss 2.41534, acc 0.34375\n",
      "2015-12-10T08:06:54.651519: step 102, loss 1.42507, acc 0.484375\n",
      "2015-12-10T08:06:54.874181: step 103, loss 1.72486, acc 0.578125\n",
      "2015-12-10T08:06:55.094667: step 104, loss 1.37905, acc 0.53125\n",
      "2015-12-10T08:06:55.311297: step 105, loss 1.71646, acc 0.546875\n",
      "2015-12-10T08:06:55.530503: step 106, loss 1.4857, acc 0.546875\n",
      "2015-12-10T08:06:55.743967: step 107, loss 1.99927, acc 0.46875\n",
      "2015-12-10T08:06:56.036784: step 108, loss 1.68477, acc 0.5\n",
      "2015-12-10T08:06:56.436795: step 109, loss 1.86528, acc 0.515625\n",
      "2015-12-10T08:06:56.704990: step 110, loss 1.63576, acc 0.53125\n",
      "2015-12-10T08:06:56.927559: step 111, loss 1.97865, acc 0.453125\n",
      "2015-12-10T08:06:57.150744: step 112, loss 1.16342, acc 0.578125\n",
      "2015-12-10T08:06:57.368816: step 113, loss 2.1234, acc 0.5625\n",
      "2015-12-10T08:06:57.582335: step 114, loss 1.55195, acc 0.5625\n",
      "2015-12-10T08:06:57.839498: step 115, loss 1.61722, acc 0.53125\n",
      "2015-12-10T08:06:58.059759: step 116, loss 1.95596, acc 0.484375\n",
      "2015-12-10T08:06:58.280209: step 117, loss 1.77148, acc 0.515625\n",
      "2015-12-10T08:06:58.506744: step 118, loss 1.79579, acc 0.484375\n",
      "2015-12-10T08:06:58.726278: step 119, loss 1.52783, acc 0.59375\n",
      "2015-12-10T08:06:58.954066: step 120, loss 1.94821, acc 0.40625\n",
      "2015-12-10T08:06:59.195448: step 121, loss 2.27453, acc 0.453125\n",
      "2015-12-10T08:06:59.437331: step 122, loss 2.09714, acc 0.484375\n",
      "2015-12-10T08:06:59.659122: step 123, loss 2.07543, acc 0.5625\n",
      "2015-12-10T08:06:59.875124: step 124, loss 1.26938, acc 0.5625\n",
      "2015-12-10T08:07:00.108044: step 125, loss 2.14415, acc 0.453125\n",
      "2015-12-10T08:07:00.322136: step 126, loss 1.72188, acc 0.46875\n",
      "2015-12-10T08:07:00.534097: step 127, loss 1.56693, acc 0.578125\n",
      "2015-12-10T08:07:00.756777: step 128, loss 2.21266, acc 0.421875\n",
      "2015-12-10T08:07:00.990009: step 129, loss 1.65044, acc 0.484375\n",
      "2015-12-10T08:07:01.197250: step 130, loss 1.6173, acc 0.53125\n",
      "2015-12-10T08:07:01.424484: step 131, loss 1.40474, acc 0.46875\n",
      "2015-12-10T08:07:01.652074: step 132, loss 2.17915, acc 0.40625\n",
      "2015-12-10T08:07:01.874897: step 133, loss 2.09444, acc 0.453125\n",
      "2015-12-10T08:07:02.080650: step 134, loss 1.92296, acc 0.453125\n",
      "2015-12-10T08:07:02.326378: step 135, loss 1.49525, acc 0.578125\n",
      "2015-12-10T08:07:02.539454: step 136, loss 1.86422, acc 0.421875\n",
      "2015-12-10T08:07:02.746691: step 137, loss 1.50674, acc 0.46875\n",
      "2015-12-10T08:07:02.972721: step 138, loss 1.23949, acc 0.671875\n",
      "2015-12-10T08:07:03.198904: step 139, loss 1.81173, acc 0.453125\n",
      "2015-12-10T08:07:03.424747: step 140, loss 1.91305, acc 0.484375\n",
      "2015-12-10T08:07:03.640791: step 141, loss 1.51416, acc 0.5625\n",
      "2015-12-10T08:07:03.850531: step 142, loss 2.24946, acc 0.375\n",
      "2015-12-10T08:07:04.093057: step 143, loss 1.50057, acc 0.578125\n",
      "2015-12-10T08:07:04.302383: step 144, loss 1.89639, acc 0.375\n",
      "2015-12-10T08:07:04.555831: step 145, loss 2.08067, acc 0.484375\n",
      "2015-12-10T08:07:04.763859: step 146, loss 1.39349, acc 0.515625\n",
      "2015-12-10T08:07:04.973786: step 147, loss 1.39653, acc 0.59375\n",
      "2015-12-10T08:07:05.221218: step 148, loss 1.86453, acc 0.453125\n",
      "2015-12-10T08:07:05.427571: step 149, loss 1.53033, acc 0.5625\n",
      "2015-12-10T08:07:05.651437: step 150, loss 1.67778, acc 0.578125\n",
      "2015-12-10T08:07:05.856501: step 151, loss 1.46886, acc 0.548387\n",
      "2015-12-10T08:07:06.091161: step 152, loss 1.97438, acc 0.53125\n",
      "2015-12-10T08:07:06.294731: step 153, loss 1.65974, acc 0.53125\n",
      "2015-12-10T08:07:06.524206: step 154, loss 2.2226, acc 0.453125\n",
      "2015-12-10T08:07:06.742614: step 155, loss 1.63506, acc 0.546875\n",
      "2015-12-10T08:07:06.944228: step 156, loss 1.58051, acc 0.546875\n",
      "2015-12-10T08:07:07.162218: step 157, loss 1.54319, acc 0.546875\n",
      "2015-12-10T08:07:07.375550: step 158, loss 1.93929, acc 0.546875\n",
      "2015-12-10T08:07:07.596191: step 159, loss 1.43057, acc 0.515625\n",
      "2015-12-10T08:07:07.810278: step 160, loss 1.86142, acc 0.453125\n",
      "2015-12-10T08:07:08.018603: step 161, loss 1.64841, acc 0.453125\n",
      "2015-12-10T08:07:08.243799: step 162, loss 1.73155, acc 0.53125\n",
      "2015-12-10T08:07:08.476099: step 163, loss 1.82383, acc 0.484375\n",
      "2015-12-10T08:07:08.696330: step 164, loss 1.52101, acc 0.453125\n",
      "2015-12-10T08:07:08.914004: step 165, loss 1.71007, acc 0.421875\n",
      "2015-12-10T08:07:09.146267: step 166, loss 2.06865, acc 0.4375\n",
      "2015-12-10T08:07:09.529864: step 167, loss 1.9061, acc 0.546875\n",
      "2015-12-10T08:07:09.836974: step 168, loss 1.8002, acc 0.578125\n",
      "2015-12-10T08:07:10.124159: step 169, loss 1.71605, acc 0.65625\n",
      "2015-12-10T08:07:10.368520: step 170, loss 1.56652, acc 0.5\n",
      "2015-12-10T08:07:10.589741: step 171, loss 1.47408, acc 0.53125\n",
      "2015-12-10T08:07:10.802358: step 172, loss 1.57157, acc 0.546875\n",
      "2015-12-10T08:07:11.005400: step 173, loss 1.20984, acc 0.609375\n",
      "2015-12-10T08:07:11.255091: step 174, loss 1.79486, acc 0.453125\n",
      "2015-12-10T08:07:11.500573: step 175, loss 1.51066, acc 0.546875\n",
      "2015-12-10T08:07:11.822123: step 176, loss 1.09353, acc 0.625\n",
      "2015-12-10T08:07:12.041313: step 177, loss 1.53303, acc 0.578125\n",
      "2015-12-10T08:07:12.255091: step 178, loss 1.39279, acc 0.515625\n",
      "2015-12-10T08:07:12.490080: step 179, loss 1.77225, acc 0.453125\n",
      "2015-12-10T08:07:12.732436: step 180, loss 1.84309, acc 0.46875\n",
      "2015-12-10T08:07:12.984003: step 181, loss 1.67828, acc 0.515625\n",
      "2015-12-10T08:07:13.253924: step 182, loss 1.61764, acc 0.484375\n",
      "2015-12-10T08:07:13.485621: step 183, loss 1.65448, acc 0.5625\n",
      "2015-12-10T08:07:13.732067: step 184, loss 1.41125, acc 0.53125\n",
      "2015-12-10T08:07:13.966422: step 185, loss 1.57063, acc 0.546875\n",
      "2015-12-10T08:07:14.197932: step 186, loss 1.44249, acc 0.484375\n",
      "2015-12-10T08:07:14.477745: step 187, loss 1.6158, acc 0.515625\n",
      "2015-12-10T08:07:14.691163: step 188, loss 1.50914, acc 0.53125\n",
      "2015-12-10T08:07:14.907052: step 189, loss 1.55985, acc 0.5\n",
      "2015-12-10T08:07:15.129172: step 190, loss 1.74852, acc 0.53125\n",
      "2015-12-10T08:07:15.337895: step 191, loss 1.92153, acc 0.46875\n",
      "2015-12-10T08:07:15.555117: step 192, loss 1.43273, acc 0.609375\n",
      "2015-12-10T08:07:15.781417: step 193, loss 1.90058, acc 0.4375\n",
      "2015-12-10T08:07:15.993634: step 194, loss 1.90783, acc 0.46875\n",
      "2015-12-10T08:07:16.234436: step 195, loss 1.95543, acc 0.46875\n",
      "2015-12-10T08:07:16.450418: step 196, loss 2.28411, acc 0.4375\n",
      "2015-12-10T08:07:16.702738: step 197, loss 1.7501, acc 0.453125\n",
      "2015-12-10T08:07:16.942597: step 198, loss 1.60829, acc 0.5\n",
      "2015-12-10T08:07:17.160085: step 199, loss 1.60035, acc 0.5\n",
      "2015-12-10T08:07:17.388409: step 200, loss 1.89127, acc 0.4375\n",
      "\n",
      "Dev Set:\n",
      "2015-12-10T08:07:18.185227: step 200, loss 0.793558, acc 0.519\n",
      "\n",
      "Saved model checkpoint to /Users/dennybritz/projects/wildml/cnn-text-classification-tf/runs/1449731188/checkpoints/model-200\n",
      "\n",
      "2015-12-10T08:07:18.598479: step 201, loss 1.59192, acc 0.5\n",
      "2015-12-10T08:07:18.830140: step 202, loss 1.8183, acc 0.484375\n",
      "2015-12-10T08:07:19.044698: step 203, loss 1.84775, acc 0.46875\n",
      "2015-12-10T08:07:19.260353: step 204, loss 1.5594, acc 0.46875\n",
      "2015-12-10T08:07:19.488996: step 205, loss 1.56644, acc 0.53125\n",
      "2015-12-10T08:07:19.708109: step 206, loss 2.37926, acc 0.375\n",
      "2015-12-10T08:07:19.928799: step 207, loss 1.56467, acc 0.578125\n",
      "2015-12-10T08:07:20.197867: step 208, loss 1.37426, acc 0.625\n",
      "2015-12-10T08:07:20.455113: step 209, loss 1.81922, acc 0.453125\n",
      "2015-12-10T08:07:20.856889: step 210, loss 1.21652, acc 0.65625\n",
      "2015-12-10T08:07:21.095199: step 211, loss 1.64292, acc 0.625\n",
      "2015-12-10T08:07:21.308032: step 212, loss 1.98449, acc 0.390625\n",
      "2015-12-10T08:07:21.529551: step 213, loss 1.4531, acc 0.59375\n",
      "2015-12-10T08:07:21.751517: step 214, loss 1.91489, acc 0.46875\n",
      "2015-12-10T08:07:21.958189: step 215, loss 1.99709, acc 0.40625\n",
      "2015-12-10T08:07:22.169493: step 216, loss 1.51909, acc 0.53125\n",
      "2015-12-10T08:07:22.383473: step 217, loss 1.78674, acc 0.453125\n",
      "2015-12-10T08:07:22.601822: step 218, loss 1.80236, acc 0.46875\n",
      "2015-12-10T08:07:22.817234: step 219, loss 1.49377, acc 0.5625\n",
      "2015-12-10T08:07:23.019002: step 220, loss 1.56002, acc 0.546875\n",
      "2015-12-10T08:07:23.223790: step 221, loss 1.95973, acc 0.53125\n",
      "2015-12-10T08:07:23.445582: step 222, loss 1.68533, acc 0.640625\n",
      "2015-12-10T08:07:23.662146: step 223, loss 1.17689, acc 0.609375\n",
      "2015-12-10T08:07:23.892639: step 224, loss 1.47911, acc 0.546875\n",
      "2015-12-10T08:07:24.121526: step 225, loss 1.3985, acc 0.53125\n",
      "2015-12-10T08:07:24.326892: step 226, loss 1.92969, acc 0.53125\n",
      "2015-12-10T08:07:24.538956: step 227, loss 1.47833, acc 0.546875\n",
      "2015-12-10T08:07:24.760856: step 228, loss 1.61924, acc 0.4375\n",
      "2015-12-10T08:07:24.983723: step 229, loss 1.87278, acc 0.390625\n",
      "2015-12-10T08:07:25.198079: step 230, loss 1.56228, acc 0.546875\n",
      "2015-12-10T08:07:25.435885: step 231, loss 1.57658, acc 0.609375\n",
      "2015-12-10T08:07:25.684612: step 232, loss 1.68742, acc 0.515625\n",
      "2015-12-10T08:07:25.892783: step 233, loss 1.59746, acc 0.5625\n",
      "2015-12-10T08:07:26.114470: step 234, loss 1.63152, acc 0.46875\n",
      "2015-12-10T08:07:26.338465: step 235, loss 1.44965, acc 0.59375\n",
      "2015-12-10T08:07:26.549705: step 236, loss 1.45986, acc 0.5625\n",
      "2015-12-10T08:07:26.772914: step 237, loss 1.95852, acc 0.515625\n",
      "2015-12-10T08:07:26.991288: step 238, loss 1.64662, acc 0.515625\n",
      "2015-12-10T08:07:27.205659: step 239, loss 1.84678, acc 0.46875\n",
      "2015-12-10T08:07:27.438763: step 240, loss 1.42252, acc 0.5625\n",
      "2015-12-10T08:07:27.656685: step 241, loss 2.37917, acc 0.40625\n",
      "2015-12-10T08:07:27.868758: step 242, loss 1.937, acc 0.484375\n",
      "2015-12-10T08:07:28.079912: step 243, loss 1.93645, acc 0.515625\n",
      "2015-12-10T08:07:28.304722: step 244, loss 1.76102, acc 0.421875\n",
      "2015-12-10T08:07:28.529989: step 245, loss 1.75819, acc 0.53125\n",
      "2015-12-10T08:07:28.740162: step 246, loss 1.19762, acc 0.59375\n",
      "2015-12-10T08:07:28.957531: step 247, loss 1.92794, acc 0.40625\n",
      "2015-12-10T08:07:29.215355: step 248, loss 1.73424, acc 0.5\n",
      "2015-12-10T08:07:29.440472: step 249, loss 2.22505, acc 0.34375\n",
      "2015-12-10T08:07:29.663034: step 250, loss 1.67617, acc 0.46875\n",
      "2015-12-10T08:07:29.872363: step 251, loss 1.82911, acc 0.46875\n",
      "2015-12-10T08:07:30.087817: step 252, loss 1.8648, acc 0.453125\n",
      "2015-12-10T08:07:30.291785: step 253, loss 1.59573, acc 0.515625\n",
      "2015-12-10T08:07:30.498336: step 254, loss 1.43085, acc 0.59375\n",
      "2015-12-10T08:07:30.725765: step 255, loss 1.28916, acc 0.609375\n",
      "2015-12-10T08:07:30.951508: step 256, loss 2.11929, acc 0.4375\n",
      "2015-12-10T08:07:31.162087: step 257, loss 1.74912, acc 0.53125\n",
      "2015-12-10T08:07:31.385863: step 258, loss 2.05802, acc 0.4375\n",
      "2015-12-10T08:07:31.597853: step 259, loss 1.36728, acc 0.59375\n",
      "2015-12-10T08:07:31.808804: step 260, loss 1.70041, acc 0.53125\n",
      "2015-12-10T08:07:32.014828: step 261, loss 1.24958, acc 0.640625\n",
      "2015-12-10T08:07:32.245387: step 262, loss 2.2947, acc 0.421875\n",
      "2015-12-10T08:07:32.467865: step 263, loss 1.58464, acc 0.5625\n",
      "2015-12-10T08:07:32.691179: step 264, loss 1.41263, acc 0.5625\n",
      "2015-12-10T08:07:32.941549: step 265, loss 1.52907, acc 0.546875\n",
      "2015-12-10T08:07:33.149034: step 266, loss 1.43119, acc 0.546875\n",
      "2015-12-10T08:07:33.396568: step 267, loss 1.41831, acc 0.578125\n",
      "2015-12-10T08:07:33.623056: step 268, loss 1.58321, acc 0.5\n",
      "2015-12-10T08:07:33.853925: step 269, loss 2.03587, acc 0.390625\n",
      "2015-12-10T08:07:34.062063: step 270, loss 1.36799, acc 0.53125\n",
      "2015-12-10T08:07:34.280981: step 271, loss 1.31032, acc 0.59375\n",
      "2015-12-10T08:07:34.497028: step 272, loss 1.88097, acc 0.40625\n",
      "2015-12-10T08:07:34.707416: step 273, loss 1.29955, acc 0.5625\n",
      "2015-12-10T08:07:34.921472: step 274, loss 1.38286, acc 0.578125\n",
      "2015-12-10T08:07:35.143334: step 275, loss 1.54441, acc 0.5\n",
      "2015-12-10T08:07:35.367312: step 276, loss 2.23879, acc 0.4375\n",
      "2015-12-10T08:07:35.586483: step 277, loss 1.94891, acc 0.4375\n",
      "2015-12-10T08:07:35.826048: step 278, loss 1.7177, acc 0.546875\n",
      "2015-12-10T08:07:36.033890: step 279, loss 1.75431, acc 0.484375\n",
      "2015-12-10T08:07:36.254181: step 280, loss 1.76376, acc 0.421875\n",
      "2015-12-10T08:07:36.477136: step 281, loss 1.35653, acc 0.53125\n",
      "2015-12-10T08:07:36.690277: step 282, loss 1.57323, acc 0.453125\n",
      "2015-12-10T08:07:36.899226: step 283, loss 1.67622, acc 0.46875\n",
      "2015-12-10T08:07:37.120358: step 284, loss 0.967791, acc 0.65625\n",
      "2015-12-10T08:07:37.341262: step 285, loss 1.63266, acc 0.578125\n",
      "2015-12-10T08:07:37.563759: step 286, loss 1.93091, acc 0.46875\n",
      "2015-12-10T08:07:37.784898: step 287, loss 1.30016, acc 0.46875\n",
      "2015-12-10T08:07:37.982784: step 288, loss 1.64621, acc 0.53125\n",
      "2015-12-10T08:07:38.185607: step 289, loss 1.42454, acc 0.546875\n",
      "2015-12-10T08:07:38.410857: step 290, loss 1.53983, acc 0.53125\n",
      "2015-12-10T08:07:38.618277: step 291, loss 1.17749, acc 0.625\n",
      "2015-12-10T08:07:38.833068: step 292, loss 1.64322, acc 0.46875\n",
      "2015-12-10T08:07:39.046062: step 293, loss 1.39219, acc 0.625\n",
      "2015-12-10T08:07:39.266411: step 294, loss 1.55716, acc 0.5625\n",
      "2015-12-10T08:07:39.482700: step 295, loss 1.44913, acc 0.578125\n",
      "2015-12-10T08:07:39.716558: step 296, loss 1.42478, acc 0.53125\n",
      "2015-12-10T08:07:39.944432: step 297, loss 1.38346, acc 0.546875\n",
      "2015-12-10T08:07:40.159403: step 298, loss 1.87466, acc 0.53125\n",
      "2015-12-10T08:07:40.384157: step 299, loss 1.46468, acc 0.546875\n",
      "2015-12-10T08:07:40.595050: step 300, loss 1.75123, acc 0.5625\n",
      "\n",
      "Dev Set:\n",
      "2015-12-10T08:07:41.295111: step 300, loss 0.784833, acc 0.519\n",
      "\n",
      "Saved model checkpoint to /Users/dennybritz/projects/wildml/cnn-text-classification-tf/runs/1449731188/checkpoints/model-300\n",
      "\n",
      "2015-12-10T08:07:41.701239: step 301, loss 1.43023, acc 0.453125\n",
      "2015-12-10T08:07:41.915951: step 302, loss 1.69305, acc 0.516129\n",
      "2015-12-10T08:07:42.124121: step 303, loss 1.49901, acc 0.53125\n",
      "2015-12-10T08:07:42.349457: step 304, loss 1.44986, acc 0.5625\n",
      "2015-12-10T08:07:42.569246: step 305, loss 1.24509, acc 0.53125\n",
      "2015-12-10T08:07:43.026502: step 306, loss 1.30024, acc 0.59375\n",
      "2015-12-10T08:07:43.293755: step 307, loss 1.48918, acc 0.546875\n",
      "2015-12-10T08:07:43.545264: step 308, loss 1.38811, acc 0.515625\n",
      "2015-12-10T08:07:43.780886: step 309, loss 1.14178, acc 0.59375\n",
      "2015-12-10T08:07:44.002061: step 310, loss 1.46599, acc 0.515625\n",
      "2015-12-10T08:07:44.213236: step 311, loss 1.72697, acc 0.484375\n",
      "2015-12-10T08:07:44.425517: step 312, loss 1.77576, acc 0.46875\n",
      "2015-12-10T08:07:44.632110: step 313, loss 1.26065, acc 0.5625\n",
      "2015-12-10T08:07:44.886627: step 314, loss 1.59599, acc 0.4375\n",
      "2015-12-10T08:07:45.089685: step 315, loss 1.2763, acc 0.625\n",
      "2015-12-10T08:07:45.291586: step 316, loss 1.39629, acc 0.5625\n",
      "2015-12-10T08:07:45.511070: step 317, loss 1.77967, acc 0.546875\n",
      "2015-12-10T08:07:45.738830: step 318, loss 1.47946, acc 0.546875\n",
      "2015-12-10T08:07:45.961732: step 319, loss 1.51078, acc 0.5\n",
      "2015-12-10T08:07:46.164627: step 320, loss 1.4166, acc 0.546875\n",
      "2015-12-10T08:07:46.386071: step 321, loss 1.41652, acc 0.515625\n",
      "2015-12-10T08:07:46.613728: step 322, loss 1.79271, acc 0.421875\n",
      "2015-12-10T08:07:46.847887: step 323, loss 1.71862, acc 0.453125\n",
      "2015-12-10T08:07:47.060501: step 324, loss 1.42172, acc 0.484375\n",
      "2015-12-10T08:07:47.286929: step 325, loss 1.82623, acc 0.4375\n",
      "2015-12-10T08:07:47.502760: step 326, loss 1.39405, acc 0.53125\n",
      "2015-12-10T08:07:47.725155: step 327, loss 1.59733, acc 0.4375\n",
      "2015-12-10T08:07:47.967101: step 328, loss 1.25913, acc 0.625\n",
      "2015-12-10T08:07:48.186978: step 329, loss 1.52298, acc 0.46875\n",
      "2015-12-10T08:07:48.411557: step 330, loss 1.66941, acc 0.484375\n",
      "2015-12-10T08:07:48.616206: step 331, loss 1.70765, acc 0.5\n",
      "2015-12-10T08:07:48.835007: step 332, loss 1.41201, acc 0.5\n",
      "2015-12-10T08:07:49.051764: step 333, loss 1.61499, acc 0.515625\n",
      "2015-12-10T08:07:49.259661: step 334, loss 1.56018, acc 0.46875\n",
      "2015-12-10T08:07:49.492916: step 335, loss 1.80987, acc 0.46875\n",
      "2015-12-10T08:07:49.707794: step 336, loss 1.57133, acc 0.453125\n",
      "2015-12-10T08:07:49.933016: step 337, loss 1.78547, acc 0.546875\n",
      "2015-12-10T08:07:50.147311: step 338, loss 1.73434, acc 0.484375\n",
      "2015-12-10T08:07:50.396326: step 339, loss 1.28717, acc 0.59375\n",
      "2015-12-10T08:07:50.617274: step 340, loss 1.07666, acc 0.609375\n",
      "2015-12-10T08:07:50.835240: step 341, loss 1.82224, acc 0.4375\n",
      "2015-12-10T08:07:51.099714: step 342, loss 1.66797, acc 0.546875\n",
      "2015-12-10T08:07:51.306451: step 343, loss 1.83192, acc 0.578125\n",
      "2015-12-10T08:07:51.534270: step 344, loss 2.013, acc 0.46875\n",
      "2015-12-10T08:07:51.750715: step 345, loss 1.24778, acc 0.5625\n",
      "2015-12-10T08:07:51.976903: step 346, loss 1.34273, acc 0.5625\n",
      "2015-12-10T08:07:52.198772: step 347, loss 1.38004, acc 0.5625\n",
      "2015-12-10T08:07:52.402999: step 348, loss 1.55522, acc 0.5\n",
      "2015-12-10T08:07:52.616910: step 349, loss 1.17711, acc 0.5625\n",
      "2015-12-10T08:07:52.883136: step 350, loss 2.17248, acc 0.453125\n",
      "2015-12-10T08:07:53.091479: step 351, loss 1.68896, acc 0.515625\n",
      "2015-12-10T08:07:53.320746: step 352, loss 1.53994, acc 0.53125\n",
      "2015-12-10T08:07:53.545253: step 353, loss 0.908509, acc 0.671875\n",
      "2015-12-10T08:07:53.783122: step 354, loss 1.69324, acc 0.515625\n",
      "2015-12-10T08:07:53.994379: step 355, loss 1.46898, acc 0.546875\n",
      "2015-12-10T08:07:54.233334: step 356, loss 1.41739, acc 0.546875\n",
      "2015-12-10T08:07:54.467070: step 357, loss 1.41654, acc 0.515625\n",
      "2015-12-10T08:07:54.718961: step 358, loss 2.02689, acc 0.453125\n",
      "2015-12-10T08:07:54.940487: step 359, loss 1.27893, acc 0.640625\n",
      "2015-12-10T08:07:55.177628: step 360, loss 1.25884, acc 0.609375\n",
      "2015-12-10T08:07:55.391689: step 361, loss 1.61908, acc 0.546875\n",
      "2015-12-10T08:07:55.607019: step 362, loss 1.62381, acc 0.53125\n",
      "2015-12-10T08:07:55.841534: step 363, loss 1.08496, acc 0.71875\n",
      "2015-12-10T08:07:56.083064: step 364, loss 1.39364, acc 0.546875\n",
      "2015-12-10T08:07:56.315265: step 365, loss 1.90117, acc 0.5\n",
      "2015-12-10T08:07:56.526773: step 366, loss 1.19028, acc 0.53125\n",
      "2015-12-10T08:07:56.753716: step 367, loss 1.43329, acc 0.578125\n",
      "2015-12-10T08:07:56.978443: step 368, loss 1.49641, acc 0.515625\n",
      "2015-12-10T08:07:57.210319: step 369, loss 1.22899, acc 0.609375\n",
      "2015-12-10T08:07:57.421366: step 370, loss 1.30519, acc 0.640625\n",
      "2015-12-10T08:07:57.659793: step 371, loss 1.73285, acc 0.515625\n",
      "2015-12-10T08:07:57.888548: step 372, loss 1.49064, acc 0.5\n",
      "2015-12-10T08:07:58.117064: step 373, loss 1.2308, acc 0.5625\n",
      "2015-12-10T08:07:58.371105: step 374, loss 1.36261, acc 0.453125\n",
      "2015-12-10T08:07:58.583605: step 375, loss 1.80298, acc 0.421875\n",
      "2015-12-10T08:07:58.823530: step 376, loss 1.24491, acc 0.5\n",
      "2015-12-10T08:07:59.036555: step 377, loss 1.32065, acc 0.546875\n",
      "2015-12-10T08:07:59.248927: step 378, loss 1.49543, acc 0.53125\n",
      "2015-12-10T08:07:59.479075: step 379, loss 1.0519, acc 0.625\n",
      "2015-12-10T08:07:59.694855: step 380, loss 1.55728, acc 0.59375\n",
      "2015-12-10T08:07:59.935325: step 381, loss 1.4544, acc 0.5625\n",
      "2015-12-10T08:08:00.169872: step 382, loss 1.38762, acc 0.515625\n",
      "2015-12-10T08:08:00.377350: step 383, loss 1.44342, acc 0.46875\n",
      "2015-12-10T08:08:00.624069: step 384, loss 1.8121, acc 0.53125\n",
      "2015-12-10T08:08:00.839761: step 385, loss 1.58462, acc 0.546875\n",
      "2015-12-10T08:08:01.049885: step 386, loss 1.79504, acc 0.453125\n",
      "2015-12-10T08:08:01.290050: step 387, loss 1.52496, acc 0.5\n",
      "2015-12-10T08:08:01.499011: step 388, loss 1.52211, acc 0.53125\n",
      "2015-12-10T08:08:01.718784: step 389, loss 1.36142, acc 0.53125\n",
      "2015-12-10T08:08:01.928444: step 390, loss 1.13488, acc 0.5625\n",
      "2015-12-10T08:08:02.156643: step 391, loss 1.13833, acc 0.515625\n",
      "2015-12-10T08:08:02.390103: step 392, loss 1.69808, acc 0.421875\n",
      "2015-12-10T08:08:02.626414: step 393, loss 1.74323, acc 0.53125\n",
      "2015-12-10T08:08:02.887018: step 394, loss 1.61391, acc 0.5\n",
      "2015-12-10T08:08:03.117139: step 395, loss 1.6719, acc 0.484375\n",
      "2015-12-10T08:08:03.333825: step 396, loss 1.93786, acc 0.46875\n",
      "2015-12-10T08:08:03.543351: step 397, loss 1.57638, acc 0.515625\n",
      "2015-12-10T08:08:03.768483: step 398, loss 1.57535, acc 0.546875\n",
      "2015-12-10T08:08:04.002703: step 399, loss 1.71355, acc 0.484375\n",
      "2015-12-10T08:08:04.438952: step 400, loss 1.28347, acc 0.59375\n",
      "\n",
      "Dev Set:\n",
      "2015-12-10T08:08:05.227088: step 400, loss 0.771138, acc 0.522\n",
      "\n",
      "Saved model checkpoint to /Users/dennybritz/projects/wildml/cnn-text-classification-tf/runs/1449731188/checkpoints/model-400\n",
      "\n",
      "2015-12-10T08:08:05.607972: step 401, loss 1.04171, acc 0.6875\n",
      "2015-12-10T08:08:05.838470: step 402, loss 1.08769, acc 0.609375\n",
      "2015-12-10T08:08:06.048991: step 403, loss 1.73841, acc 0.546875\n",
      "2015-12-10T08:08:06.269558: step 404, loss 1.59362, acc 0.421875\n",
      "2015-12-10T08:08:06.525300: step 405, loss 1.38113, acc 0.609375\n",
      "2015-12-10T08:08:06.962666: step 406, loss 1.58279, acc 0.46875\n",
      "2015-12-10T08:08:07.231766: step 407, loss 1.44625, acc 0.5625\n",
      "2015-12-10T08:08:07.449106: step 408, loss 1.73869, acc 0.5625\n",
      "2015-12-10T08:08:07.684246: step 409, loss 1.57917, acc 0.46875\n",
      "2015-12-10T08:08:07.911573: step 410, loss 1.8747, acc 0.484375\n",
      "2015-12-10T08:08:08.117405: step 411, loss 1.49052, acc 0.5\n",
      "2015-12-10T08:08:08.360352: step 412, loss 1.646, acc 0.5\n",
      "2015-12-10T08:08:08.571569: step 413, loss 1.47351, acc 0.484375\n",
      "2015-12-10T08:08:08.781067: step 414, loss 1.35984, acc 0.484375\n",
      "2015-12-10T08:08:08.988930: step 415, loss 1.84165, acc 0.484375\n",
      "2015-12-10T08:08:09.200266: step 416, loss 1.33655, acc 0.578125\n",
      "2015-12-10T08:08:09.410794: step 417, loss 1.66077, acc 0.484375\n",
      "2015-12-10T08:08:09.624785: step 418, loss 1.19934, acc 0.578125\n",
      "2015-12-10T08:08:09.842671: step 419, loss 1.54984, acc 0.5\n",
      "2015-12-10T08:08:10.078081: step 420, loss 1.19368, acc 0.609375\n",
      "2015-12-10T08:08:10.285806: step 421, loss 2.02449, acc 0.46875\n",
      "2015-12-10T08:08:10.495759: step 422, loss 1.65991, acc 0.421875\n",
      "2015-12-10T08:08:10.705153: step 423, loss 1.55274, acc 0.4375\n",
      "2015-12-10T08:08:10.918100: step 424, loss 1.52853, acc 0.421875\n",
      "2015-12-10T08:08:11.131507: step 425, loss 1.8398, acc 0.453125\n",
      "2015-12-10T08:08:11.344295: step 426, loss 1.36716, acc 0.546875\n",
      "2015-12-10T08:08:11.552727: step 427, loss 1.74129, acc 0.390625\n",
      "2015-12-10T08:08:11.759546: step 428, loss 1.13491, acc 0.65625\n",
      "2015-12-10T08:08:11.962571: step 429, loss 1.58692, acc 0.375\n",
      "2015-12-10T08:08:12.167628: step 430, loss 1.97055, acc 0.46875\n",
      "2015-12-10T08:08:12.374935: step 431, loss 1.49457, acc 0.453125\n",
      "2015-12-10T08:08:12.589714: step 432, loss 1.63719, acc 0.453125\n",
      "2015-12-10T08:08:12.799673: step 433, loss 1.32913, acc 0.53125\n",
      "2015-12-10T08:08:13.007728: step 434, loss 2.02443, acc 0.453125\n",
      "2015-12-10T08:08:13.214267: step 435, loss 1.63986, acc 0.484375\n",
      "2015-12-10T08:08:13.428894: step 436, loss 1.06754, acc 0.59375\n",
      "2015-12-10T08:08:13.657912: step 437, loss 1.52245, acc 0.5\n",
      "2015-12-10T08:08:13.872423: step 438, loss 1.83842, acc 0.546875\n",
      "2015-12-10T08:08:14.083728: step 439, loss 1.39699, acc 0.59375\n",
      "2015-12-10T08:08:14.296145: step 440, loss 1.42999, acc 0.59375\n",
      "2015-12-10T08:08:14.502830: step 441, loss 1.58462, acc 0.546875\n",
      "2015-12-10T08:08:14.714296: step 442, loss 1.19629, acc 0.53125\n",
      "2015-12-10T08:08:14.926750: step 443, loss 1.09988, acc 0.609375\n",
      "2015-12-10T08:08:15.132852: step 444, loss 1.34118, acc 0.5625\n",
      "2015-12-10T08:08:15.347121: step 445, loss 1.34418, acc 0.53125\n",
      "2015-12-10T08:08:15.549808: step 446, loss 1.39031, acc 0.625\n",
      "2015-12-10T08:08:15.762257: step 447, loss 1.17173, acc 0.625\n",
      "2015-12-10T08:08:15.978825: step 448, loss 1.81958, acc 0.390625\n",
      "2015-12-10T08:08:16.194330: step 449, loss 1.76595, acc 0.59375\n",
      "2015-12-10T08:08:16.396822: step 450, loss 1.58034, acc 0.640625\n",
      "2015-12-10T08:08:16.599664: step 451, loss 1.19183, acc 0.578125\n",
      "2015-12-10T08:08:16.807823: step 452, loss 1.4803, acc 0.46875\n",
      "2015-12-10T08:08:17.009897: step 453, loss 1.32588, acc 0.5\n",
      "2015-12-10T08:08:17.219187: step 454, loss 1.2908, acc 0.59375\n",
      "2015-12-10T08:08:17.431146: step 455, loss 1.41292, acc 0.546875\n",
      "2015-12-10T08:08:17.643962: step 456, loss 1.54829, acc 0.5625\n",
      "2015-12-10T08:08:17.858174: step 457, loss 1.12315, acc 0.609375\n",
      "2015-12-10T08:08:18.065588: step 458, loss 1.40027, acc 0.421875\n",
      "2015-12-10T08:08:18.272043: step 459, loss 1.63814, acc 0.46875\n",
      "2015-12-10T08:08:18.495359: step 460, loss 1.40882, acc 0.5625\n",
      "2015-12-10T08:08:18.714056: step 461, loss 1.61353, acc 0.546875\n",
      "2015-12-10T08:08:18.928341: step 462, loss 1.14134, acc 0.546875\n",
      "2015-12-10T08:08:19.167213: step 463, loss 1.70953, acc 0.46875\n",
      "2015-12-10T08:08:19.402872: step 464, loss 1.70456, acc 0.5\n",
      "2015-12-10T08:08:19.609216: step 465, loss 1.26497, acc 0.578125\n",
      "2015-12-10T08:08:19.822728: step 466, loss 1.3299, acc 0.53125\n",
      "2015-12-10T08:08:20.051950: step 467, loss 1.36463, acc 0.609375\n",
      "2015-12-10T08:08:20.277928: step 468, loss 1.05732, acc 0.65625\n",
      "2015-12-10T08:08:20.584090: step 469, loss 1.22521, acc 0.609375\n",
      "2015-12-10T08:08:21.008552: step 470, loss 1.21572, acc 0.609375\n",
      "2015-12-10T08:08:21.230462: step 471, loss 1.47633, acc 0.46875\n",
      "2015-12-10T08:08:21.601158: step 472, loss 1.04788, acc 0.625\n",
      "2015-12-10T08:08:21.851903: step 473, loss 1.64789, acc 0.515625\n",
      "2015-12-10T08:08:22.132264: step 474, loss 1.05342, acc 0.625\n",
      "2015-12-10T08:08:22.383926: step 475, loss 1.61563, acc 0.59375\n",
      "2015-12-10T08:08:22.607625: step 476, loss 1.28758, acc 0.546875\n",
      "2015-12-10T08:08:22.874119: step 477, loss 1.47584, acc 0.53125\n",
      "2015-12-10T08:08:23.203910: step 478, loss 1.55368, acc 0.53125\n",
      "2015-12-10T08:08:23.492304: step 479, loss 1.05682, acc 0.609375\n",
      "2015-12-10T08:08:23.720706: step 480, loss 1.8343, acc 0.484375\n",
      "2015-12-10T08:08:23.962020: step 481, loss 1.28449, acc 0.484375\n",
      "2015-12-10T08:08:24.271058: step 482, loss 1.45243, acc 0.5\n",
      "2015-12-10T08:08:24.527397: step 483, loss 1.88966, acc 0.515625\n",
      "2015-12-10T08:08:24.754209: step 484, loss 1.01075, acc 0.625\n",
      "2015-12-10T08:08:25.035083: step 485, loss 1.16119, acc 0.5625\n",
      "2015-12-10T08:08:25.271726: step 486, loss 1.31097, acc 0.546875\n",
      "2015-12-10T08:08:25.658053: step 487, loss 1.65717, acc 0.46875\n",
      "2015-12-10T08:08:25.919235: step 488, loss 1.2926, acc 0.5625\n",
      "2015-12-10T08:08:26.153411: step 489, loss 0.994258, acc 0.625\n",
      "2015-12-10T08:08:26.465827: step 490, loss 1.70442, acc 0.5\n",
      "2015-12-10T08:08:26.795046: step 491, loss 1.83325, acc 0.421875\n",
      "2015-12-10T08:08:27.054847: step 492, loss 1.32804, acc 0.546875\n",
      "2015-12-10T08:08:27.295472: step 493, loss 1.47087, acc 0.578125\n",
      "2015-12-10T08:08:27.555732: step 494, loss 1.37316, acc 0.5625\n",
      "2015-12-10T08:08:27.859808: step 495, loss 1.45325, acc 0.59375\n",
      "2015-12-10T08:08:28.115530: step 496, loss 1.59135, acc 0.453125\n",
      "2015-12-10T08:08:28.373191: step 497, loss 1.31036, acc 0.640625\n",
      "2015-12-10T08:08:28.674991: step 498, loss 1.5901, acc 0.484375\n",
      "2015-12-10T08:08:28.965474: step 499, loss 1.56635, acc 0.484375\n",
      "2015-12-10T08:08:29.206212: step 500, loss 1.52521, acc 0.484375\n",
      "\n",
      "Dev Set:\n",
      "2015-12-10T08:08:29.987348: step 500, loss 0.757255, acc 0.539\n",
      "\n",
      "Saved model checkpoint to /Users/dennybritz/projects/wildml/cnn-text-classification-tf/runs/1449731188/checkpoints/model-500\n",
      "\n",
      "2015-12-10T08:08:30.506106: step 501, loss 1.50995, acc 0.515625\n",
      "2015-12-10T08:08:30.800716: step 502, loss 1.50922, acc 0.546875\n",
      "2015-12-10T08:08:31.074187: step 503, loss 1.67027, acc 0.53125\n",
      "2015-12-10T08:08:31.304644: step 504, loss 1.63561, acc 0.46875\n",
      "2015-12-10T08:08:31.540755: step 505, loss 2.05828, acc 0.5\n",
      "2015-12-10T08:08:31.796877: step 506, loss 1.2414, acc 0.59375\n",
      "2015-12-10T08:08:32.274088: step 507, loss 1.64369, acc 0.5\n",
      "2015-12-10T08:08:32.502384: step 508, loss 1.31962, acc 0.578125\n",
      "2015-12-10T08:08:32.837884: step 509, loss 1.40372, acc 0.53125\n",
      "2015-12-10T08:08:33.162562: step 510, loss 1.40111, acc 0.546875\n",
      "2015-12-10T08:08:33.387186: step 511, loss 1.61033, acc 0.484375\n",
      "2015-12-10T08:08:33.612760: step 512, loss 1.62179, acc 0.4375\n",
      "2015-12-10T08:08:33.846814: step 513, loss 1.3494, acc 0.625\n",
      "2015-12-10T08:08:34.060415: step 514, loss 1.37993, acc 0.5625\n",
      "2015-12-10T08:08:34.279686: step 515, loss 1.16445, acc 0.578125\n",
      "2015-12-10T08:08:34.521788: step 516, loss 1.78251, acc 0.453125\n",
      "2015-12-10T08:08:34.742369: step 517, loss 1.35807, acc 0.515625\n",
      "2015-12-10T08:08:34.977097: step 518, loss 1.41768, acc 0.5625\n",
      "2015-12-10T08:08:35.204382: step 519, loss 1.57377, acc 0.5\n",
      "2015-12-10T08:08:35.428097: step 520, loss 1.78664, acc 0.4375\n",
      "2015-12-10T08:08:35.636911: step 521, loss 1.2425, acc 0.59375\n",
      "2015-12-10T08:08:35.867872: step 522, loss 1.3574, acc 0.625\n",
      "2015-12-10T08:08:36.095465: step 523, loss 1.74918, acc 0.5\n",
      "2015-12-10T08:08:36.340900: step 524, loss 1.65693, acc 0.546875\n",
      "2015-12-10T08:08:36.555640: step 525, loss 1.28733, acc 0.53125\n",
      "2015-12-10T08:08:36.791347: step 526, loss 1.23096, acc 0.5625\n",
      "2015-12-10T08:08:36.995496: step 527, loss 1.18178, acc 0.5625\n",
      "2015-12-10T08:08:37.200086: step 528, loss 2.03242, acc 0.453125\n",
      "2015-12-10T08:08:37.422268: step 529, loss 1.56575, acc 0.546875\n",
      "2015-12-10T08:08:37.636388: step 530, loss 1.62661, acc 0.46875\n",
      "2015-12-10T08:08:37.866253: step 531, loss 1.56719, acc 0.5\n",
      "2015-12-10T08:08:38.074376: step 532, loss 1.42558, acc 0.546875\n",
      "2015-12-10T08:08:38.289044: step 533, loss 1.55225, acc 0.546875\n",
      "2015-12-10T08:08:38.501536: step 534, loss 1.38133, acc 0.53125\n",
      "2015-12-10T08:08:38.725843: step 535, loss 1.58819, acc 0.4375\n",
      "2015-12-10T08:08:38.942512: step 536, loss 1.23757, acc 0.59375\n",
      "2015-12-10T08:08:39.153129: step 537, loss 1.31731, acc 0.5625\n",
      "2015-12-10T08:08:39.388572: step 538, loss 1.86753, acc 0.453125\n",
      "2015-12-10T08:08:39.606474: step 539, loss 1.46674, acc 0.46875\n",
      "2015-12-10T08:08:39.840990: step 540, loss 1.24721, acc 0.515625\n",
      "2015-12-10T08:08:40.044429: step 541, loss 1.53723, acc 0.515625\n",
      "2015-12-10T08:08:40.251382: step 542, loss 1.71136, acc 0.5625\n",
      "2015-12-10T08:08:40.496821: step 543, loss 1.51241, acc 0.578125\n",
      "2015-12-10T08:08:40.727258: step 544, loss 1.50329, acc 0.546875\n",
      "2015-12-10T08:08:40.953965: step 545, loss 1.57223, acc 0.515625\n",
      "2015-12-10T08:08:41.171352: step 546, loss 1.3961, acc 0.546875\n",
      "2015-12-10T08:08:41.561849: step 547, loss 1.38118, acc 0.578125\n",
      "2015-12-10T08:08:41.870363: step 548, loss 1.34635, acc 0.53125\n",
      "2015-12-10T08:08:42.264069: step 549, loss 1.62462, acc 0.515625\n",
      "2015-12-10T08:08:42.561054: step 550, loss 1.49079, acc 0.515625\n",
      "2015-12-10T08:08:42.776188: step 551, loss 1.75679, acc 0.4375\n",
      "2015-12-10T08:08:43.002035: step 552, loss 1.33009, acc 0.59375\n",
      "2015-12-10T08:08:43.267655: step 553, loss 1.19328, acc 0.546875\n",
      "2015-12-10T08:08:43.497388: step 554, loss 1.2914, acc 0.546875\n",
      "2015-12-10T08:08:43.732402: step 555, loss 1.62765, acc 0.46875\n",
      "2015-12-10T08:08:43.952174: step 556, loss 1.15129, acc 0.59375\n",
      "2015-12-10T08:08:44.181801: step 557, loss 1.30119, acc 0.53125\n",
      "2015-12-10T08:08:44.485246: step 558, loss 1.69095, acc 0.4375\n",
      "2015-12-10T08:08:44.730909: step 559, loss 1.04176, acc 0.640625\n",
      "2015-12-10T08:08:44.956786: step 560, loss 1.69268, acc 0.5\n",
      "2015-12-10T08:08:45.168504: step 561, loss 1.18107, acc 0.515625\n",
      "2015-12-10T08:08:45.386360: step 562, loss 1.78756, acc 0.484375\n",
      "2015-12-10T08:08:45.614209: step 563, loss 1.58583, acc 0.515625\n",
      "2015-12-10T08:08:45.837452: step 564, loss 1.43267, acc 0.484375\n",
      "2015-12-10T08:08:46.065586: step 565, loss 1.44247, acc 0.5625\n",
      "2015-12-10T08:08:46.286676: step 566, loss 1.41046, acc 0.5625\n",
      "2015-12-10T08:08:46.511995: step 567, loss 1.89427, acc 0.4375\n",
      "2015-12-10T08:08:46.737404: step 568, loss 1.13886, acc 0.59375\n",
      "2015-12-10T08:08:46.963253: step 569, loss 1.74329, acc 0.40625\n",
      "2015-12-10T08:08:47.200489: step 570, loss 1.52266, acc 0.46875\n",
      "2015-12-10T08:08:47.408557: step 571, loss 1.51607, acc 0.546875\n",
      "2015-12-10T08:08:47.627378: step 572, loss 1.49969, acc 0.5625\n",
      "2015-12-10T08:08:47.858569: step 573, loss 1.19347, acc 0.578125\n",
      "2015-12-10T08:08:48.088604: step 574, loss 1.59771, acc 0.515625\n",
      "2015-12-10T08:08:48.294731: step 575, loss 1.13727, acc 0.546875\n",
      "2015-12-10T08:08:48.539000: step 576, loss 1.33333, acc 0.515625\n",
      "2015-12-10T08:08:48.751860: step 577, loss 1.75862, acc 0.453125\n",
      "2015-12-10T08:08:48.987141: step 578, loss 1.09613, acc 0.609375\n",
      "2015-12-10T08:08:49.202013: step 579, loss 1.04826, acc 0.609375\n",
      "2015-12-10T08:08:49.428657: step 580, loss 1.25941, acc 0.5625\n",
      "2015-12-10T08:08:49.645402: step 581, loss 1.01745, acc 0.671875\n",
      "2015-12-10T08:08:49.867025: step 582, loss 1.81249, acc 0.421875\n",
      "2015-12-10T08:08:50.106355: step 583, loss 1.17426, acc 0.515625\n",
      "2015-12-10T08:08:50.313842: step 584, loss 1.29676, acc 0.578125\n",
      "2015-12-10T08:08:50.522593: step 585, loss 1.20927, acc 0.5625\n",
      "2015-12-10T08:08:50.745702: step 586, loss 1.44689, acc 0.53125\n",
      "2015-12-10T08:08:50.973472: step 587, loss 1.43401, acc 0.546875\n",
      "2015-12-10T08:08:51.188972: step 588, loss 1.20834, acc 0.640625\n",
      "2015-12-10T08:08:51.400574: step 589, loss 1.16511, acc 0.609375\n",
      "2015-12-10T08:08:51.630075: step 590, loss 1.14551, acc 0.578125\n",
      "2015-12-10T08:08:51.847187: step 591, loss 1.51364, acc 0.515625\n",
      "2015-12-10T08:08:52.073610: step 592, loss 1.66669, acc 0.484375\n",
      "2015-12-10T08:08:52.284577: step 593, loss 1.57759, acc 0.5\n",
      "2015-12-10T08:08:52.510184: step 594, loss 1.8385, acc 0.4375\n",
      "2015-12-10T08:08:52.740201: step 595, loss 1.25826, acc 0.546875\n",
      "2015-12-10T08:08:52.967042: step 596, loss 1.66823, acc 0.5625\n",
      "2015-12-10T08:08:53.181439: step 597, loss 1.14468, acc 0.515625\n",
      "2015-12-10T08:08:53.389793: step 598, loss 1.25758, acc 0.59375\n",
      "2015-12-10T08:08:53.600544: step 599, loss 1.07815, acc 0.609375\n",
      "2015-12-10T08:08:53.818596: step 600, loss 1.26693, acc 0.53125\n",
      "\n",
      "Dev Set:\n",
      "2015-12-10T08:08:54.603150: step 600, loss 0.747, acc 0.552\n",
      "\n",
      "Saved model checkpoint to /Users/dennybritz/projects/wildml/cnn-text-classification-tf/runs/1449731188/checkpoints/model-600\n",
      "\n",
      "2015-12-10T08:08:54.989335: step 601, loss 1.64765, acc 0.484375\n",
      "2015-12-10T08:08:55.206247: step 602, loss 1.23475, acc 0.609375\n",
      "2015-12-10T08:08:55.425220: step 603, loss 0.878421, acc 0.640625\n",
      "2015-12-10T08:08:55.644222: step 604, loss 1.43647, acc 0.5\n",
      "2015-12-10T08:08:55.855595: step 605, loss 1.44093, acc 0.515625\n",
      "2015-12-10T08:08:56.275021: step 606, loss 1.17618, acc 0.609375\n",
      "2015-12-10T08:08:56.604325: step 607, loss 1.21509, acc 0.5\n",
      "2015-12-10T08:08:56.822710: step 608, loss 1.70802, acc 0.4375\n",
      "2015-12-10T08:08:57.053215: step 609, loss 1.81457, acc 0.515625\n",
      "2015-12-10T08:08:57.275411: step 610, loss 1.2848, acc 0.640625\n",
      "2015-12-10T08:08:57.541498: step 611, loss 1.4272, acc 0.515625\n",
      "2015-12-10T08:08:57.756386: step 612, loss 1.28536, acc 0.53125\n",
      "2015-12-10T08:08:57.978416: step 613, loss 1.40569, acc 0.53125\n",
      "2015-12-10T08:08:58.219045: step 614, loss 1.13497, acc 0.515625\n",
      "2015-12-10T08:08:58.422459: step 615, loss 1.18313, acc 0.546875\n",
      "2015-12-10T08:08:58.647160: step 616, loss 1.50975, acc 0.5\n",
      "2015-12-10T08:08:58.868171: step 617, loss 1.20357, acc 0.515625\n",
      "2015-12-10T08:08:59.107566: step 618, loss 1.27021, acc 0.609375\n",
      "2015-12-10T08:08:59.317856: step 619, loss 1.24818, acc 0.53125\n",
      "2015-12-10T08:08:59.561615: step 620, loss 0.988411, acc 0.625\n",
      "2015-12-10T08:08:59.775070: step 621, loss 1.18917, acc 0.640625\n",
      "2015-12-10T08:09:00.008814: step 622, loss 0.740101, acc 0.703125\n",
      "2015-12-10T08:09:00.226324: step 623, loss 0.844547, acc 0.703125\n",
      "2015-12-10T08:09:00.463682: step 624, loss 0.981347, acc 0.59375\n",
      "2015-12-10T08:09:00.684662: step 625, loss 1.33484, acc 0.5\n",
      "2015-12-10T08:09:00.895395: step 626, loss 1.13321, acc 0.53125\n",
      "2015-12-10T08:09:01.113100: step 627, loss 1.04962, acc 0.5625\n",
      "2015-12-10T08:09:01.340935: step 628, loss 1.74786, acc 0.421875\n",
      "2015-12-10T08:09:01.550365: step 629, loss 1.3319, acc 0.546875\n",
      "2015-12-10T08:09:01.772847: step 630, loss 1.29401, acc 0.59375\n",
      "2015-12-10T08:09:02.011063: step 631, loss 1.0217, acc 0.53125\n",
      "2015-12-10T08:09:02.223128: step 632, loss 1.16085, acc 0.640625\n",
      "2015-12-10T08:09:02.431363: step 633, loss 1.60858, acc 0.453125\n",
      "2015-12-10T08:09:02.657066: step 634, loss 1.39814, acc 0.546875\n",
      "2015-12-10T08:09:02.868643: step 635, loss 1.59551, acc 0.546875\n",
      "2015-12-10T08:09:03.121243: step 636, loss 1.43143, acc 0.5\n",
      "2015-12-10T08:09:03.357751: step 637, loss 1.3148, acc 0.546875\n",
      "2015-12-10T08:09:03.567638: step 638, loss 1.53157, acc 0.546875\n",
      "2015-12-10T08:09:03.815016: step 639, loss 1.36286, acc 0.5625\n",
      "2015-12-10T08:09:04.030478: step 640, loss 1.3406, acc 0.515625\n",
      "2015-12-10T08:09:04.238971: step 641, loss 1.34739, acc 0.484375\n",
      "2015-12-10T08:09:04.453550: step 642, loss 0.940789, acc 0.640625\n",
      "2015-12-10T08:09:04.678914: step 643, loss 1.33982, acc 0.46875\n",
      "2015-12-10T08:09:04.893307: step 644, loss 1.65031, acc 0.515625\n",
      "2015-12-10T08:09:05.120177: step 645, loss 1.31602, acc 0.53125\n",
      "2015-12-10T08:09:05.353038: step 646, loss 1.83311, acc 0.4375\n",
      "2015-12-10T08:09:05.561862: step 647, loss 1.57302, acc 0.453125\n",
      "2015-12-10T08:09:05.768774: step 648, loss 1.26091, acc 0.609375\n",
      "2015-12-10T08:09:05.975421: step 649, loss 1.26286, acc 0.515625\n",
      "2015-12-10T08:09:06.181204: step 650, loss 1.34492, acc 0.484375\n",
      "2015-12-10T08:09:06.398104: step 651, loss 0.97562, acc 0.5625\n",
      "2015-12-10T08:09:06.607980: step 652, loss 1.67475, acc 0.5\n",
      "2015-12-10T08:09:06.814905: step 653, loss 1.46329, acc 0.46875\n",
      "2015-12-10T08:09:07.040759: step 654, loss 1.09704, acc 0.6875\n",
      "2015-12-10T08:09:07.245070: step 655, loss 1.50709, acc 0.546875\n",
      "2015-12-10T08:09:07.449278: step 656, loss 0.991829, acc 0.609375\n",
      "2015-12-10T08:09:07.661289: step 657, loss 1.49649, acc 0.4375\n",
      "2015-12-10T08:09:07.868877: step 658, loss 1.31804, acc 0.546875\n",
      "2015-12-10T08:09:08.109844: step 659, loss 1.43097, acc 0.546875\n",
      "2015-12-10T08:09:08.326437: step 660, loss 1.21707, acc 0.59375\n",
      "2015-12-10T08:09:08.580849: step 661, loss 1.26199, acc 0.53125\n",
      "2015-12-10T08:09:08.797572: step 662, loss 1.08697, acc 0.53125\n",
      "2015-12-10T08:09:09.007968: step 663, loss 1.35203, acc 0.59375\n",
      "2015-12-10T08:09:09.252354: step 664, loss 1.62228, acc 0.453125\n",
      "2015-12-10T08:09:09.473575: step 665, loss 1.46062, acc 0.515625\n",
      "2015-12-10T08:09:09.707434: step 666, loss 1.27048, acc 0.5625\n",
      "2015-12-10T08:09:09.927891: step 667, loss 1.09421, acc 0.578125\n",
      "2015-12-10T08:09:10.179770: step 668, loss 1.3423, acc 0.578125\n",
      "2015-12-10T08:09:10.395535: step 669, loss 1.2521, acc 0.59375\n",
      "2015-12-10T08:09:10.635826: step 670, loss 1.38401, acc 0.46875\n",
      "2015-12-10T08:09:10.846429: step 671, loss 1.29862, acc 0.5\n",
      "2015-12-10T08:09:11.093200: step 672, loss 1.19028, acc 0.625\n",
      "2015-12-10T08:09:11.300395: step 673, loss 1.5179, acc 0.46875\n",
      "2015-12-10T08:09:11.510205: step 674, loss 1.10878, acc 0.515625\n",
      "2015-12-10T08:09:11.727141: step 675, loss 1.38853, acc 0.53125\n",
      "2015-12-10T08:09:11.955203: step 676, loss 1.25357, acc 0.625\n",
      "2015-12-10T08:09:12.180770: step 677, loss 1.08351, acc 0.5625\n",
      "2015-12-10T08:09:12.387945: step 678, loss 1.05467, acc 0.625\n",
      "2015-12-10T08:09:12.613503: step 679, loss 1.22403, acc 0.46875\n",
      "2015-12-10T08:09:12.845236: step 680, loss 1.43191, acc 0.515625\n",
      "2015-12-10T08:09:13.075239: step 681, loss 1.16576, acc 0.53125\n",
      "2015-12-10T08:09:13.296054: step 682, loss 1.30682, acc 0.515625\n",
      "2015-12-10T08:09:13.523264: step 683, loss 1.57195, acc 0.5\n",
      "2015-12-10T08:09:13.744766: step 684, loss 1.32719, acc 0.484375\n",
      "2015-12-10T08:09:13.959616: step 685, loss 1.54075, acc 0.546875\n",
      "2015-12-10T08:09:14.173969: step 686, loss 0.888814, acc 0.671875\n",
      "2015-12-10T08:09:14.393263: step 687, loss 1.60887, acc 0.5\n",
      "2015-12-10T08:09:14.624659: step 688, loss 1.11708, acc 0.65625\n",
      "2015-12-10T08:09:14.837231: step 689, loss 1.11841, acc 0.625\n",
      "2015-12-10T08:09:15.062417: step 690, loss 1.50726, acc 0.46875\n",
      "2015-12-10T08:09:15.275575: step 691, loss 1.05229, acc 0.578125\n",
      "2015-12-10T08:09:15.496190: step 692, loss 1.2629, acc 0.5625\n",
      "2015-12-10T08:09:15.722178: step 693, loss 1.09187, acc 0.640625\n",
      "2015-12-10T08:09:15.933703: step 694, loss 1.1285, acc 0.578125\n",
      "2015-12-10T08:09:16.165995: step 695, loss 0.985565, acc 0.625\n",
      "2015-12-10T08:09:16.389403: step 696, loss 1.42185, acc 0.578125\n",
      "2015-12-10T08:09:16.621022: step 697, loss 1.35581, acc 0.5625\n",
      "2015-12-10T08:09:16.838244: step 698, loss 1.25302, acc 0.625\n",
      "2015-12-10T08:09:17.055095: step 699, loss 1.27931, acc 0.53125\n",
      "2015-12-10T08:09:17.265819: step 700, loss 1.53986, acc 0.53125\n",
      "\n",
      "Dev Set:\n",
      "2015-12-10T08:09:17.981502: step 700, loss 0.740539, acc 0.549\n",
      "\n",
      "Saved model checkpoint to /Users/dennybritz/projects/wildml/cnn-text-classification-tf/runs/1449731188/checkpoints/model-700\n",
      "\n",
      "2015-12-10T08:09:18.408023: step 701, loss 1.28528, acc 0.578125\n",
      "2015-12-10T08:09:18.626080: step 702, loss 1.15286, acc 0.625\n",
      "2015-12-10T08:09:18.844144: step 703, loss 1.02433, acc 0.546875\n",
      "2015-12-10T08:09:19.071550: step 704, loss 1.37056, acc 0.578125\n",
      "2015-12-10T08:09:19.305597: step 705, loss 1.03295, acc 0.640625\n",
      "2015-12-10T08:09:19.510479: step 706, loss 1.72731, acc 0.4375\n",
      "2015-12-10T08:09:19.722613: step 707, loss 0.983228, acc 0.671875\n",
      "2015-12-10T08:09:20.147254: step 708, loss 1.43936, acc 0.453125\n",
      "2015-12-10T08:09:20.416673: step 709, loss 1.32743, acc 0.53125\n",
      "2015-12-10T08:09:20.648769: step 710, loss 1.19738, acc 0.578125\n",
      "2015-12-10T08:09:20.900387: step 711, loss 1.22014, acc 0.609375\n",
      "2015-12-10T08:09:21.119495: step 712, loss 1.24479, acc 0.578125\n",
      "2015-12-10T08:09:21.349589: step 713, loss 1.57667, acc 0.46875\n",
      "2015-12-10T08:09:21.560303: step 714, loss 1.19546, acc 0.6875\n",
      "2015-12-10T08:09:21.806345: step 715, loss 1.77248, acc 0.46875\n",
      "2015-12-10T08:09:22.037424: step 716, loss 1.1, acc 0.515625\n",
      "2015-12-10T08:09:22.256383: step 717, loss 1.00462, acc 0.625\n",
      "2015-12-10T08:09:22.487220: step 718, loss 1.19342, acc 0.515625\n",
      "2015-12-10T08:09:22.697973: step 719, loss 1.56061, acc 0.53125\n",
      "2015-12-10T08:09:22.935146: step 720, loss 1.38656, acc 0.484375\n",
      "2015-12-10T08:09:23.154752: step 721, loss 1.05253, acc 0.640625\n",
      "2015-12-10T08:09:23.381677: step 722, loss 1.1118, acc 0.609375\n",
      "2015-12-10T08:09:23.608392: step 723, loss 1.11138, acc 0.640625\n",
      "2015-12-10T08:09:23.832656: step 724, loss 1.17538, acc 0.5625\n",
      "2015-12-10T08:09:24.064525: step 725, loss 1.24689, acc 0.578125\n",
      "2015-12-10T08:09:24.287868: step 726, loss 1.29758, acc 0.578125\n",
      "2015-12-10T08:09:24.509442: step 727, loss 1.66823, acc 0.453125\n",
      "2015-12-10T08:09:24.739182: step 728, loss 1.27369, acc 0.578125\n",
      "2015-12-10T08:09:24.958457: step 729, loss 1.31689, acc 0.546875\n",
      "2015-12-10T08:09:25.176173: step 730, loss 1.5993, acc 0.5\n",
      "2015-12-10T08:09:25.406622: step 731, loss 0.984768, acc 0.53125\n",
      "2015-12-10T08:09:25.608377: step 732, loss 1.27711, acc 0.546875\n",
      "2015-12-10T08:09:25.822698: step 733, loss 1.25228, acc 0.546875\n",
      "2015-12-10T08:09:26.039674: step 734, loss 0.939919, acc 0.578125\n",
      "2015-12-10T08:09:26.265674: step 735, loss 1.14763, acc 0.5625\n",
      "2015-12-10T08:09:26.486606: step 736, loss 1.28804, acc 0.59375\n",
      "2015-12-10T08:09:26.715592: step 737, loss 0.980859, acc 0.609375\n",
      "2015-12-10T08:09:26.933224: step 738, loss 1.4, acc 0.578125\n",
      "2015-12-10T08:09:27.163335: step 739, loss 1.49139, acc 0.515625\n",
      "2015-12-10T08:09:27.378954: step 740, loss 1.6101, acc 0.515625\n",
      "2015-12-10T08:09:27.604152: step 741, loss 0.827837, acc 0.703125\n",
      "2015-12-10T08:09:27.846499: step 742, loss 1.35578, acc 0.5\n",
      "2015-12-10T08:09:28.055963: step 743, loss 1.34065, acc 0.578125\n",
      "2015-12-10T08:09:28.311216: step 744, loss 1.50977, acc 0.53125\n",
      "2015-12-10T08:09:28.522074: step 745, loss 1.18989, acc 0.546875\n",
      "2015-12-10T08:09:28.757458: step 746, loss 1.3598, acc 0.5\n",
      "2015-12-10T08:09:28.982423: step 747, loss 1.27432, acc 0.546875\n",
      "2015-12-10T08:09:29.203002: step 748, loss 1.30845, acc 0.578125\n",
      "2015-12-10T08:09:29.439703: step 749, loss 1.32039, acc 0.484375\n",
      "2015-12-10T08:09:29.655135: step 750, loss 1.63612, acc 0.5\n",
      "2015-12-10T08:09:29.872558: step 751, loss 0.890512, acc 0.6875\n",
      "2015-12-10T08:09:30.118327: step 752, loss 1.17704, acc 0.53125\n",
      "2015-12-10T08:09:30.332764: step 753, loss 1.37884, acc 0.546875\n",
      "2015-12-10T08:09:30.582148: step 754, loss 1.4877, acc 0.578125\n",
      "2015-12-10T08:09:30.797525: step 755, loss 1.30927, acc 0.483871\n",
      "2015-12-10T08:09:31.062656: step 756, loss 1.34355, acc 0.53125\n",
      "2015-12-10T08:09:31.271431: step 757, loss 1.54647, acc 0.515625\n",
      "2015-12-10T08:09:31.499420: step 758, loss 1.3764, acc 0.5625\n",
      "2015-12-10T08:09:31.730394: step 759, loss 1.08739, acc 0.5625\n",
      "2015-12-10T08:09:31.953735: step 760, loss 1.81835, acc 0.515625\n",
      "2015-12-10T08:09:32.176326: step 761, loss 1.8357, acc 0.390625\n",
      "2015-12-10T08:09:32.394013: step 762, loss 1.04863, acc 0.609375\n",
      "2015-12-10T08:09:32.608957: step 763, loss 0.983934, acc 0.53125\n",
      "2015-12-10T08:09:32.825748: step 764, loss 1.01608, acc 0.578125\n",
      "2015-12-10T08:09:33.061095: step 765, loss 1.77814, acc 0.453125\n",
      "2015-12-10T08:09:33.285982: step 766, loss 1.64878, acc 0.5\n",
      "2015-12-10T08:09:33.516283: step 767, loss 1.01208, acc 0.625\n",
      "2015-12-10T08:09:33.727797: step 768, loss 1.38118, acc 0.5\n",
      "2015-12-10T08:09:34.004247: step 769, loss 1.49445, acc 0.453125\n",
      "2015-12-10T08:09:34.214623: step 770, loss 1.21149, acc 0.578125\n",
      "2015-12-10T08:09:34.434625: step 771, loss 1.28473, acc 0.609375\n",
      "2015-12-10T08:09:34.659199: step 772, loss 1.02838, acc 0.578125\n",
      "2015-12-10T08:09:34.882618: step 773, loss 1.63722, acc 0.40625\n",
      "2015-12-10T08:09:35.100508: step 774, loss 1.41735, acc 0.5\n",
      "2015-12-10T08:09:35.318232: step 775, loss 1.08286, acc 0.65625\n",
      "2015-12-10T08:09:35.544582: step 776, loss 1.14279, acc 0.625\n",
      "2015-12-10T08:09:35.751911: step 777, loss 1.49657, acc 0.484375\n",
      "2015-12-10T08:09:35.970491: step 778, loss 1.61784, acc 0.484375\n",
      "2015-12-10T08:09:36.185331: step 779, loss 1.26193, acc 0.578125\n",
      "2015-12-10T08:09:36.410373: step 780, loss 1.09897, acc 0.578125\n",
      "2015-12-10T08:09:36.628585: step 781, loss 1.20063, acc 0.640625\n",
      "2015-12-10T08:09:36.850836: step 782, loss 1.44653, acc 0.515625\n",
      "2015-12-10T08:09:37.060487: step 783, loss 1.32752, acc 0.46875\n",
      "2015-12-10T08:09:37.273420: step 784, loss 1.09117, acc 0.53125\n",
      "2015-12-10T08:09:37.535115: step 785, loss 1.597, acc 0.421875\n",
      "2015-12-10T08:09:37.751189: step 786, loss 1.01956, acc 0.59375\n",
      "2015-12-10T08:09:37.968397: step 787, loss 1.16164, acc 0.578125\n",
      "2015-12-10T08:09:38.178697: step 788, loss 1.2017, acc 0.53125\n",
      "2015-12-10T08:09:38.405282: step 789, loss 1.55531, acc 0.453125\n",
      "2015-12-10T08:09:38.616730: step 790, loss 1.26696, acc 0.5625\n",
      "2015-12-10T08:09:38.824486: step 791, loss 1.18743, acc 0.5\n",
      "2015-12-10T08:09:39.078788: step 792, loss 1.63458, acc 0.453125\n",
      "2015-12-10T08:09:39.287085: step 793, loss 1.0761, acc 0.546875\n",
      "2015-12-10T08:09:39.510477: step 794, loss 1.1085, acc 0.625\n",
      "2015-12-10T08:09:39.725597: step 795, loss 1.28622, acc 0.59375\n",
      "2015-12-10T08:09:39.945836: step 796, loss 1.26404, acc 0.59375\n",
      "2015-12-10T08:09:40.171834: step 797, loss 1.18571, acc 0.578125\n",
      "2015-12-10T08:09:40.395010: step 798, loss 1.12371, acc 0.5625\n",
      "2015-12-10T08:09:40.621627: step 799, loss 1.21312, acc 0.578125\n",
      "2015-12-10T08:09:40.838086: step 800, loss 1.19721, acc 0.515625\n",
      "\n",
      "Dev Set:\n",
      "2015-12-10T08:09:41.503582: step 800, loss 0.727368, acc 0.551\n",
      "\n",
      "Saved model checkpoint to /Users/dennybritz/projects/wildml/cnn-text-classification-tf/runs/1449731188/checkpoints/model-800\n",
      "\n",
      "2015-12-10T08:09:41.892404: step 801, loss 1.23023, acc 0.5\n",
      "2015-12-10T08:09:42.116380: step 802, loss 1.50064, acc 0.53125\n",
      "2015-12-10T08:09:42.336766: step 803, loss 1.55101, acc 0.578125\n",
      "2015-12-10T08:09:42.551261: step 804, loss 1.28504, acc 0.546875\n",
      "2015-12-10T08:09:42.769551: step 805, loss 1.01919, acc 0.578125\n",
      "2015-12-10T08:09:43.183698: step 806, loss 1.35618, acc 0.46875\n",
      "2015-12-10T08:09:43.487618: step 807, loss 1.04527, acc 0.609375\n",
      "2015-12-10T08:09:43.721608: step 808, loss 1.20211, acc 0.609375\n",
      "2015-12-10T08:09:43.956619: step 809, loss 0.734556, acc 0.6875\n",
      "2015-12-10T08:09:44.194117: step 810, loss 1.12739, acc 0.46875\n",
      "2015-12-10T08:09:44.419305: step 811, loss 1.27184, acc 0.546875\n",
      "2015-12-10T08:09:44.641792: step 812, loss 1.38962, acc 0.5\n",
      "2015-12-10T08:09:44.861418: step 813, loss 0.942066, acc 0.640625\n",
      "2015-12-10T08:09:45.080569: step 814, loss 1.11584, acc 0.5625\n",
      "2015-12-10T08:09:45.295425: step 815, loss 1.58571, acc 0.453125\n",
      "2015-12-10T08:09:45.536474: step 816, loss 1.19024, acc 0.5625\n",
      "2015-12-10T08:09:45.767918: step 817, loss 1.39716, acc 0.5625\n",
      "2015-12-10T08:09:45.979452: step 818, loss 0.918201, acc 0.65625\n",
      "2015-12-10T08:09:46.224827: step 819, loss 1.21545, acc 0.5625\n",
      "2015-12-10T08:09:46.460709: step 820, loss 1.25306, acc 0.453125\n",
      "2015-12-10T08:09:46.672119: step 821, loss 0.891448, acc 0.703125\n",
      "2015-12-10T08:09:46.895955: step 822, loss 1.45198, acc 0.40625\n",
      "2015-12-10T08:09:47.097879: step 823, loss 1.29198, acc 0.5625\n",
      "2015-12-10T08:09:47.320815: step 824, loss 1.40349, acc 0.5625\n",
      "2015-12-10T08:09:47.537439: step 825, loss 1.24134, acc 0.4375\n",
      "2015-12-10T08:09:47.759184: step 826, loss 1.46263, acc 0.5\n",
      "2015-12-10T08:09:47.983223: step 827, loss 1.22, acc 0.5625\n",
      "2015-12-10T08:09:48.214502: step 828, loss 1.28599, acc 0.53125\n",
      "2015-12-10T08:09:48.441826: step 829, loss 1.23978, acc 0.46875\n",
      "2015-12-10T08:09:48.668974: step 830, loss 1.40515, acc 0.53125\n",
      "2015-12-10T08:09:48.890314: step 831, loss 1.517, acc 0.515625\n",
      "2015-12-10T08:09:49.106597: step 832, loss 1.34486, acc 0.546875\n",
      "2015-12-10T08:09:49.327138: step 833, loss 1.6592, acc 0.375\n",
      "2015-12-10T08:09:49.567408: step 834, loss 1.45298, acc 0.546875\n",
      "2015-12-10T08:09:49.797637: step 835, loss 1.23294, acc 0.625\n",
      "2015-12-10T08:09:50.012079: step 836, loss 0.873588, acc 0.640625\n",
      "2015-12-10T08:09:50.228251: step 837, loss 1.19103, acc 0.578125\n",
      "2015-12-10T08:09:50.464997: step 838, loss 1.3092, acc 0.578125\n",
      "2015-12-10T08:09:50.687198: step 839, loss 1.15839, acc 0.4375\n",
      "2015-12-10T08:09:50.924486: step 840, loss 1.01305, acc 0.609375\n",
      "2015-12-10T08:09:51.143999: step 841, loss 0.986799, acc 0.609375\n",
      "2015-12-10T08:09:51.362479: step 842, loss 1.31643, acc 0.515625\n",
      "2015-12-10T08:09:51.599052: step 843, loss 0.894606, acc 0.671875\n",
      "2015-12-10T08:09:51.831506: step 844, loss 1.4398, acc 0.484375\n",
      "2015-12-10T08:09:52.063359: step 845, loss 1.49263, acc 0.46875\n",
      "2015-12-10T08:09:52.280832: step 846, loss 1.42866, acc 0.546875\n",
      "2015-12-10T08:09:52.523438: step 847, loss 1.20085, acc 0.65625\n",
      "2015-12-10T08:09:52.753196: step 848, loss 1.28126, acc 0.453125\n",
      "2015-12-10T08:09:53.013955: step 849, loss 1.33993, acc 0.578125\n",
      "2015-12-10T08:09:53.247113: step 850, loss 1.11344, acc 0.484375\n",
      "2015-12-10T08:09:53.500048: step 851, loss 1.54284, acc 0.59375\n",
      "2015-12-10T08:09:53.729786: step 852, loss 1.01121, acc 0.625\n",
      "2015-12-10T08:09:53.951248: step 853, loss 1.18675, acc 0.5\n",
      "2015-12-10T08:09:54.173894: step 854, loss 1.12595, acc 0.546875\n",
      "2015-12-10T08:09:54.554967: step 855, loss 1.02456, acc 0.59375\n",
      "2015-12-10T08:09:54.840572: step 856, loss 1.0184, acc 0.59375\n",
      "2015-12-10T08:09:55.077061: step 857, loss 1.22268, acc 0.59375\n",
      "2015-12-10T08:09:55.300119: step 858, loss 1.2222, acc 0.546875\n",
      "2015-12-10T08:09:55.544443: step 859, loss 0.974721, acc 0.53125\n",
      "2015-12-10T08:09:55.760789: step 860, loss 1.13872, acc 0.609375\n",
      "2015-12-10T08:09:55.969315: step 861, loss 0.989749, acc 0.609375\n",
      "2015-12-10T08:09:56.199432: step 862, loss 1.34175, acc 0.515625\n",
      "2015-12-10T08:09:56.405792: step 863, loss 1.02697, acc 0.5625\n",
      "2015-12-10T08:09:56.621946: step 864, loss 1.13133, acc 0.59375\n",
      "2015-12-10T08:09:56.841392: step 865, loss 1.60169, acc 0.46875\n",
      "2015-12-10T08:09:57.068557: step 866, loss 1.15144, acc 0.5625\n",
      "2015-12-10T08:09:57.298543: step 867, loss 1.12667, acc 0.484375\n",
      "2015-12-10T08:09:57.514445: step 868, loss 1.02623, acc 0.53125\n",
      "2015-12-10T08:09:57.740159: step 869, loss 1.01288, acc 0.59375\n",
      "2015-12-10T08:09:57.951387: step 870, loss 0.998012, acc 0.59375\n",
      "2015-12-10T08:09:58.206971: step 871, loss 1.17587, acc 0.53125\n",
      "2015-12-10T08:09:58.431450: step 872, loss 1.11058, acc 0.625\n",
      "2015-12-10T08:09:58.658935: step 873, loss 1.0029, acc 0.609375\n",
      "2015-12-10T08:09:58.880419: step 874, loss 0.966319, acc 0.53125\n",
      "2015-12-10T08:09:59.106215: step 875, loss 1.04307, acc 0.640625\n",
      "2015-12-10T08:09:59.317209: step 876, loss 0.879051, acc 0.640625\n",
      "2015-12-10T08:09:59.535830: step 877, loss 0.95065, acc 0.609375\n",
      "2015-12-10T08:09:59.754241: step 878, loss 1.34426, acc 0.46875\n",
      "2015-12-10T08:09:59.978909: step 879, loss 1.46453, acc 0.46875\n",
      "2015-12-10T08:10:00.206276: step 880, loss 1.22098, acc 0.625\n",
      "2015-12-10T08:10:00.433412: step 881, loss 1.38774, acc 0.4375\n",
      "2015-12-10T08:10:00.659762: step 882, loss 1.21407, acc 0.5625\n",
      "2015-12-10T08:10:00.874176: step 883, loss 1.35369, acc 0.53125\n",
      "2015-12-10T08:10:01.097179: step 884, loss 1.30493, acc 0.5625\n",
      "2015-12-10T08:10:01.315068: step 885, loss 1.08859, acc 0.578125\n",
      "2015-12-10T08:10:01.525727: step 886, loss 1.25172, acc 0.609375\n",
      "2015-12-10T08:10:01.754059: step 887, loss 1.61086, acc 0.46875\n",
      "2015-12-10T08:10:01.970297: step 888, loss 0.967071, acc 0.5625\n",
      "2015-12-10T08:10:02.191765: step 889, loss 1.34326, acc 0.515625\n",
      "2015-12-10T08:10:02.402379: step 890, loss 1.344, acc 0.5625\n",
      "2015-12-10T08:10:02.614438: step 891, loss 1.30193, acc 0.53125\n",
      "2015-12-10T08:10:02.849627: step 892, loss 1.36704, acc 0.546875\n",
      "2015-12-10T08:10:03.071225: step 893, loss 1.19151, acc 0.515625\n",
      "2015-12-10T08:10:03.294010: step 894, loss 1.24474, acc 0.484375\n",
      "2015-12-10T08:10:03.548452: step 895, loss 1.03757, acc 0.59375\n",
      "2015-12-10T08:10:03.755121: step 896, loss 1.01543, acc 0.625\n",
      "2015-12-10T08:10:03.977811: step 897, loss 0.814141, acc 0.65625\n",
      "2015-12-10T08:10:04.192511: step 898, loss 1.22094, acc 0.59375\n",
      "2015-12-10T08:10:04.412179: step 899, loss 1.09283, acc 0.609375\n",
      "2015-12-10T08:10:04.621255: step 900, loss 1.15018, acc 0.53125\n",
      "\n",
      "Dev Set:\n",
      "2015-12-10T08:10:05.398711: step 900, loss 0.725482, acc 0.556\n",
      "\n",
      "Saved model checkpoint to /Users/dennybritz/projects/wildml/cnn-text-classification-tf/runs/1449731188/checkpoints/model-900\n",
      "\n",
      "2015-12-10T08:10:05.770660: step 901, loss 1.4991, acc 0.546875\n",
      "2015-12-10T08:10:06.010859: step 902, loss 1.34784, acc 0.53125\n",
      "2015-12-10T08:10:06.227330: step 903, loss 0.928824, acc 0.640625\n",
      "2015-12-10T08:10:06.439924: step 904, loss 1.06632, acc 0.578125\n",
      "2015-12-10T08:10:06.858333: step 905, loss 0.94264, acc 0.59375\n",
      "2015-12-10T08:10:07.108930: step 906, loss 1.19533, acc 0.483871\n",
      "2015-12-10T08:10:07.355493: step 907, loss 1.32372, acc 0.484375\n",
      "2015-12-10T08:10:07.584937: step 908, loss 0.714208, acc 0.6875\n",
      "2015-12-10T08:10:07.834044: step 909, loss 1.1703, acc 0.59375\n",
      "2015-12-10T08:10:08.084926: step 910, loss 1.18483, acc 0.546875\n",
      "2015-12-10T08:10:08.300010: step 911, loss 0.930286, acc 0.609375\n",
      "2015-12-10T08:10:08.515114: step 912, loss 1.15967, acc 0.453125\n",
      "2015-12-10T08:10:08.732150: step 913, loss 1.21365, acc 0.484375\n",
      "2015-12-10T08:10:08.951758: step 914, loss 0.938499, acc 0.53125\n",
      "2015-12-10T08:10:09.160504: step 915, loss 1.29732, acc 0.546875\n",
      "2015-12-10T08:10:09.373865: step 916, loss 1.20064, acc 0.53125\n",
      "2015-12-10T08:10:09.591229: step 917, loss 0.989939, acc 0.53125\n",
      "2015-12-10T08:10:09.804992: step 918, loss 0.854297, acc 0.640625\n",
      "2015-12-10T08:10:10.023578: step 919, loss 1.10178, acc 0.46875\n",
      "2015-12-10T08:10:10.235058: step 920, loss 0.968653, acc 0.515625\n",
      "2015-12-10T08:10:10.445269: step 921, loss 1.0282, acc 0.65625\n",
      "2015-12-10T08:10:10.667274: step 922, loss 1.19191, acc 0.546875\n",
      "2015-12-10T08:10:10.887785: step 923, loss 1.00892, acc 0.6875\n",
      "2015-12-10T08:10:11.130735: step 924, loss 1.46277, acc 0.515625\n",
      "2015-12-10T08:10:11.355250: step 925, loss 1.15418, acc 0.578125\n",
      "2015-12-10T08:10:11.586527: step 926, loss 1.30114, acc 0.5\n",
      "2015-12-10T08:10:11.806313: step 927, loss 1.12422, acc 0.578125\n",
      "2015-12-10T08:10:12.029110: step 928, loss 0.947213, acc 0.609375\n",
      "2015-12-10T08:10:12.254352: step 929, loss 1.24139, acc 0.546875\n",
      "2015-12-10T08:10:12.472029: step 930, loss 1.12061, acc 0.59375\n",
      "2015-12-10T08:10:12.693002: step 931, loss 1.26875, acc 0.59375\n",
      "2015-12-10T08:10:12.930931: step 932, loss 0.844315, acc 0.703125\n",
      "2015-12-10T08:10:13.164352: step 933, loss 1.07836, acc 0.546875\n",
      "2015-12-10T08:10:13.375966: step 934, loss 1.55607, acc 0.515625\n",
      "2015-12-10T08:10:13.604502: step 935, loss 1.047, acc 0.53125\n",
      "2015-12-10T08:10:13.820637: step 936, loss 1.1448, acc 0.484375\n",
      "2015-12-10T08:10:14.032316: step 937, loss 1.02862, acc 0.5625\n",
      "2015-12-10T08:10:14.246404: step 938, loss 0.966906, acc 0.5\n",
      "2015-12-10T08:10:14.472958: step 939, loss 1.2421, acc 0.578125\n",
      "2015-12-10T08:10:14.682734: step 940, loss 1.11071, acc 0.546875\n",
      "2015-12-10T08:10:14.894172: step 941, loss 1.06253, acc 0.625\n",
      "2015-12-10T08:10:15.110467: step 942, loss 1.52797, acc 0.484375\n",
      "2015-12-10T08:10:15.327830: step 943, loss 0.94869, acc 0.640625\n",
      "2015-12-10T08:10:15.536737: step 944, loss 1.22059, acc 0.5\n",
      "2015-12-10T08:10:15.746635: step 945, loss 1.04544, acc 0.625\n",
      "2015-12-10T08:10:15.974408: step 946, loss 1.43842, acc 0.4375\n",
      "2015-12-10T08:10:16.204775: step 947, loss 1.14108, acc 0.59375\n",
      "2015-12-10T08:10:16.431666: step 948, loss 0.693647, acc 0.65625\n",
      "2015-12-10T08:10:16.657867: step 949, loss 0.69534, acc 0.671875\n",
      "2015-12-10T08:10:16.877122: step 950, loss 0.955803, acc 0.5625\n",
      "2015-12-10T08:10:17.091404: step 951, loss 1.01313, acc 0.609375\n",
      "2015-12-10T08:10:17.310808: step 952, loss 1.11349, acc 0.625\n",
      "2015-12-10T08:10:17.520324: step 953, loss 1.1505, acc 0.5625\n",
      "2015-12-10T08:10:17.732988: step 954, loss 1.12509, acc 0.59375\n",
      "2015-12-10T08:10:17.955309: step 955, loss 1.42976, acc 0.5625\n",
      "2015-12-10T08:10:18.174231: step 956, loss 1.23651, acc 0.515625\n",
      "2015-12-10T08:10:18.415935: step 957, loss 1.50586, acc 0.46875\n",
      "2015-12-10T08:10:18.614050: step 958, loss 1.18774, acc 0.5\n",
      "2015-12-10T08:10:18.830142: step 959, loss 1.358, acc 0.453125\n",
      "2015-12-10T08:10:19.049697: step 960, loss 1.15385, acc 0.515625\n",
      "2015-12-10T08:10:19.265968: step 961, loss 1.55918, acc 0.46875\n",
      "2015-12-10T08:10:19.485469: step 962, loss 1.04662, acc 0.625\n",
      "2015-12-10T08:10:19.702948: step 963, loss 0.938031, acc 0.59375\n",
      "2015-12-10T08:10:19.910169: step 964, loss 1.0521, acc 0.546875\n",
      "2015-12-10T08:10:20.130182: step 965, loss 1.20142, acc 0.453125\n",
      "2015-12-10T08:10:20.354302: step 966, loss 1.18827, acc 0.46875\n",
      "2015-12-10T08:10:20.598778: step 967, loss 1.06966, acc 0.640625\n",
      "2015-12-10T08:10:20.823226: step 968, loss 1.1043, acc 0.515625\n",
      "2015-12-10T08:10:21.078981: step 969, loss 1.23044, acc 0.5\n",
      "2015-12-10T08:10:21.305495: step 970, loss 1.06893, acc 0.625\n",
      "2015-12-10T08:10:21.524423: step 971, loss 1.52471, acc 0.4375\n",
      "2015-12-10T08:10:21.741757: step 972, loss 1.09728, acc 0.5625\n",
      "2015-12-10T08:10:21.986955: step 973, loss 1.00932, acc 0.59375\n",
      "2015-12-10T08:10:22.248185: step 974, loss 0.787959, acc 0.6875\n",
      "2015-12-10T08:10:22.482759: step 975, loss 1.00871, acc 0.65625\n",
      "2015-12-10T08:10:22.724738: step 976, loss 1.02561, acc 0.625\n",
      "2015-12-10T08:10:22.951171: step 977, loss 1.08214, acc 0.59375\n",
      "2015-12-10T08:10:23.192015: step 978, loss 1.1674, acc 0.46875\n",
      "2015-12-10T08:10:23.426234: step 979, loss 1.44968, acc 0.5\n",
      "2015-12-10T08:10:23.647853: step 980, loss 1.03662, acc 0.609375\n",
      "2015-12-10T08:10:23.862400: step 981, loss 1.58381, acc 0.421875\n",
      "2015-12-10T08:10:24.085015: step 982, loss 1.3122, acc 0.453125\n",
      "2015-12-10T08:10:24.313130: step 983, loss 1.5369, acc 0.40625\n",
      "2015-12-10T08:10:24.546710: step 984, loss 1.50372, acc 0.546875\n",
      "2015-12-10T08:10:24.757558: step 985, loss 1.11734, acc 0.578125\n",
      "2015-12-10T08:10:24.983253: step 986, loss 1.22558, acc 0.5\n",
      "2015-12-10T08:10:25.207379: step 987, loss 1.24036, acc 0.53125\n",
      "2015-12-10T08:10:25.427204: step 988, loss 1.17071, acc 0.578125\n",
      "2015-12-10T08:10:25.642288: step 989, loss 1.20249, acc 0.5625\n",
      "2015-12-10T08:10:25.897194: step 990, loss 1.12591, acc 0.59375\n",
      "2015-12-10T08:10:26.121755: step 991, loss 1.32836, acc 0.53125\n",
      "2015-12-10T08:10:26.381471: step 992, loss 1.07751, acc 0.5625\n",
      "2015-12-10T08:10:26.600350: step 993, loss 1.06874, acc 0.609375\n",
      "2015-12-10T08:10:26.834505: step 994, loss 1.28661, acc 0.53125\n",
      "2015-12-10T08:10:27.059832: step 995, loss 1.12358, acc 0.5625\n",
      "2015-12-10T08:10:27.274402: step 996, loss 1.27218, acc 0.46875\n",
      "2015-12-10T08:10:27.502322: step 997, loss 1.10387, acc 0.5625\n",
      "2015-12-10T08:10:27.735760: step 998, loss 1.04897, acc 0.546875\n",
      "2015-12-10T08:10:27.948439: step 999, loss 1.2215, acc 0.546875\n",
      "2015-12-10T08:10:28.175029: step 1000, loss 1.14837, acc 0.59375\n",
      "\n",
      "Dev Set:\n",
      "2015-12-10T08:10:28.888709: step 1000, loss 0.714543, acc 0.567\n",
      "\n",
      "Saved model checkpoint to /Users/dennybritz/projects/wildml/cnn-text-classification-tf/runs/1449731188/checkpoints/model-1000\n",
      "\n",
      "2015-12-10T08:10:29.294014: step 1001, loss 1.52324, acc 0.421875\n",
      "2015-12-10T08:10:29.522586: step 1002, loss 1.11871, acc 0.5\n",
      "2015-12-10T08:10:29.796821: step 1003, loss 1.1614, acc 0.546875\n",
      "2015-12-10T08:10:30.204908: step 1004, loss 1.02057, acc 0.53125\n",
      "2015-12-10T08:10:30.448920: step 1005, loss 1.2238, acc 0.5\n",
      "2015-12-10T08:10:31.170033: step 1006, loss 0.960927, acc 0.609375\n",
      "2015-12-10T08:10:31.597009: step 1007, loss 0.729278, acc 0.640625\n",
      "2015-12-10T08:10:31.868169: step 1008, loss 1.35173, acc 0.5\n",
      "2015-12-10T08:10:32.131479: step 1009, loss 1.13001, acc 0.578125\n",
      "2015-12-10T08:10:32.389729: step 1010, loss 1.21298, acc 0.546875\n",
      "2015-12-10T08:10:32.789799: step 1011, loss 0.995702, acc 0.578125\n",
      "2015-12-10T08:10:33.098758: step 1012, loss 1.7126, acc 0.46875\n",
      "2015-12-10T08:10:33.415142: step 1013, loss 1.23345, acc 0.46875\n",
      "2015-12-10T08:10:33.762007: step 1014, loss 1.24817, acc 0.453125\n",
      "2015-12-10T08:10:34.104931: step 1015, loss 1.17695, acc 0.53125\n",
      "2015-12-10T08:10:34.354637: step 1016, loss 1.1322, acc 0.53125\n",
      "2015-12-10T08:10:34.665468: step 1017, loss 1.08977, acc 0.59375\n",
      "2015-12-10T08:10:34.886257: step 1018, loss 1.19579, acc 0.5625\n",
      "2015-12-10T08:10:35.115982: step 1019, loss 1.11594, acc 0.53125\n",
      "2015-12-10T08:10:35.328932: step 1020, loss 1.28923, acc 0.53125\n",
      "2015-12-10T08:10:35.557981: step 1021, loss 1.39762, acc 0.453125\n",
      "2015-12-10T08:10:35.777704: step 1022, loss 0.852546, acc 0.71875\n",
      "2015-12-10T08:10:35.988530: step 1023, loss 1.48517, acc 0.484375\n",
      "2015-12-10T08:10:36.213079: step 1024, loss 1.09701, acc 0.609375\n",
      "2015-12-10T08:10:36.427535: step 1025, loss 1.02315, acc 0.578125\n",
      "2015-12-10T08:10:36.643488: step 1026, loss 1.19998, acc 0.53125\n",
      "2015-12-10T08:10:36.876691: step 1027, loss 1.21956, acc 0.46875\n",
      "2015-12-10T08:10:37.097993: step 1028, loss 1.16792, acc 0.578125\n",
      "2015-12-10T08:10:37.321281: step 1029, loss 1.31371, acc 0.515625\n",
      "2015-12-10T08:10:37.547800: step 1030, loss 1.14374, acc 0.53125\n",
      "2015-12-10T08:10:37.766814: step 1031, loss 1.20481, acc 0.5\n",
      "2015-12-10T08:10:37.999858: step 1032, loss 0.955693, acc 0.546875\n",
      "2015-12-10T08:10:38.219463: step 1033, loss 0.985194, acc 0.65625\n",
      "2015-12-10T08:10:38.438293: step 1034, loss 1.26588, acc 0.515625\n",
      "2015-12-10T08:10:38.665214: step 1035, loss 1.06641, acc 0.609375\n",
      "2015-12-10T08:10:38.893677: step 1036, loss 1.01871, acc 0.5625\n",
      "2015-12-10T08:10:39.130893: step 1037, loss 1.27923, acc 0.546875\n",
      "2015-12-10T08:10:39.368659: step 1038, loss 1.36812, acc 0.5625\n",
      "2015-12-10T08:10:39.606644: step 1039, loss 1.58874, acc 0.453125\n",
      "2015-12-10T08:10:39.828327: step 1040, loss 1.02021, acc 0.59375\n",
      "2015-12-10T08:10:40.051174: step 1041, loss 1.0573, acc 0.546875\n",
      "2015-12-10T08:10:40.270570: step 1042, loss 0.988683, acc 0.609375\n",
      "2015-12-10T08:10:40.485053: step 1043, loss 1.28992, acc 0.5\n",
      "2015-12-10T08:10:40.694711: step 1044, loss 1.1424, acc 0.578125\n",
      "2015-12-10T08:10:40.919103: step 1045, loss 1.49316, acc 0.484375\n",
      "2015-12-10T08:10:41.149350: step 1046, loss 1.28213, acc 0.4375\n",
      "2015-12-10T08:10:41.363565: step 1047, loss 0.977592, acc 0.5\n",
      "2015-12-10T08:10:41.584409: step 1048, loss 1.20511, acc 0.53125\n",
      "2015-12-10T08:10:41.800696: step 1049, loss 0.909312, acc 0.578125\n",
      "2015-12-10T08:10:42.026363: step 1050, loss 1.16047, acc 0.59375\n",
      "2015-12-10T08:10:42.246785: step 1051, loss 1.13173, acc 0.515625\n",
      "2015-12-10T08:10:42.465011: step 1052, loss 1.48866, acc 0.515625\n",
      "2015-12-10T08:10:42.691972: step 1053, loss 1.18286, acc 0.546875\n",
      "2015-12-10T08:10:42.915494: step 1054, loss 0.930434, acc 0.640625\n",
      "2015-12-10T08:10:43.126964: step 1055, loss 1.19603, acc 0.53125\n",
      "2015-12-10T08:10:43.342391: step 1056, loss 1.16367, acc 0.546875\n",
      "2015-12-10T08:10:43.569330: step 1057, loss 0.994578, acc 0.580645\n",
      "2015-12-10T08:10:43.819906: step 1058, loss 1.37154, acc 0.5\n",
      "2015-12-10T08:10:44.040181: step 1059, loss 0.986312, acc 0.53125\n",
      "2015-12-10T08:10:44.266238: step 1060, loss 0.92753, acc 0.609375\n",
      "2015-12-10T08:10:44.482355: step 1061, loss 0.911563, acc 0.6875\n",
      "2015-12-10T08:10:44.711352: step 1062, loss 1.01954, acc 0.59375\n",
      "2015-12-10T08:10:44.933022: step 1063, loss 1.17933, acc 0.484375\n",
      "2015-12-10T08:10:45.142151: step 1064, loss 1.00549, acc 0.5625\n",
      "2015-12-10T08:10:45.371110: step 1065, loss 1.55916, acc 0.515625\n",
      "2015-12-10T08:10:45.589711: step 1066, loss 1.18017, acc 0.578125\n",
      "2015-12-10T08:10:45.817414: step 1067, loss 1.25638, acc 0.5\n",
      "2015-12-10T08:10:46.025418: step 1068, loss 0.981431, acc 0.59375\n",
      "2015-12-10T08:10:46.238667: step 1069, loss 1.0827, acc 0.515625\n",
      "2015-12-10T08:10:46.467527: step 1070, loss 1.08254, acc 0.5625\n",
      "2015-12-10T08:10:46.678781: step 1071, loss 0.985345, acc 0.625\n",
      "2015-12-10T08:10:46.889466: step 1072, loss 1.01091, acc 0.578125\n",
      "2015-12-10T08:10:47.099432: step 1073, loss 1.02116, acc 0.484375\n",
      "2015-12-10T08:10:47.319061: step 1074, loss 1.51569, acc 0.421875\n",
      "2015-12-10T08:10:47.529777: step 1075, loss 1.0595, acc 0.53125\n",
      "2015-12-10T08:10:47.769339: step 1076, loss 1.29626, acc 0.484375\n",
      "2015-12-10T08:10:48.003402: step 1077, loss 1.0129, acc 0.640625\n",
      "2015-12-10T08:10:48.216198: step 1078, loss 1.07393, acc 0.609375\n",
      "2015-12-10T08:10:48.441484: step 1079, loss 1.07912, acc 0.609375\n",
      "2015-12-10T08:10:48.673452: step 1080, loss 0.941295, acc 0.65625\n",
      "2015-12-10T08:10:48.891739: step 1081, loss 0.836196, acc 0.59375\n",
      "2015-12-10T08:10:49.113564: step 1082, loss 0.963249, acc 0.640625\n",
      "2015-12-10T08:10:49.320966: step 1083, loss 1.27641, acc 0.53125\n",
      "2015-12-10T08:10:49.568666: step 1084, loss 0.907412, acc 0.546875\n",
      "2015-12-10T08:10:49.806557: step 1085, loss 1.05957, acc 0.578125\n",
      "2015-12-10T08:10:50.019814: step 1086, loss 0.957199, acc 0.609375\n",
      "2015-12-10T08:10:50.280560: step 1087, loss 0.966532, acc 0.546875\n",
      "2015-12-10T08:10:50.500557: step 1088, loss 1.11511, acc 0.515625\n",
      "2015-12-10T08:10:50.716005: step 1089, loss 1.17158, acc 0.546875\n",
      "2015-12-10T08:10:50.955948: step 1090, loss 1.17959, acc 0.484375\n",
      "2015-12-10T08:10:51.166975: step 1091, loss 1.14024, acc 0.65625\n",
      "2015-12-10T08:10:51.399025: step 1092, loss 1.1958, acc 0.5625\n",
      "2015-12-10T08:10:51.621021: step 1093, loss 0.920479, acc 0.53125\n",
      "2015-12-10T08:10:51.842908: step 1094, loss 1.09027, acc 0.5625\n",
      "2015-12-10T08:10:52.054109: step 1095, loss 1.28408, acc 0.5625\n",
      "2015-12-10T08:10:52.267940: step 1096, loss 1.20597, acc 0.546875\n",
      "2015-12-10T08:10:52.480764: step 1097, loss 1.14132, acc 0.5625\n",
      "2015-12-10T08:10:52.691976: step 1098, loss 1.07507, acc 0.515625\n",
      "2015-12-10T08:10:52.904007: step 1099, loss 1.10798, acc 0.53125\n",
      "2015-12-10T08:10:53.125963: step 1100, loss 0.988667, acc 0.59375\n",
      "\n",
      "Dev Set:\n",
      "2015-12-10T08:10:53.891326: step 1100, loss 0.71042, acc 0.564\n",
      "\n",
      "Saved model checkpoint to /Users/dennybritz/projects/wildml/cnn-text-classification-tf/runs/1449731188/checkpoints/model-1100\n",
      "\n",
      "2015-12-10T08:10:54.319950: step 1101, loss 1.21901, acc 0.59375\n",
      "2015-12-10T08:10:54.538473: step 1102, loss 0.940605, acc 0.59375\n",
      "2015-12-10T08:10:54.762451: step 1103, loss 1.09047, acc 0.578125\n",
      "2015-12-10T08:10:54.981559: step 1104, loss 0.861938, acc 0.65625\n",
      "2015-12-10T08:10:55.191669: step 1105, loss 1.08944, acc 0.609375\n",
      "2015-12-10T08:10:55.655658: step 1106, loss 1.2585, acc 0.546875\n",
      "2015-12-10T08:10:55.941213: step 1107, loss 0.83051, acc 0.59375\n",
      "2015-12-10T08:10:56.203545: step 1108, loss 1.56801, acc 0.484375\n",
      "2015-12-10T08:10:56.429935: step 1109, loss 0.934322, acc 0.5625\n",
      "2015-12-10T08:10:56.669126: step 1110, loss 1.18508, acc 0.5\n",
      "2015-12-10T08:10:56.885260: step 1111, loss 0.771387, acc 0.609375\n",
      "2015-12-10T08:10:57.253182: step 1112, loss 1.11028, acc 0.578125\n",
      "2015-12-10T08:10:57.539323: step 1113, loss 1.0623, acc 0.515625\n",
      "2015-12-10T08:10:57.759063: step 1114, loss 1.09248, acc 0.515625\n",
      "2015-12-10T08:10:58.001212: step 1115, loss 1.14486, acc 0.53125\n",
      "2015-12-10T08:10:58.232072: step 1116, loss 1.20969, acc 0.625\n",
      "2015-12-10T08:10:58.483899: step 1117, loss 1.14157, acc 0.5625\n",
      "2015-12-10T08:10:58.695782: step 1118, loss 1.1409, acc 0.5625\n",
      "2015-12-10T08:10:58.929309: step 1119, loss 1.30154, acc 0.53125\n",
      "2015-12-10T08:10:59.156436: step 1120, loss 1.11129, acc 0.546875\n",
      "2015-12-10T08:10:59.377662: step 1121, loss 1.05664, acc 0.609375\n",
      "2015-12-10T08:10:59.600454: step 1122, loss 1.02631, acc 0.609375\n",
      "2015-12-10T08:10:59.819819: step 1123, loss 1.00407, acc 0.625\n",
      "2015-12-10T08:11:00.052052: step 1124, loss 1.35801, acc 0.53125\n",
      "2015-12-10T08:11:00.287615: step 1125, loss 0.913571, acc 0.65625\n",
      "2015-12-10T08:11:00.511705: step 1126, loss 1.08997, acc 0.578125\n",
      "2015-12-10T08:11:00.754382: step 1127, loss 1.04634, acc 0.640625\n",
      "2015-12-10T08:11:00.987059: step 1128, loss 0.818994, acc 0.578125\n",
      "2015-12-10T08:11:01.192787: step 1129, loss 0.657894, acc 0.6875\n",
      "2015-12-10T08:11:01.415337: step 1130, loss 1.20634, acc 0.59375\n",
      "2015-12-10T08:11:01.628701: step 1131, loss 1.02481, acc 0.625\n",
      "2015-12-10T08:11:01.856845: step 1132, loss 1.25336, acc 0.5\n",
      "2015-12-10T08:11:02.068945: step 1133, loss 1.0914, acc 0.5625\n",
      "2015-12-10T08:11:02.276154: step 1134, loss 1.0966, acc 0.609375\n",
      "2015-12-10T08:11:02.497119: step 1135, loss 1.07569, acc 0.640625\n",
      "2015-12-10T08:11:02.710348: step 1136, loss 1.22993, acc 0.578125\n",
      "2015-12-10T08:11:02.936989: step 1137, loss 0.978138, acc 0.515625\n",
      "2015-12-10T08:11:03.139588: step 1138, loss 0.922401, acc 0.578125\n",
      "2015-12-10T08:11:03.355634: step 1139, loss 1.25696, acc 0.5\n",
      "2015-12-10T08:11:03.579431: step 1140, loss 1.33954, acc 0.5\n",
      "2015-12-10T08:11:03.793686: step 1141, loss 1.15929, acc 0.484375\n",
      "2015-12-10T08:11:04.009397: step 1142, loss 0.895127, acc 0.625\n",
      "2015-12-10T08:11:04.215622: step 1143, loss 1.28082, acc 0.53125\n",
      "2015-12-10T08:11:04.428805: step 1144, loss 1.17968, acc 0.53125\n",
      "2015-12-10T08:11:04.646348: step 1145, loss 1.11109, acc 0.515625\n",
      "2015-12-10T08:11:04.887805: step 1146, loss 1.06573, acc 0.578125\n",
      "2015-12-10T08:11:05.100199: step 1147, loss 0.848946, acc 0.609375\n",
      "2015-12-10T08:11:05.307459: step 1148, loss 0.970704, acc 0.515625\n",
      "2015-12-10T08:11:05.526073: step 1149, loss 1.34267, acc 0.46875\n",
      "2015-12-10T08:11:05.740767: step 1150, loss 1.18151, acc 0.53125\n",
      "2015-12-10T08:11:05.969443: step 1151, loss 0.84642, acc 0.640625\n",
      "2015-12-10T08:11:06.205561: step 1152, loss 0.947565, acc 0.609375\n",
      "2015-12-10T08:11:06.418166: step 1153, loss 1.03252, acc 0.625\n",
      "2015-12-10T08:11:06.640470: step 1154, loss 1.18444, acc 0.59375\n",
      "2015-12-10T08:11:06.857181: step 1155, loss 1.27757, acc 0.5\n",
      "2015-12-10T08:11:07.071857: step 1156, loss 1.01392, acc 0.53125\n",
      "2015-12-10T08:11:07.277817: step 1157, loss 0.932241, acc 0.625\n",
      "2015-12-10T08:11:07.507518: step 1158, loss 1.20574, acc 0.53125\n",
      "2015-12-10T08:11:07.730451: step 1159, loss 1.03603, acc 0.53125\n",
      "2015-12-10T08:11:07.952472: step 1160, loss 1.10461, acc 0.46875\n",
      "2015-12-10T08:11:08.153416: step 1161, loss 1.15203, acc 0.59375\n",
      "2015-12-10T08:11:08.375288: step 1162, loss 1.11791, acc 0.546875\n",
      "2015-12-10T08:11:08.604719: step 1163, loss 1.42696, acc 0.53125\n",
      "2015-12-10T08:11:08.832749: step 1164, loss 0.99795, acc 0.59375\n",
      "2015-12-10T08:11:09.061389: step 1165, loss 0.944399, acc 0.671875\n",
      "2015-12-10T08:11:09.266638: step 1166, loss 1.0758, acc 0.515625\n",
      "2015-12-10T08:11:09.497645: step 1167, loss 0.78454, acc 0.734375\n",
      "2015-12-10T08:11:09.722720: step 1168, loss 0.951918, acc 0.53125\n",
      "2015-12-10T08:11:09.935937: step 1169, loss 1.31421, acc 0.515625\n",
      "2015-12-10T08:11:10.149521: step 1170, loss 0.847389, acc 0.65625\n",
      "2015-12-10T08:11:10.362832: step 1171, loss 1.35371, acc 0.484375\n",
      "2015-12-10T08:11:10.585515: step 1172, loss 0.731795, acc 0.640625\n",
      "2015-12-10T08:11:10.804456: step 1173, loss 0.725076, acc 0.671875\n",
      "2015-12-10T08:11:11.027180: step 1174, loss 0.91743, acc 0.578125\n",
      "2015-12-10T08:11:11.229832: step 1175, loss 1.01856, acc 0.609375\n",
      "2015-12-10T08:11:11.440368: step 1176, loss 0.990151, acc 0.65625\n",
      "2015-12-10T08:11:11.664873: step 1177, loss 1.13295, acc 0.53125\n",
      "2015-12-10T08:11:11.884803: step 1178, loss 1.03979, acc 0.609375\n",
      "2015-12-10T08:11:12.091602: step 1179, loss 0.849163, acc 0.6875\n",
      "2015-12-10T08:11:12.297903: step 1180, loss 0.858364, acc 0.609375\n",
      "2015-12-10T08:11:12.515012: step 1181, loss 1.09919, acc 0.546875\n",
      "2015-12-10T08:11:12.728576: step 1182, loss 0.84313, acc 0.625\n",
      "2015-12-10T08:11:12.945698: step 1183, loss 1.14068, acc 0.578125\n",
      "2015-12-10T08:11:13.152413: step 1184, loss 0.911658, acc 0.609375\n",
      "2015-12-10T08:11:13.362348: step 1185, loss 0.886144, acc 0.59375\n",
      "2015-12-10T08:11:13.603379: step 1186, loss 1.05888, acc 0.609375\n",
      "2015-12-10T08:11:13.820101: step 1187, loss 0.998387, acc 0.546875\n",
      "2015-12-10T08:11:14.036510: step 1188, loss 0.810158, acc 0.671875\n",
      "2015-12-10T08:11:14.249860: step 1189, loss 0.822896, acc 0.71875\n",
      "2015-12-10T08:11:14.472067: step 1190, loss 1.21668, acc 0.59375\n",
      "2015-12-10T08:11:14.695122: step 1191, loss 1.05075, acc 0.578125\n",
      "2015-12-10T08:11:14.908026: step 1192, loss 0.945162, acc 0.609375\n",
      "2015-12-10T08:11:15.117002: step 1193, loss 0.675788, acc 0.671875\n",
      "2015-12-10T08:11:15.319672: step 1194, loss 0.963863, acc 0.6875\n",
      "2015-12-10T08:11:15.539990: step 1195, loss 1.11034, acc 0.53125\n",
      "2015-12-10T08:11:15.759849: step 1196, loss 1.18735, acc 0.53125\n",
      "2015-12-10T08:11:15.983945: step 1197, loss 0.857001, acc 0.640625\n",
      "2015-12-10T08:11:16.196043: step 1198, loss 1.1251, acc 0.515625\n",
      "2015-12-10T08:11:16.420768: step 1199, loss 1.10623, acc 0.578125\n",
      "2015-12-10T08:11:16.645285: step 1200, loss 1.23237, acc 0.484375\n",
      "\n",
      "Dev Set:\n",
      "2015-12-10T08:11:17.365133: step 1200, loss 0.702322, acc 0.583\n",
      "\n",
      "Saved model checkpoint to /Users/dennybritz/projects/wildml/cnn-text-classification-tf/runs/1449731188/checkpoints/model-1200\n",
      "\n",
      "2015-12-10T08:11:17.758426: step 1201, loss 0.768717, acc 0.71875\n",
      "2015-12-10T08:11:17.985789: step 1202, loss 1.26909, acc 0.46875\n",
      "2015-12-10T08:11:18.202259: step 1203, loss 1.12456, acc 0.5625\n",
      "2015-12-10T08:11:18.424432: step 1204, loss 1.07779, acc 0.515625\n",
      "2015-12-10T08:11:18.662197: step 1205, loss 1.15319, acc 0.515625\n",
      "2015-12-10T08:11:18.888971: step 1206, loss 1.13406, acc 0.546875\n",
      "2015-12-10T08:11:19.291332: step 1207, loss 1.22776, acc 0.484375\n",
      "2015-12-10T08:11:19.596970: step 1208, loss 1.19147, acc 0.580645\n",
      "2015-12-10T08:11:19.829784: step 1209, loss 1.03863, acc 0.5625\n",
      "2015-12-10T08:11:20.044382: step 1210, loss 0.835539, acc 0.6875\n",
      "2015-12-10T08:11:20.261413: step 1211, loss 1.25462, acc 0.5\n",
      "2015-12-10T08:11:20.485985: step 1212, loss 1.02086, acc 0.515625\n",
      "2015-12-10T08:11:20.713313: step 1213, loss 0.979915, acc 0.5625\n",
      "2015-12-10T08:11:20.936837: step 1214, loss 1.37095, acc 0.515625\n",
      "2015-12-10T08:11:21.148623: step 1215, loss 1.21756, acc 0.53125\n",
      "2015-12-10T08:11:21.357298: step 1216, loss 1.00856, acc 0.578125\n",
      "2015-12-10T08:11:21.583522: step 1217, loss 0.990157, acc 0.53125\n",
      "2015-12-10T08:11:21.805118: step 1218, loss 1.39341, acc 0.484375\n",
      "2015-12-10T08:11:22.016736: step 1219, loss 1.24657, acc 0.546875\n",
      "2015-12-10T08:11:22.226185: step 1220, loss 1.09606, acc 0.578125\n",
      "2015-12-10T08:11:22.452813: step 1221, loss 1.12919, acc 0.5\n",
      "2015-12-10T08:11:22.668369: step 1222, loss 0.837229, acc 0.609375\n",
      "2015-12-10T08:11:22.883894: step 1223, loss 1.11711, acc 0.5\n",
      "2015-12-10T08:11:23.097936: step 1224, loss 1.04422, acc 0.53125\n",
      "2015-12-10T08:11:23.306321: step 1225, loss 1.13719, acc 0.53125\n",
      "2015-12-10T08:11:23.529874: step 1226, loss 1.03292, acc 0.5625\n",
      "2015-12-10T08:11:23.755400: step 1227, loss 0.819241, acc 0.625\n",
      "2015-12-10T08:11:23.972159: step 1228, loss 0.901568, acc 0.59375\n",
      "2015-12-10T08:11:24.187834: step 1229, loss 1.03047, acc 0.5625\n",
      "2015-12-10T08:11:24.403295: step 1230, loss 0.983026, acc 0.5625\n",
      "2015-12-10T08:11:24.620647: step 1231, loss 1.1053, acc 0.5625\n",
      "2015-12-10T08:11:24.852689: step 1232, loss 1.21235, acc 0.546875\n",
      "2015-12-10T08:11:25.074209: step 1233, loss 0.914281, acc 0.671875\n",
      "2015-12-10T08:11:25.281390: step 1234, loss 1.50229, acc 0.484375\n",
      "2015-12-10T08:11:25.506778: step 1235, loss 1.0976, acc 0.5625\n",
      "2015-12-10T08:11:25.733022: step 1236, loss 1.46856, acc 0.53125\n",
      "2015-12-10T08:11:25.957448: step 1237, loss 1.13718, acc 0.5625\n",
      "2015-12-10T08:11:26.169180: step 1238, loss 0.913524, acc 0.578125\n",
      "2015-12-10T08:11:26.400319: step 1239, loss 0.835772, acc 0.640625\n",
      "2015-12-10T08:11:26.628014: step 1240, loss 0.939598, acc 0.640625\n",
      "2015-12-10T08:11:26.856775: step 1241, loss 0.877738, acc 0.625\n",
      "2015-12-10T08:11:27.077849: step 1242, loss 0.820425, acc 0.59375\n",
      "2015-12-10T08:11:27.306783: step 1243, loss 1.3192, acc 0.5\n",
      "2015-12-10T08:11:27.524687: step 1244, loss 0.955701, acc 0.59375\n",
      "2015-12-10T08:11:27.745228: step 1245, loss 0.821721, acc 0.671875\n",
      "2015-12-10T08:11:27.980328: step 1246, loss 0.837816, acc 0.640625\n",
      "2015-12-10T08:11:28.200260: step 1247, loss 0.77245, acc 0.671875\n",
      "2015-12-10T08:11:28.422229: step 1248, loss 0.899657, acc 0.609375\n",
      "2015-12-10T08:11:28.667643: step 1249, loss 1.02649, acc 0.546875\n",
      "2015-12-10T08:11:28.932342: step 1250, loss 1.35132, acc 0.40625\n",
      "2015-12-10T08:11:29.169431: step 1251, loss 0.968735, acc 0.53125\n",
      "2015-12-10T08:11:29.382410: step 1252, loss 0.919714, acc 0.640625\n",
      "2015-12-10T08:11:29.605748: step 1253, loss 1.02497, acc 0.578125\n",
      "2015-12-10T08:11:29.820355: step 1254, loss 0.784188, acc 0.65625\n",
      "2015-12-10T08:11:30.033407: step 1255, loss 0.818586, acc 0.6875\n",
      "2015-12-10T08:11:30.263659: step 1256, loss 1.0867, acc 0.46875\n",
      "2015-12-10T08:11:30.530708: step 1257, loss 0.971731, acc 0.59375\n",
      "2015-12-10T08:11:30.756057: step 1258, loss 1.11565, acc 0.5\n",
      "2015-12-10T08:11:30.982160: step 1259, loss 1.02415, acc 0.53125\n",
      "2015-12-10T08:11:31.220254: step 1260, loss 0.953065, acc 0.515625\n",
      "2015-12-10T08:11:31.430728: step 1261, loss 0.909583, acc 0.640625\n",
      "2015-12-10T08:11:31.655628: step 1262, loss 1.04426, acc 0.5625\n",
      "2015-12-10T08:11:31.877369: step 1263, loss 0.778948, acc 0.609375\n",
      "2015-12-10T08:11:32.103360: step 1264, loss 0.929881, acc 0.609375\n",
      "2015-12-10T08:11:32.337098: step 1265, loss 0.823551, acc 0.5625\n",
      "2015-12-10T08:11:32.562178: step 1266, loss 0.908505, acc 0.625\n",
      "2015-12-10T08:11:32.777211: step 1267, loss 1.00722, acc 0.65625\n",
      "2015-12-10T08:11:32.996825: step 1268, loss 0.958401, acc 0.625\n",
      "2015-12-10T08:11:33.221910: step 1269, loss 1.29967, acc 0.453125\n",
      "2015-12-10T08:11:33.436268: step 1270, loss 1.2289, acc 0.59375\n",
      "2015-12-10T08:11:33.668817: step 1271, loss 0.911137, acc 0.578125\n",
      "2015-12-10T08:11:33.888057: step 1272, loss 0.771941, acc 0.640625\n",
      "2015-12-10T08:11:34.103895: step 1273, loss 1.04425, acc 0.546875\n",
      "2015-12-10T08:11:34.328960: step 1274, loss 0.875069, acc 0.671875\n",
      "2015-12-10T08:11:34.538092: step 1275, loss 1.27181, acc 0.53125\n",
      "2015-12-10T08:11:34.795915: step 1276, loss 0.885383, acc 0.53125\n",
      "2015-12-10T08:11:35.006387: step 1277, loss 0.881939, acc 0.59375\n",
      "2015-12-10T08:11:35.220113: step 1278, loss 1.00275, acc 0.5625\n",
      "2015-12-10T08:11:35.450076: step 1279, loss 0.819435, acc 0.59375\n",
      "2015-12-10T08:11:35.679997: step 1280, loss 1.18598, acc 0.46875\n",
      "2015-12-10T08:11:35.904057: step 1281, loss 1.21999, acc 0.5625\n",
      "2015-12-10T08:11:36.117919: step 1282, loss 1.03973, acc 0.65625\n",
      "2015-12-10T08:11:36.327518: step 1283, loss 1.053, acc 0.546875\n",
      "2015-12-10T08:11:36.578047: step 1284, loss 0.965977, acc 0.640625\n",
      "2015-12-10T08:11:36.790344: step 1285, loss 1.05495, acc 0.609375\n",
      "2015-12-10T08:11:37.017735: step 1286, loss 1.11438, acc 0.59375\n",
      "2015-12-10T08:11:37.233890: step 1287, loss 1.14218, acc 0.5625\n",
      "2015-12-10T08:11:37.483036: step 1288, loss 1.06724, acc 0.5625\n",
      "2015-12-10T08:11:37.699344: step 1289, loss 0.92757, acc 0.546875\n",
      "2015-12-10T08:11:37.917610: step 1290, loss 1.08796, acc 0.546875\n",
      "2015-12-10T08:11:38.137123: step 1291, loss 0.729995, acc 0.671875\n",
      "2015-12-10T08:11:38.359842: step 1292, loss 0.927684, acc 0.609375\n",
      "2015-12-10T08:11:38.588120: step 1293, loss 0.859894, acc 0.671875\n",
      "2015-12-10T08:11:38.810617: step 1294, loss 0.759651, acc 0.640625\n",
      "2015-12-10T08:11:39.019955: step 1295, loss 0.884306, acc 0.609375\n",
      "2015-12-10T08:11:39.285732: step 1296, loss 1.23671, acc 0.515625\n",
      "2015-12-10T08:11:39.508223: step 1297, loss 0.81343, acc 0.59375\n",
      "2015-12-10T08:11:39.736025: step 1298, loss 1.26539, acc 0.5\n",
      "2015-12-10T08:11:39.962035: step 1299, loss 0.988194, acc 0.578125\n",
      "2015-12-10T08:11:40.177629: step 1300, loss 0.90531, acc 0.59375\n",
      "\n",
      "Dev Set:\n",
      "2015-12-10T08:11:40.907582: step 1300, loss 0.69652, acc 0.587\n",
      "\n",
      "Saved model checkpoint to /Users/dennybritz/projects/wildml/cnn-text-classification-tf/runs/1449731188/checkpoints/model-1300\n",
      "\n",
      "2015-12-10T08:11:41.309170: step 1301, loss 1.39377, acc 0.5\n",
      "2015-12-10T08:11:41.532030: step 1302, loss 1.1614, acc 0.5625\n",
      "2015-12-10T08:11:41.789345: step 1303, loss 1.13864, acc 0.515625\n",
      "2015-12-10T08:11:42.014083: step 1304, loss 0.96993, acc 0.59375\n",
      "2015-12-10T08:11:42.229779: step 1305, loss 1.2411, acc 0.5\n",
      "2015-12-10T08:11:42.681990: step 1306, loss 1.03937, acc 0.59375\n",
      "2015-12-10T08:11:43.005840: step 1307, loss 1.01338, acc 0.59375\n",
      "2015-12-10T08:11:43.240693: step 1308, loss 1.12977, acc 0.484375\n",
      "2015-12-10T08:11:43.485943: step 1309, loss 1.02345, acc 0.5625\n",
      "2015-12-10T08:11:43.755449: step 1310, loss 0.809641, acc 0.671875\n",
      "2015-12-10T08:11:43.990815: step 1311, loss 1.12537, acc 0.53125\n",
      "2015-12-10T08:11:44.217260: step 1312, loss 1.15017, acc 0.5\n",
      "2015-12-10T08:11:44.444686: step 1313, loss 1.2287, acc 0.5\n",
      "2015-12-10T08:11:44.892559: step 1314, loss 1.11437, acc 0.484375\n",
      "2015-12-10T08:11:45.152020: step 1315, loss 1.04364, acc 0.546875\n",
      "2015-12-10T08:11:45.366715: step 1316, loss 0.981809, acc 0.5625\n",
      "2015-12-10T08:11:45.600311: step 1317, loss 0.667749, acc 0.65625\n",
      "2015-12-10T08:11:45.833084: step 1318, loss 0.91059, acc 0.640625\n",
      "2015-12-10T08:11:46.059775: step 1319, loss 1.16318, acc 0.484375\n",
      "2015-12-10T08:11:46.274476: step 1320, loss 0.87658, acc 0.609375\n",
      "2015-12-10T08:11:46.502303: step 1321, loss 1.27785, acc 0.5625\n",
      "2015-12-10T08:11:46.715618: step 1322, loss 0.851293, acc 0.65625\n",
      "2015-12-10T08:11:46.940863: step 1323, loss 1.22792, acc 0.5\n",
      "2015-12-10T08:11:47.167118: step 1324, loss 1.40954, acc 0.484375\n",
      "2015-12-10T08:11:47.392850: step 1325, loss 1.0372, acc 0.515625\n",
      "2015-12-10T08:11:47.631578: step 1326, loss 1.17988, acc 0.5\n",
      "2015-12-10T08:11:47.864347: step 1327, loss 0.913096, acc 0.609375\n",
      "2015-12-10T08:11:48.093144: step 1328, loss 0.981066, acc 0.5625\n",
      "2015-12-10T08:11:48.320205: step 1329, loss 0.96106, acc 0.546875\n",
      "2015-12-10T08:11:48.532647: step 1330, loss 1.27083, acc 0.46875\n",
      "2015-12-10T08:11:48.814973: step 1331, loss 0.940133, acc 0.59375\n",
      "2015-12-10T08:11:49.080667: step 1332, loss 0.918986, acc 0.609375\n",
      "2015-12-10T08:11:49.328869: step 1333, loss 1.10638, acc 0.53125\n",
      "2015-12-10T08:11:49.580522: step 1334, loss 0.88946, acc 0.578125\n",
      "2015-12-10T08:11:49.823475: step 1335, loss 1.28617, acc 0.484375\n",
      "2015-12-10T08:11:50.059057: step 1336, loss 1.04633, acc 0.578125\n",
      "2015-12-10T08:11:50.279871: step 1337, loss 1.33332, acc 0.5\n",
      "2015-12-10T08:11:50.513248: step 1338, loss 1.07483, acc 0.421875\n",
      "2015-12-10T08:11:50.747407: step 1339, loss 1.31371, acc 0.46875\n",
      "2015-12-10T08:11:50.961210: step 1340, loss 1.0697, acc 0.640625\n",
      "2015-12-10T08:11:51.185445: step 1341, loss 1.00036, acc 0.5625\n",
      "2015-12-10T08:11:51.415128: step 1342, loss 0.929477, acc 0.609375\n",
      "2015-12-10T08:11:51.644003: step 1343, loss 1.17142, acc 0.5\n",
      "2015-12-10T08:11:51.879979: step 1344, loss 0.939727, acc 0.65625\n",
      "2015-12-10T08:11:52.091316: step 1345, loss 1.05539, acc 0.578125\n",
      "2015-12-10T08:11:52.367981: step 1346, loss 1.27246, acc 0.53125\n",
      "2015-12-10T08:11:52.764290: step 1347, loss 0.958191, acc 0.59375\n",
      "2015-12-10T08:11:53.023646: step 1348, loss 0.913691, acc 0.65625\n",
      "2015-12-10T08:11:53.263472: step 1349, loss 0.901697, acc 0.578125\n",
      "2015-12-10T08:11:53.490277: step 1350, loss 0.703877, acc 0.625\n",
      "2015-12-10T08:11:53.744190: step 1351, loss 0.985363, acc 0.578125\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-7275fcc19fd0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m             \u001b[0mcurrent_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcurrent_step\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mEVALUATE_EVERY\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-7275fcc19fd0>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(x_batch, y_batch)\u001b[0m\n\u001b[1;32m     66\u001b[0m             _, step, summaries, loss, accuracy = sess.run(\n\u001b[1;32m     67\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_summary_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m                 feed_dict)\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0mtime_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misoformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}: step {}, loss {:g}, acc {:g}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dennybritz/projects/python-venvs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munique_fetch_targets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[0;31m# User may have fetched the same tensor multiple times, but we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dennybritz/projects/python-venvs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, target_list, fetch_list, feed_dict)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m       return tf_session.TF_Run(self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 404\u001b[0;31m                                target_list)\n\u001b[0m\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatusNotOK\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 500\n",
    "EVALUATE_EVERY = CHECKPOINT_EVERY = 100\n",
    "DROPOUT_KEEP_PROB = 0.5\n",
    "SEQUENCE_LENGTH = x_train.shape[1]\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=True)      \n",
    "    sess = tf.Session(config=session_conf)  \n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(\n",
    "            vocabulary_size=len(vocabulary),\n",
    "            sequence_length=SEQUENCE_LENGTH,\n",
    "            num_classes=2,\n",
    "            embedding_size=128,\n",
    "            filter_sizes=[3, 4, 5],\n",
    "            num_filters=80)\n",
    "        \n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\")\n",
    "        optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "        \n",
    "        # Keep track of gradient values and sparsity\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.histogram_summary(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.scalar_summary(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.merge_summary(grad_summaries)\n",
    "        \n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "        \n",
    "        # Train Summaries\n",
    "        loss_summary = tf.scalar_summary(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.scalar_summary(\"accuracy\", cnn.accuracy)\n",
    "        train_summary_op = tf.merge_summary([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.train.SummaryWriter(train_summary_dir, sess.graph_def)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.merge_summary([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.train.SummaryWriter(dev_summary_dir, sess.graph_def)\n",
    "       \n",
    "        # Checkpointing\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        # Tensorflow assumes this directory already exists so we need to create it\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.all_variables())\n",
    "        \n",
    "        # Initialize all variables\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "    \n",
    "        # A single training step\n",
    "        def train_step(x_batch, y_batch):\n",
    "            feed_dict = { cnn.input_x: x_batch, cnn.input_y: y_batch, cnn.dropout_keep_prob: DROPOUT_KEEP_PROB }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "            \n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            feed_dict = {cnn.input_x: x_batch, cnn.input_y: y_batch, cnn.dropout_keep_prob: 1.0 }\n",
    "            step, summaries, loss, accuracy = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "        \n",
    "        batches = data_helpers.batch_iter(zip(x_train, y_train), BATCH_SIZE, NUM_EPOCHS)\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % EVALUATE_EVERY == 0:\n",
    "                print(\"\\nDev Set:\")\n",
    "                dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % CHECKPOINT_EVERY == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))                \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
