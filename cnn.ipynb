{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import itertools\n",
    "import gc\n",
    "import gensim\n",
    "from collections import Counter\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load data from files\n",
    "positive_examples = list(open(\"./data/rt-polaritydata/rt-polarity.pos\").readlines())\n",
    "positive_examples = [s.strip() for s in positive_examples]\n",
    "negative_examples = list(open(\"./data/rt-polaritydata/rt-polarity.neg\").readlines())\n",
    "negative_examples = [s.strip() for s in negative_examples]\n",
    "\n",
    "# Generate labels\n",
    "positive_labels = [[0, 1] for _ in positive_examples]\n",
    "negative_labels = [[1, 0] for _ in negative_examples]\n",
    "# Concatenate positive and negative examples\n",
    "x_text = positive_examples + negative_examples\n",
    "y = np.concatenate([positive_labels, negative_labels], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at 90 minutes this movie is short , but it feels much longer .\n",
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "print x_text[len(positive_examples) + 242]\n",
    "print y[len(positive_examples) + 242]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Padding our sentences to the same length\n",
    "x_text_split = [s.split(\" \") for s in x_text]\n",
    "padded_sentences = [] \n",
    "# We pad all sentences to the maximum sentence length in the dataset\n",
    "SEQUENCE_LENGTH = max(len(x) for x in x_text_split)\n",
    "PADDING_WORD = \"<PAD/>\"\n",
    "for i in range(len(x_text_split)):\n",
    "    sentence = x_text_split[i]\n",
    "    num_padding = SEQUENCE_LENGTH - len(sentence)\n",
    "    new_sentence = sentence + [PADDING_WORD] * num_padding\n",
    "    padded_sentences.append(new_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build vocabulary\n",
    "word_counts = Counter(itertools.chain(*padded_sentences))\n",
    "# Mapping from index to word\n",
    "vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "# Mapping from word to index\n",
    "vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dennybritz/projects/python-venvs/tensorflow/lib/python2.7/site-packages/gensim/models/word2vec.py:1296: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "  return word in self.vocab\n"
     ]
    }
   ],
   "source": [
    "# Use word2vec for initial embeddings (optional)\n",
    "\n",
    "# Start with zero embeddings\n",
    "# initial_embeddings = np.random.randn(len(vocabulary), 300)\n",
    "\n",
    "# # Load word2vec and get embeddings\n",
    "# w2v_model = Word2Vec.load_word2vec_format(\"/Users/dennybritz/Downloads/GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "# for idx, word in enumerate(vocabulary_inv):\n",
    "#     if word in w2v_model:\n",
    "#         initial_embeddings[idx,:] = w2v_model[word]\n",
    "\n",
    "# # Save initial embeddings to avoid loading word2vec every time\n",
    "# np.save(\"./data/rt-polaritydata/initial_embeddings\", initial_embeddings)\n",
    "\n",
    "# # Clear word2vec\n",
    "# w2v_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optional: Load initial embeddings\n",
    "initial_embeddings = np.load(\"./data/rt-polaritydata/initial_embeddings.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples: 10662\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "\n",
    "# Our training data\n",
    "x = np.array([[vocabulary[word] for word in sentence] for sentence in padded_sentences])\n",
    "y = np.array(y)\n",
    "\n",
    "# Randomly shuffle data\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]\n",
    "\n",
    "print(\"Training examples: {:d}\".format(len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_example(x, y):\n",
    "    text = \" \".join([vocabulary_inv[i] for i in x])\n",
    "    label = \"POS\" if y[1] == 1 else \"NEG\"\n",
    "    print(\"{}: {}\".format(y, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Do this properly?\n",
    "x_train, x_dev = x_shuffled[:-1000], x_shuffled[-1000:]\n",
    "y_train, y_dev = y_shuffled[:-1000], y_shuffled[-1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]: this is surely one of the most frantic , virulent and foul-natured christmas season pics ever delivered by a hollywood studio . <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/> <PAD/>\n"
     ]
    }
   ],
   "source": [
    "print_example(x_train[2], y_train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Helper function to batch data\n",
    "def batch_iter(data, batch_size, num_epochs):\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int(len(data)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_data = data[shuffle_indices]      \n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 3]\n"
     ]
    }
   ],
   "source": [
    "# Test data!?\n",
    "with tf.Graph().as_default():\n",
    "    sess = tf.Session()\n",
    "    with sess.as_default():\n",
    "        x = tf.convert_to_tensor([[1,1,1,0,0,0,0],[1,1,1,0,0,0,0]])\n",
    "        mask = tf.argmin(x, 1)\n",
    "        print mask.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classifications\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, vocabulary_size, sequence_length, num_classes=2, embedding_size=300,\n",
    "        filter_sizes=[3, 4, 5], num_filters=100, affine_dim=256):\n",
    "        \n",
    "        # Placeholders for our input and output\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32)\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedded_chars = self._build_embedding(\n",
    "            [vocabulary_size, embedding_size], self.input_x)\n",
    "        # Add another dimension, expected by the convolutional layer\n",
    "        self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "\n",
    "        # Create a convolution + maxpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.variable_scope(\"filter-%s\" % filter_size):\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                pool_ksize = [1, sequence_length - filter_size + 1, 1, 1]\n",
    "                pooled = self._build_conv_maxpool(filter_shape, pool_ksize, self.embedded_chars_expanded)\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.h_pool = tf.concat(3, pooled_outputs)\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "\n",
    "        # Affine Layer with dropout\n",
    "        # self.h_affine = self._build_affine([num_filters_total, affine_dim], self.h_pool_flat)\n",
    "        # self.h_drop = tf.nn.dropout(self.h_affine, dropout_keep_prob)\n",
    "        \n",
    "        # Add dropout\n",
    "        self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "        \n",
    "        # Softmax Layer (Final output)\n",
    "        self.y = self._build_softmax([num_filters_total, num_classes], self.h_drop)\n",
    "        self.predictions = tf.argmax(self.y, 1, name=\"predictions\")\n",
    "\n",
    "        # Our loss expression\n",
    "        self.loss = self._build_mean_ce_loss(self.y, self.input_y)\n",
    "        # Expression for the accuracy\n",
    "        self.accuracy = self._build_accuracy(self.y, self.input_y)\n",
    "\n",
    "        # Summaries\n",
    "        total_loss_summary = tf.scalar_summary(\"loss\", self.loss)\n",
    "        accuracy_summmary = tf.scalar_summary(\"accuracy\", self.accuracy)\n",
    "\n",
    "    def _build_embedding(self, shape, input_tensor):\n",
    "        \"\"\"\n",
    "        Builds an embedding layer. Returns the embedded tensor.\n",
    "        \"\"\"\n",
    "        # We force this on the CPU because the op isn't implemented for the GPU\n",
    "        with tf.device('/cpu:0'), tf.variable_scope(\"embedding\"):\n",
    "            W_intializer = tf.random_uniform_initializer(-1.0, 1.0)\n",
    "            W_embeddings = tf.get_variable(\"W\", shape, initializer=W_intializer)\n",
    "            return tf.nn.embedding_lookup(W_embeddings, input_tensor)\n",
    "\n",
    "    def _build_affine(self, shape, input_tensor, activation_func=tf.nn.relu):\n",
    "        \"\"\"\n",
    "        Builds an affine (fully-connected) layer\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"affine\"):\n",
    "            W = tf.get_variable(\"W\", shape, initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "            b = tf.get_variable(\"b\", shape[-1], initializer=tf.constant_initializer(0.1))\n",
    "            h = activation_func(tf.matmul(input_tensor, W) + b, name=\"h\")\n",
    "        return h\n",
    "\n",
    "    def _build_softmax(self, shape, input_tensor):\n",
    "        \"\"\"\n",
    "        Builds a softmax layer\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"softmax\"):\n",
    "            W_initializer = tf.truncated_normal_initializer(stddev=0.1)\n",
    "            b_initializer = tf.constant_initializer(0.1)\n",
    "            W = tf.get_variable(\"W\", shape, initializer=W_initializer)\n",
    "            b = tf.get_variable(\"b\", shape[-1:], initializer=b_initializer)\n",
    "            return tf.nn.softmax(tf.nn.bias_add(tf.matmul(input_tensor, W), b), name=\"y\")\n",
    "    \n",
    "    def _build_mean_ce_loss(self, predictions, labels):\n",
    "        \"\"\"\n",
    "        Calculates the mean cross-entropy loss\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"mean-ce-loss\"):\n",
    "            return -tf.reduce_mean(labels * tf.log(predictions), name=\"mean_ce_loss\")\n",
    "        \n",
    "    def _build_accuracy(self, predictions, labels):\n",
    "        \"\"\"\n",
    "        Returns the accuracy\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(tf.argmax(predictions, 1), tf.argmax(labels, 1))\n",
    "            return tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "    \n",
    "    def _build_conv_maxpool(self, filter_shape, pool_shape, input_tensor):\n",
    "        \"\"\"\n",
    "        Builds a convolutional layer with ReLU activation followed by a  max-pooling layer.\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"conv-maxpool\"):\n",
    "            W = tf.get_variable(\"W\", filter_shape, initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "            conv = tf.nn.conv2d(input_tensor, W, strides=[1, 1, 1, 1], padding=\"VALID\")\n",
    "            begin = tf.to_int32(tf.zeros([4]))\n",
    "            b = tf.get_variable(\"b\", filter_shape[-1], initializer=tf.constant_initializer(0.1))\n",
    "            h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"conv\")\n",
    "            return tf.nn.max_pool(h, ksize=pool_shape, strides=[1, 1, 1, 1], padding='VALID', name=\"pool\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /Users/dennybritz/projects/wildml/cnn-text-classification-tf/runs/1449189839\n",
      "\n",
      "\n",
      "Dev Set:\n",
      "2015-12-04T09:45:37.519971: step 100, loss 0.48426, acc 0.506\n",
      "2015-12-04T09:45:43.162219: step 100, loss 0.44472, acc 0.541\n",
      "\n",
      "Saved model checkpoint to /Users/dennybritz/projects/wildml/cnn-text-classification-tf/runs/1449189839/checkpoints/model-100\n",
      "\n",
      "\n",
      "Dev Set:\n",
      "2015-12-04T09:47:22.212004: step 200, loss 0.467162, acc 0.531\n",
      "2015-12-04T09:47:27.342086: step 200, loss 0.403516, acc 0.578\n",
      "\n",
      "Saved model checkpoint to /Users/dennybritz/projects/wildml/cnn-text-classification-tf/runs/1449189839/checkpoints/model-200\n",
      "\n",
      "\n",
      "Dev Set:\n",
      "2015-12-04T09:49:05.575714: step 300, loss 0.460155, acc 0.557\n",
      "2015-12-04T09:49:10.936702: step 300, loss 0.378775, acc 0.591\n",
      "\n",
      "Saved model checkpoint to /Users/dennybritz/projects/wildml/cnn-text-classification-tf/runs/1449189839/checkpoints/model-300\n",
      "\n",
      "\n",
      "Dev Set:\n",
      "2015-12-04T09:50:48.187866: step 400, loss 0.446946, acc 0.555\n",
      "2015-12-04T09:50:54.736985: step 400, loss 0.3541, acc 0.614\n",
      "\n",
      "Saved model checkpoint to /Users/dennybritz/projects/wildml/cnn-text-classification-tf/runs/1449189839/checkpoints/model-400\n",
      "\n",
      "\n",
      "Dev Set:\n",
      "2015-12-04T09:52:33.564588: step 500, loss 0.440271, acc 0.565\n",
      "2015-12-04T09:52:38.663521: step 500, loss 0.334067, acc 0.646\n",
      "\n",
      "Saved model checkpoint to /Users/dennybritz/projects/wildml/cnn-text-classification-tf/runs/1449189839/checkpoints/model-500\n",
      "\n",
      "\n",
      "Dev Set:\n",
      "2015-12-04T09:54:10.541371: step 600, loss 0.427158, acc 0.573\n",
      "2015-12-04T09:54:15.961737: step 600, loss 0.312127, acc 0.667\n",
      "\n",
      "Saved model checkpoint to /Users/dennybritz/projects/wildml/cnn-text-classification-tf/runs/1449189839/checkpoints/model-600\n",
      "\n",
      "\n",
      "Dev Set:\n",
      "2015-12-04T09:55:55.165646: step 700, loss 0.418976, acc 0.57\n",
      "2015-12-04T09:56:00.206814: step 700, loss 0.298894, acc 0.692\n",
      "\n",
      "Saved model checkpoint to /Users/dennybritz/projects/wildml/cnn-text-classification-tf/runs/1449189839/checkpoints/model-700\n",
      "\n",
      "\n",
      "Dev Set:\n",
      "2015-12-04T09:57:41.814687: step 800, loss 0.40455, acc 0.575\n",
      "2015-12-04T09:57:47.079431: step 800, loss 0.279982, acc 0.715\n",
      "\n",
      "Saved model checkpoint to /Users/dennybritz/projects/wildml/cnn-text-classification-tf/runs/1449189839/checkpoints/model-800\n",
      "\n",
      "\n",
      "Dev Set:\n",
      "2015-12-04T09:59:15.651868: step 900, loss 0.396629, acc 0.571\n",
      "2015-12-04T09:59:20.765780: step 900, loss 0.271544, acc 0.732\n",
      "\n",
      "Saved model checkpoint to /Users/dennybritz/projects/wildml/cnn-text-classification-tf/runs/1449189839/checkpoints/model-900\n",
      "\n",
      "\n",
      "Dev Set:\n",
      "2015-12-04T10:00:51.775448: step 1000, loss 0.385867, acc 0.582\n",
      "2015-12-04T10:00:57.103389: step 1000, loss 0.257064, acc 0.736\n",
      "\n",
      "Saved model checkpoint to /Users/dennybritz/projects/wildml/cnn-text-classification-tf/runs/1449189839/checkpoints/model-1000\n",
      "\n",
      "\n",
      "Dev Set:\n",
      "2015-12-04T10:02:30.406720: step 1100, loss 0.377711, acc 0.576\n",
      "2015-12-04T10:02:35.592898: step 1100, loss 0.249027, acc 0.751\n",
      "\n",
      "Saved model checkpoint to /Users/dennybritz/projects/wildml/cnn-text-classification-tf/runs/1449189839/checkpoints/model-1100\n",
      "\n",
      "\n",
      "Dev Set:\n",
      "2015-12-04T10:04:07.337482: step 1200, loss 0.369378, acc 0.581\n",
      "2015-12-04T10:04:12.523269: step 1200, loss 0.24108, acc 0.76\n",
      "\n",
      "Saved model checkpoint to /Users/dennybritz/projects/wildml/cnn-text-classification-tf/runs/1449189839/checkpoints/model-1200\n",
      "\n",
      "\n",
      "Dev Set:\n",
      "2015-12-04T10:06:10.996343: step 1300, loss 0.364546, acc 0.582\n",
      "2015-12-04T10:06:18.311508: step 1300, loss 0.238382, acc 0.776\n",
      "\n",
      "Saved model checkpoint to /Users/dennybritz/projects/wildml/cnn-text-classification-tf/runs/1449189839/checkpoints/model-1300\n",
      "\n",
      "\n",
      "Dev Set:\n",
      "2015-12-04T10:07:56.403448: step 1400, loss 0.355595, acc 0.583\n",
      "2015-12-04T10:08:01.496800: step 1400, loss 0.230497, acc 0.785\n",
      "\n",
      "Saved model checkpoint to /Users/dennybritz/projects/wildml/cnn-text-classification-tf/runs/1449189839/checkpoints/model-1400\n",
      "\n",
      "\n",
      "Dev Set:\n",
      "2015-12-04T10:09:39.979834: step 1500, loss 0.350413, acc 0.592\n",
      "2015-12-04T10:09:45.855091: step 1500, loss 0.230002, acc 0.793\n",
      "\n",
      "Saved model checkpoint to /Users/dennybritz/projects/wildml/cnn-text-classification-tf/runs/1449189839/checkpoints/model-1500\n",
      "\n",
      "\n",
      "Dev Set:\n",
      "2015-12-04T10:11:27.656006: step 1600, loss 0.34366, acc 0.587\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-123-7cb948574b53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nDev Set:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0meval_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdev_summary_writer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                 \u001b[0meval_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcurrent_step\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mCHECKPOINT_EVERY\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-123-7cb948574b53>\u001b[0m in \u001b[0;36meval_loss\u001b[0;34m(x_batch, y_batch, writer)\u001b[0m\n\u001b[1;32m     57\u001b[0m             step, summaries, loss, accuracy = sess.run(\n\u001b[1;32m     58\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m                 feed_dict)\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0mtime_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misoformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}: step {}, loss {:g}, acc {:g}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dennybritz/projects/python-venvs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munique_fetch_targets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[0;31m# User may have fetched the same tensor multiple times, but we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dennybritz/projects/python-venvs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, target_list, fetch_list, feed_dict)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m       return tf_session.TF_Run(self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 404\u001b[0;31m                                target_list)\n\u001b[0m\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatusNotOK\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 500\n",
    "EVALUATE_EVERY = CHECKPOINT_EVERY = 100\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=True)      \n",
    "    sess = tf.Session(config=session_conf)  \n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(\n",
    "            vocabulary_size=len(vocabulary),\n",
    "            sequence_length=SEQUENCE_LENGTH,\n",
    "            num_classes=2,\n",
    "            embedding_size=300,\n",
    "            filter_sizes=[3, 4, 5],\n",
    "            num_filters=100)\n",
    "        \n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\")\n",
    "        optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        # Keep track of gradient values and sparsity\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                tf.histogram_summary(\"{}/grad\".format(v.name), g)\n",
    "                tf.scalar_summary(\"{}/grad-sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "        \n",
    "        \n",
    "        # Summary Writers\n",
    "        summary_op = tf.merge_all_summaries()\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.train.SummaryWriter(train_summary_dir, sess.graph_def)\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.train.SummaryWriter(dev_summary_dir, sess.graph_def)\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "        \n",
    "        # Checkpointing\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        # Tensorflow assumes this directory already exists so we need to create it\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.all_variables())\n",
    "        \n",
    "        # Initialize all variables\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        \n",
    "        # Optional: Initialize embeddings\n",
    "#         with tf.variable_scope(\"embedding\", reuse=True):\n",
    "#             embedding_W = tf.get_variable(\"W\")\n",
    "#         sess.run(embedding_W.assign(initial_embeddings))\n",
    "    \n",
    "        def eval_loss(x_batch, y_batch, writer=None):\n",
    "            feed_dict = {cnn.input_x: x_batch, cnn.input_y: y_batch, cnn.dropout_keep_prob: 1.0 }\n",
    "            step, summaries, loss, accuracy = sess.run(\n",
    "                [global_step, summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "        \n",
    "        # A single training step\n",
    "        def train_step(x_batch, y_batch):\n",
    "            feed_dict = { cnn.input_x: x_batch, cnn.input_y: y_batch, cnn.dropout_keep_prob: 0.25 }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            # Print and write metrics\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            # print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "        \n",
    "        batches = batch_iter(zip(x_train, y_train), BATCH_SIZE, NUM_EPOCHS)\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % EVALUATE_EVERY == 0:\n",
    "                print(\"\\nDev Set:\")\n",
    "                eval_loss(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                eval_loss(x_train[:1000], y_train[:1000], writer=None)\n",
    "                print(\"\")\n",
    "            if current_step % CHECKPOINT_EVERY == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))                \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
